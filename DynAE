import tensorflow as tf
import metrics
import math
import numpy as np
import matplotlib.pyplot as plt
import os
from sklearn.metrics import silhouette_score 
from time import time
from tensorflow.keras.models import Model
from tensorflow.keras.losses import mean_squared_error
from datasets import generate
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras.layers import Layer, Flatten, Lambda, InputSpec, Input, Dense
from tensorflow.keras.initializers import VarianceScaling
from tensorflow.keras import callbacks
from tensorflow.keras.utils import plot_model
from tensorflow.keras import backend as K
from sklearn.cluster import KMeans, SpectralClustering
from sklearn.decomposition import PCA
from sklearn.neighbors import NearestNeighbors
from sklearn.neighbors import KNeighborsClassifier
from datasets import generate_data_batch, generate_transformed_batch
from scipy.spatial import distance
from intrinsic_dimension import estimate, block_analysis
from scipy.spatial.distance import pdist, squareform
import tensorflow.experimental.numpy as tnp
from sklearn.preprocessing import StandardScaler
from sklearn.cluster import AgglomerativeClustering
from sklearn.cluster import Birch
from sklearn.mixture import GaussianMixture
from sklearn.cluster import DBSCAN
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import f1_score, precision_score, recall_score
from sklearn.metrics import accuracy_score
from munkres import Munkres, print_matrix
from sklearn.metrics import roc_auc_score

##
#import tensorflow.compat.v2 as tf
#from keras import backend
##from keras.distribute import distributed_file_utils
##from keras.distribute import worker_training_state
#from keras.optimizers import optimizer_experimental
##from keras.optimizers.schedules import learning_rate_schedule
#from keras.utils import generic_utils
#from keras.utils import generic_utils

#from keras.utils import generic_utils
#from keras.utils import io_utils
#from keras.utils import tf_utils
#from keras.utils import version_utils
#from keras.utils.data_utils import Sequence
#from keras.utils.generic_utils import Progbar
#from keras.utils.mode_keys import ModeKeys
##



#from sklearn.cluster import kmedoids
#
#import torch.nn as nn
#import torch.nn.functional as F
#import matplotlib.pyplot as plt
#from tqdm import tqdm
#from sklearn import metrics
#from munkres import Munkres
#from torch.nn import Parameter
#from torch.nn import Parameter
#from sklearn.metrics import f1_score

#from torch.optim import Adam, SGD, RMSprop
#from torch.optim.lr_scheduler import StepLR
#from sklearn.cluster import KMeans
#from sklearn.neighbors import NearestNeighbors
#from intrinsic_dimension import estimate, block_analysis
#from scipy.spatial.distance import pdist, squareform
#
#from sklearn.decomposition import PCA
#from sklearn.cluster import SpectralClustering
#import torchvision
#import torchvision.transforms as transforms
#


PATH_RESULT = '/content/drive/My Drive/Colab/DynAE_Amal/results'
PATH_VIS = '/content/drive/My Drive/Colab/DynAE_Amal/visualisation'


model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(filepath=PATH_RESULT,save_weights_only=True,monitor='val_accuracy', verbose=1, mode='max',save_best_only=True)
callbacks_list = [model_checkpoint_callback]

def computeID(emb, nres=1, fraction=1):
    ID = []
    r = emb.astype(np.float64)
    n = int(np.round(r.shape[0] * fraction))            
    dist = squareform(pdist(r, 'euclidean'))
    for i in range(nres):
        dist_s = dist
        perm = np.random.permutation(r.shape[0])[0:n]
        dist_s = dist_s[perm,:]
        dist_s = dist_s[:,perm]
        ID.append(estimate(dist_s)[2]) 
    return ID

def computePC_ID(emb_np, th=0.9999):
    scaler = StandardScaler()
    scaler.fit(emb_np)
    embn = scaler.transform(emb_np)
    pca = PCA()
    pca.fit(embn)
    #cs = np.cumsum(pca.explained_variance_ratio_)
    sv = pca.singular_values_
    evr = pca.explained_variance_ratio_
    cs = np.cumsum(pca.explained_variance_ratio_)
    return np.argwhere(cs > th)[0][0]

    
def map_vector_to_clusters(y_true, y_pred):
  y_true = y_true.astype(np.int64)
#  D = max(y_pred.max(), y_true.max()) + 1
  D = max(y_pred.max(), y_true.max()) + 1
  w = np.zeros((D, D), dtype=np.int64)
  for i in range(y_pred.size):
    w[y_true[i], y_pred[i]] += 1
  from scipy.optimize import linear_sum_assignment
  row_ind, col_ind = linear_sum_assignment(w.max() - w)

  y_true_mapped = np.zeros(y_pred.shape)
  for i in range(y_pred.shape[0]):
    y_true_mapped[i] = col_ind[y_true[i]]
  return y_true_mapped.astype(int)

def q_mat(X, centers, alpha=1.0):
    
    if X.size == 0:
        q = np.array([])
    else:
        q = 1.0 / (1.0 + (np.sum(np.square(np.expand_dims(X, 1) - centers), axis=2) / alpha))
        #print('qfirst', q)
        q = q**((alpha+1.0)/2.0)
        #print('sessecond', q)
        #print('np.transpose(q)',np.transpose(q))
        #print('np.sum(q, axis=1)',np.sum(q, axis=1) )
        q = np.transpose(np.transpose(q)/np.sum(q, axis=1))
        #print('alpha',alpha)
        #print('inside else',q)
        #print('inside else qq', q.argmax(0)) 
    return q
    
def q_mat_oldbeforemed(X, centers, alpha=1.0):
    if X.size == 0:
        q = np.array([])
    else:
        q = 1.0 / (1.0 + (np.sum(np.square(np.expand_dims(X, 1) - centers), axis=2) / alpha))
        q = q**((alpha+1.0)/2.0)
        q = np.transpose(np.transpose(q)/np.sum(q, axis=1))
    return q
    
    
def kl(x, y):
    X = tf.distributions.Categorical(probs=x)
    Y = tf.distributions.Categorical(probs=y)
    return tf.distributions.kl_divergence(X, Y)

def generate_supervisory_signals(x_emb, x_img, centers_emb_fixed, centers_img_fixed, beta1, beta2):
    q = q_mat(x_emb, centers_emb_fixed, alpha=1.0)
    y_pred = q.argmax(1)
    confidence1 = q.max(1) 
    confidence2 = np.zeros((q.shape[0],))
    ind = np.argsort(q, axis=1)[:,-2]
    Y_encoder = []
    Y_autoencoder = []
    for i in range(x_img.shape[0]):
        confidence2[i] = q[i,ind[i]]
        if (confidence1[i]) > beta1 and (confidence1[i] - confidence2[i]) > beta2:
            Y_encoder.append(centers_emb_fixed[y_pred[i]])
            Y_autoencoder.append(centers_img_fixed[y_pred[i]])
        else:
            Y_encoder.append(x_emb[i])
            Y_autoencoder.append(x_img[i])    
    Y_autoencoder = np.asarray(Y_autoencoder)
    Y_encoder = np.asarray(Y_encoder)
    return Y_encoder, Y_autoencoder
    

def draw_centers(n_clusters, centers_img, img_h=28, img_w=28):
    plt.figure(figsize=(n_clusters, 4))
    for i in range(n_clusters):
        ax = plt.subplot(1, n_clusters, i + 1)
        plt.imshow(centers_img[i].reshape(img_h, img_w))
        plt.gray()
        ax.get_xaxis().set_visible(False)
        ax.get_yaxis().set_visible(False)
    plt.show()

def total_loss(y_true, y_pred):
    #
    return y_pred
 
def encoder_constructor(dims, visualisation_dir, act='relu'):
    #dims=[x.shape[-1], 500, 500, 2000, 2]
    n_stacks = len(dims) - 1
    #print('n_stacks',n_stacks)
    #print('act',act)
    init = VarianceScaling(scale=1. / 3., mode='fan_in', distribution='uniform')
    # input
    x = Input(shape=(dims[0],), name='input_encoder')
    #print('xxxxxxxxxencoder',x)
    #x = Input(shape=(dims[0],), name='input_encoder')
    h = x
    # internal layers in encoder
   # dims=[x.shape[-1], 500, 500, 2000, 2]
    for i in range(n_stacks-1):
        h = Dense(dims[i + 1], activation=act, kernel_initializer=init, name='encoder_%d' % i)(h)
    # hidden layer
    h = Dense(dims[-1], kernel_initializer=init, name='encoder_%d' % (n_stacks - 1))(h)  # hidden layer, features are extracted from here
    encoder = Model(inputs=x, outputs=h, name='encoder')
    plot_model(encoder, show_shapes=True, show_layer_names=True, to_file=visualisation_dir + '/graph/FcEncoder.png')
    return encoder

def decoder_constructor(dims, visualisation_dir, act='relu'):
    n_stacks = len(dims) - 1
    init = VarianceScaling(scale=1. / 3., mode='fan_in', distribution='uniform')
    # input
    z = Input(shape=(dims[-1],), name='input_decoder')
    y = z
    # internal layers in decoder
    for i in range(n_stacks-1, 0, -1):
        y = Dense(dims[i], activation=act, kernel_initializer=init, name='decoder_%d' % i)(y)
    # output
    y = Dense(dims[0], kernel_initializer=init, name='decoder_0')(y)
    decoder = Model(inputs=z, outputs=y, name='decoder')
    plot_model(decoder, show_shapes=True, show_layer_names=True, to_file=visualisation_dir + '/graph/FcDecoder.png')
    return decoder

def ae_constructor(encoder, decoder, dims, visualisation_dir):
    x = Input(shape=(dims[0],), name='input_autencoder')
    autoencoder = Model(inputs=x, outputs=decoder(encoder(x)), name='autoencoder')
    plot_model(autoencoder, show_shapes=True, show_layer_names=True, to_file=visualisation_dir + '/graph/FcAutoencoder.png')
    return autoencoder

def dynAE_constructor(encoder, ae, dims, gamma, visualisation_dir):
    input1 = Input(shape=(dims[0],), name='input_dynAE')
    target1 = Input(shape=(dims[-1],), name='target1_dynAE')
    target2 = Input(shape=(dims[0],), name='target2_dynAE')
    target3 = Input(shape=(dims[-1],), name='target3_dynAE')

    def loss_dynAE(x):
        encoder_output = x[0]
        ae_output = x[1]
        target1 = x[2]
        target2 = x[3]
        loss1 = tf.losses.mean_squared_error(encoder_output, target1)
        loss2 = tf.losses.mean_squared_error(ae_output, target2)
        return loss1 + gamma * loss2

    x1 = encoder(input1)
    x2 = ae(input1)
    out1 = Lambda(lambda x: loss_dynAE(x), name="output1_dynAE")((x1, x2, target1, target2))
    dynAE = Model(inputs=[input1, target1, target2], outputs=[out1], name='dynAE')
    plot_model(dynAE, show_shapes=True, show_layer_names=True, to_file=visualisation_dir + '/graph/FcDynAE.png')
    return dynAE
    # Computing Feature Randomness and Feature Drift coefficients
    #loss_pseudo_supervised = tf.losses.mean_squared_error(x1, target1)
    #loss_self_supervised = tf.losses.mean_squared_error(x2, target2)
    #loss_supervised = tf.losses.mean_squared_error(x1, target3)
    #loss_dynAE =  loss_pseudo_supervised + gamma * loss_self_supervised
    #grad_loss_pseudo_supervised = []
    #grad_loss_self_supervised = []
    #grad_loss_supervised = []
    #grad_loss_dynAE = []
    #for i in range((len(dims) - 1) * 2):
    #    grad_loss_pseudo_supervised.append(tf.gradients(loss_pseudo_supervised, dynAE.trainable_weights[i]))
    #    grad_loss_self_supervised.append(tf.gradients(loss_self_supervised, dynAE.trainable_weights[i]))
    #    grad_loss_supervised.append(tf.gradients(loss_supervised, dynAE.trainable_weights[i])) 
    #    grad_loss_dynAE.append(tf.gradients(loss_dynAE, dynAE.trainable_weights[i])) 
    #return dynAE, grad_loss_pseudo_supervised, grad_loss_self_supervised, grad_loss_supervised, grad_loss_dynAE

def i_ae_constructor(encoder, decoder, dims, visualisation_dir):
    input1 = Input(shape=(dims[0],), name='input_i_ae')
    alpha1 = Input(shape=(1,), name='alpha_i_ae')

    def mix_layer(x):
        encode = x[0]
        alpha1 = x[1]     
        encode_mix = alpha1 * encode + (1 - alpha1) * encode[::-1]   
        return encode_mix

    encode = encoder(input1)
    encode_mix = Lambda(mix_layer, name="mix_layer")((encode, alpha1))  
    decode_mix = decoder(encode_mix)
    i_ae = Model(inputs=[input1, alpha1], outputs=decode_mix, name='interpolation_autoencoder')
    plot_model(i_ae, show_shapes=True, show_layer_names=True, to_file=visualisation_dir + '/graph/FcI_AE.png')
    return i_ae
    
def critic_constructor(dims, visualisation_dir, act='relu'):
    n_stacks = len(dims) - 1 
    init = VarianceScaling(scale=1. / 3., mode='fan_in', distribution='uniform')    
    # input
    x = Input(shape=(dims[0],), name='input_critic')
    h = x
    # internal layers in encoder
    for i in range(n_stacks-1):
        h = Dense(dims[i + 1], activation=act, kernel_initializer=init, name='critic_%d' % i)(h)
    # hidden layer
    h = Dense(dims[-1], kernel_initializer=init, name='critic_%d' % (n_stacks - 1))(h)  # hidden layer, features are extracted from here  
    h = Lambda(lambda x: tf.reduce_mean(x, [1]), name="reduce_mean_layer")(h)
    critic = Model(inputs=x, outputs=h, name='critic')
    plot_model(critic, show_shapes=True, show_layer_names=True, to_file=visualisation_dir + '/graph/FcCritic.png')
    return critic

def disc_constructor(critic, dims, visualisation_dir):
    input1 = Input(shape=(dims[0],), name='input1_disc')
    input2 = Input(shape=(dims[0],), name='input2_disc')
    alpha1 = Input(shape=(1,), name='alpha1_disc')
    alpha2 = Input(shape=(1,), name='alpha2_disc')
 
    def loss_disc(x):
        x1 = x[0]
        x2 = x[1]
        alpha1 = x[2]
        alpha2 = x[3]
        loss_within_cluster_interp = tf.reduce_mean(tf.square(x1 - alpha1))
        loss_between_cluster_interp = tf.reduce_mean(tf.square(x2 - alpha2))
        return loss_within_cluster_interp + loss_between_cluster_interp

    x1 = critic(input1)
    x2 = critic(input2)
    out1 = Lambda(loss_disc, name="output1_disc")((x1, x2, alpha1, alpha2))

    disc = Model(inputs=[input1, input2, alpha1, alpha2], outputs=[out1], name='discriminator')

    plot_model(disc, show_shapes=True, show_layer_names=True, to_file=visualisation_dir + '/graph/FcDiscriminator.png')
    return disc

def aci_ae_constructor(critic, ae, i_ae, advweight, dims, visualisation_dir):
    critic.trainable = False
    input1 = Input(shape=(dims[0],), name='input_aci_ae')
    alpha1 = Input(shape=(1,), name='alpha_aci_ae')

    def loss_aci_ae(x):
        input1 = x[0]
        x1 = x[1]
        x2 = x[2]
        loss_ae_critic = tf.reduce_mean(tf.square(x2))
        loss_ae = tf.losses.mean_squared_error(input1, x1)
        return loss_ae + advweight * loss_ae_critic

    x1 = ae(input1)
    x2 = critic(i_ae([input1, alpha1]))

    out1 = Lambda(lambda x: loss_aci_ae(x), name="output1_aci_ae")((input1, x1, x2))
    aci_ae = Model(inputs=[input1, alpha1], outputs=[out1], name='aci_ae')
    plot_model(aci_ae, show_shapes=True, show_layer_names=True, to_file=visualisation_dir + '/graph/FcACI_AE.png')
    return aci_ae




class DynAE:
    def __init__(self, batch_size, dataset, dims, loss_weight, gamma=100, n_clusters=10, alpha=1.0, visualisation_dir=PATH_VIS, ws=0.1, hs=0.1, rot=10, scale=0.0 ):
   #best breast def __init__(self, batch_size, dataset, dims, loss_weight, gamma=100, n_clusters=10, alpha=1.0, visualisation_dir=PATH_VIS, ws=0.1, hs=0.1, rot=10, scale=0.0):
        self.batch_size = batch_size
        self.dataset = dataset
        self.dims = dims
        self.visualisation_dir = visualisation_dir
        self.n_clusters = n_clusters
        self.alpha = alpha
        self.loss_weight = loss_weight
        self.datagen = ImageDataGenerator(width_shift_range=ws, height_shift_range=hs, rotation_range=rot, zoom_range=scale)
        self.ws = ws 
        self.hs = hs
        self.rot = rot
        self.scale = scale
        self.gamma = gamma
        #models
        self.encoder = encoder_constructor(self.dims, self.visualisation_dir)
        self.decoder = decoder_constructor(self.dims, self.visualisation_dir)
        self.ae = ae_constructor(self.encoder, self.decoder, self.dims, self.visualisation_dir)
        self.i_ae = i_ae_constructor(self.encoder, self.decoder, self.dims, self.visualisation_dir) 
        #self.dynAE, self.grad_loss_pseudo_supervised, self.grad_loss_self_supervised, self.grad_loss_supervised, self.grad_loss_dynAE = dynAE_constructor(self.encoder, self.ae, self.dims, self.gamma, self.visualisation_dir)
        self.dynAE = dynAE_constructor(self.encoder, self.ae, self.dims, self.gamma, self.visualisation_dir)
        self.critic = critic_constructor(self.dims, self.visualisation_dir)
        self.disc = disc_constructor(self.critic, self.dims, self.visualisation_dir)
        self.aci_ae = aci_ae_constructor(self.critic, self.ae, self.i_ae, self.loss_weight, self.dims, self.visualisation_dir)
       
        

    def predict_ae(self, x):
        x_recons = self.ae.predict(x, verbose=0)
        return x_recons

    def predict_encoder(self, x):
     
        #print('xxxxxx',x)
        x_encode = self.encoder.predict(x, verbose=0)
        #print('x_encode',x_encode)
        return x_encode

    def predict_i_ae(self, x, alpha):
        x_inter_recons = self.i_ae.predict([x, alpha], verbose=0)
        return x_inter_recons

    def compile_ae(self, optimizer='sgd'):
        #
        self.ae.compile(optimizer=optimizer, loss='mse')

    def compile_dynAE(self, optimizer='sgd'): 
        #                                                                  
        self.dynAE.compile(optimizer=optimizer, loss=total_loss)

    def compile_critic(self, optimizer='sgd'):
        #
        self.critic.compile(optimizer=optimizer, loss='mse')

    def compile_disc(self, optimizer='sgd'):
        #
        self.disc.compile(optimizer=optimizer, loss=total_loss)

    def compile_aci_ae(self, optimizer='sgd'):
        #
        self.aci_ae.compile(optimizer=optimizer, loss=total_loss)

    def train_on_batch_ae(self, x, y):
        #
        return self.ae.train_on_batch(x, y)

    
    def train_on_batch_dynAE_old(self, x, y1, y2):
        #
        y = np.zeros((x.shape[0],))
        gamma = 100
        #print('gamma', gamma)
        return self.dynAE.train_on_batch([x, y1, y2], y)


    def train_on_batch_dynAE(self, x, y1, y2):
        #
        import numpy as np
        from sklearn.impute import SimpleImputer
        imp = SimpleImputer(missing_values=np.nan, strategy='mean')
        imp.fit(x)
        SimpleImputer()
     #   print('x',x)
       
      #  print('xv2222',x)
        y = np.zeros((x.shape[0],))
        return self.dynAE.train_on_batch([ x, y1, y2], y)
       # print('yyyyyy',y)
    


    def train_on_batch_disc(self, x1, x2, y1, y2):
        y = np.zeros((x1.shape[0],))
        return self.disc.train_on_batch([x1, x2, y1, y2], y)

    def train_on_batch_aci_ae(self, x1, x2):
        y = np.zeros((x1.shape[0],))
        return self.aci_ae.train_on_batch([x1, x2], y)
     
    def train_ae(self, x, y=None, optimizer='adam', epochs=500, batch_size=256, save_dir=PATH_RESULT, verbose=1, aug_train=True):
        print('Begin pretraining: ', '-' * 60)
        self.ae.compile(optimizer=optimizer, loss='mse')
        #csv_logger = callbacks.CSVLogger(save_dir + '/' + self.dataset + '/pretrain/log_ae.csv')
        #cb = [csv_logger]
        

        #Prepare yyyyyylog file
        
        import csv, os
        if not os.path.exists(save_dir):
            os.makedirs(save_dir)
        logfile = open(save_dir + '/' + self.dataset + '/pretrain/pretrain.csv', 'w')
        logwriter = csv.DictWriter(logfile, fieldnames=['iter', 'ID_global_mean', 'ID_local_mean', 'PC_ID_mean', 'ID_global_error', 'ID_local_error', 'FT','acc','nmi'])
        logwriter.writeheader()
       
       
        cb = []
        if y is not None and verbose > 0:
            class PrintACC(callbacks.Callback):
                def __init__(self, x, y, encoder):
                    self.x = x
                    self.y = y
                    self.encoder = encoder
                    super(PrintACC, self).__init__()

                def on_epoch_end(self, epoch, logs=None):
                    if epoch % 20 != 0:
                        return
                    features = self.encoder.predict(self.x)
                    km = KMeans(n_clusters=len(np.unique(y)), n_init=20)
                    y_pred = km.fit_predict(features)
                    print(' '*8 + '|==>  acc: %.4f,  nmi: %.4f  <==|'% (metrics.acc(y, y_pred), metrics.nmi(y, y_pred)))
            
            class PrintID_LID(callbacks.Callback):
                def __init__(self, x, y, encoder):
                    self.x = x
                    self.y = y
                    self.encoder = encoder
                    super(PrintID_LID, self).__init__()

                def on_epoch_end(self, epoch, logs=None):
                    features = self.encoder.predict(self.x)
                    km = KMeans(n_clusters=len(np.unique(y)), n_init=20)
                    y_pred = km.fit_predict(features)
                    ID = [] 
                    PC_ID = []
                    for k in range(len(np.unique(y))):
                       ID.append(computeID(features[y_pred==k]))
                       PC_ID.append(computePC_ID(features[y_pred==k]))
                    ID = np.asarray(ID)
                    PC_ID = np.asarray(PC_ID)
                    ID_global_mean = np.mean(ID)
                    ID_local_mean = np.mean(ID, axis=1) 
                    ID_global_error = np.std(ID)
                    ID_local_error = np.std(ID, axis=1)
                    PC_ID_mean = np.mean(PC_ID)
                    FT = PC_ID_mean - ID_global_mean
                    print ("ID = ", ID_global_mean)
                    print ("LID = ", PC_ID_mean)
                    logdict = dict(iter=epoch, ID_global_mean=ID_global_mean, ID_local_mean=ID_local_mean, PC_ID_mean=PC_ID_mean, ID_global_error=ID_global_error, ID_local_error=ID_local_error, FT=FT,acc=metrics.acc(y, y_pred) ,nmi=metrics.nmi(y, y_pred)  )
                    logwriter.writerow(logdict)
                    logfile.flush()
                    
                    
            cb.append(PrintACC(x, y, self.encoder))
            cb.append(PrintID_LID(x, y, self.encoder))
        # begin pretraining
        t0 = time()
        if not aug_train:
            self.ae.fit(x, x, batch_size=batch_size, epochs=epochs, callbacks=cb, verbose=verbose)
        else:
            print('-=*'*20)
            print('Using augmentation for ae')
            print('-=*'*20)
            def gen(x, batch_size):
                if len(x.shape) > 2:  # image
                    gen0 = self.datagen.flow(x, shuffle=True, batch_size=batch_size)
                    while True:
                        batch_x = gen0.next()
                        yield (batch_x, batch_x)
                else:
                    width = int(np.sqrt(x.shape[-1]))
                    if width * width == x.shape[-1]:  # gray
                        im_shape = [-1, width, width, 1]
                    else:  # RGB
                        width = int(np.sqrt(x.shape[-1] / 3.0))
                        im_shape = [-1, width, width, 3]
                    gen0 = self.datagen.flow(np.reshape(x, im_shape), shuffle=True, batch_size=batch_size)
                    while True:
                        batch_x = gen0.next()
                        batch_x = np.reshape(batch_x, [batch_x.shape[0], x.shape[-1]])
                        yield (batch_x, batch_x)
            self.ae.fit_generator(gen(x, batch_size), steps_per_epoch=int(x.shape[0]/batch_size), epochs=epochs, callbacks=cb, verbose=verbose)
        exec_time = time() - t0
        print('Pretraining time: %ds' % (exec_time))
        self.ae.save_weights(save_dir + '/' + self.dataset + '/pretrain/ae_weights.h5')
        print('Pretrained weights are saved to %s/%s/pretrain/ae_weights.h5' % (save_dir, self.dataset))
        if y is not None:
            features = self.encoder.predict(x)
            km = KMeans(n_clusters=len(np.unique(y)), n_init=20)
            y_pred = km.fit_predict(features)
            f = open(save_dir + '/' + self.dataset + '/pretrain/results_train_ae.txt', 'w+')
            f.write('|==> ACC: %.4f,  NMI: %.4f  <==| \n' % (metrics.acc(y, y_pred), metrics.nmi(y, y_pred)))
            f.write('|==> Execution time: %.5f <==| \n' % (exec_time))
            f.close()
            logfile.close()
        print('End pretraining: ', '-' * 60)
        


#########   
    def train_aci_ae(self, x, y=None, maxiter=120e3, batch_size=256, validate_interval=2800, save_interval=2800, save_dir=PATH_RESULT, verbose=1, aug_train=True):
        print('Begin aci_ae training: ', '-' * 60)
        
        #Prepare log file
        import csv, os
        if not os.path.exists(save_dir):
            os.makedirs(save_dir)
        logfile = open(save_dir + '/' + self.dataset + '/pretrain/train_aci_ae_log.csv', 'w')
       # logwriter = csv.DictWriter(logfile, fieldnames=['iter', 'acc', 'nmi', 'loss_aci_ae', 'loss_disc'])
        logwriter = csv.DictWriter(logfile, fieldnames=['iter', 'acc', 'nmi', 'loss_aci_ae', 'loss_disc', 'ID_global_mean', 'ID_local_mean', 'PC_ID_mean', 'ID_global_error', 'ID_local_error', 'FT'])
        logwriter.writeheader()
       
        #Initialization
        t0 = time()
        loss_aci_ae = 0
        loss_disc = 0
        index = 0
        index_array = np.arange(x.shape[0])
        acc_prev = 0
        #iter_max = 0
        
        #Training loop
        for ite in range(int(maxiter)):
            #Validation interval
            if ite % validate_interval == 0: 
                if y is not None and verbose > 0: 
                    avg_loss_aci_ae = loss_aci_ae / validate_interval 
                    avg_loss_disc = loss_disc / validate_interval   
                    loss_aci_ae = 0. 
                    loss_disc = 0.            
                    features = self.predict_encoder(x)
                    km = KMeans(n_clusters=self.n_clusters, n_init=20)
                    y_pred = km.fit_predict(features)
                    acc = np.round(metrics.nmi(y, y_pred), 5)
                    nmi = np.round(metrics.nmi(y, y_pred), 5)
                    ari = np.round(metrics.ari(y, y_pred), 5)
        # New Part
                    x_emb = self.predict_encoder(x)
                    ID = [] 
                    PC_ID = []
                    for k in range(self.n_clusters):
                       ID.append(computeID(x_emb[y_pred==k]))
                       PC_ID.append(computePC_ID(x_emb[y_pred==k]))
                    ID = np.asarray(ID)
                    PC_ID = np.asarray(PC_ID)
                    ID_global_mean = np.mean(ID)
                    ID_local_mean = np.mean(ID, axis=1) 
                    ID_global_error = np.std(ID)
                    ID_local_error = np.std(ID, axis=1)
                    PC_ID_mean = np.mean(PC_ID)
                    FT = PC_ID_mean - ID_global_mean
                    #print ("FT", FT)
                    print ("ID_local_mean = ", ID_local_mean)
                    print ("ID_global_mean = ", ID_global_mean)
        #End New part
                    print('Iter %d: acc=%.5f, nmi=%.5f, loss_aci_ae=%.5f, loss_disc=%.5f' % (ite, acc, nmi, avg_loss_aci_ae, avg_loss_disc))
                   # logdict = dict(iter=ite, acc=acc, nmi=nmi, loss_aci_ae=avg_loss_aci_ae, loss_disc=avg_loss_disc)
                    logdict = dict(iter=ite, acc=acc, nmi=nmi, loss_aci_ae=avg_loss_aci_ae, loss_disc=avg_loss_disc,ID_global_mean=ID_global_mean,ID_local_mean=ID_local_mean,PC_ID_mean=PC_ID_mean,ID_global_error=ID_global_error,ID_local_error=ID_local_error,FT=FT )
                    logwriter.writerow(logdict)
                    logfile.flush()
            
            #Save interval
            if ite % save_interval == 0:
                self.ae.save_weights(save_dir + '/' + self.dataset + '/pretrain/ae_weights.h5')
                self.critic.save_weights(save_dir + '/' + self.dataset + '/pretrain/critic_weights.h5')

            #Train on batch
            idx = index_array[index * batch_size: min((index+1) * batch_size, x.shape[0])]
            x_batch = x[idx]
            np.random.shuffle(x_batch)
            x_batch = self.random_transform(x_batch, ws=self.ws, hs=self.hs, rot=self.rot, scale=self.scale) if aug_train else x_batch
            alpha_interp = np.random.uniform(low=0.0, high=1.0, size=[x_batch.shape[0]])
            beta_interp = np.random.uniform(low=0.0, high=1.0, size=None)
            beta_interp = 0.5 - np.abs(beta_interp - 0.5)
            
            x_batch_recons = self.predict_ae(x_batch)
            x_batch_recons_interp = self.predict_i_ae(x_batch, alpha_interp)
            x_batch_mix = np.multiply(beta_interp, x_batch) + np.multiply(1 - beta_interp, x_batch_recons)

            loss1 = self.train_on_batch_aci_ae(x_batch, alpha_interp)
            loss2 = self.train_on_batch_disc(x_batch_recons_interp, x_batch_mix, alpha_interp, np.zeros((x_batch.shape[0],)))

            loss_aci_ae = loss_aci_ae + loss1
            loss_disc = loss_disc + loss2
            index = index + 1 if (index + 1) * batch_size <= x.shape[0] else 0
            #print("acc = ", acc)
            if acc > acc_prev:
                acc_prev = acc
                #iter_max = ite
                #print("ite max = ", iter_max)
                #print("acc_prev2 = ", acc_prev)
                self.ae.save_weights(save_dir + '/' + self.dataset + '/pretrain/ae_weights.h5')
                self.critic.save_weights(save_dir + '/' + self.dataset + '/pretrain/critic_weights.h5')
                print('trained weights are saved to %s/%s/pretrain/ae_weights.h5' % (save_dir, self.dataset))
                print('trained weights are saved to %s/%s/critic_weights.h5' % (save_dir, self.dataset))
                

        logfile.close()
        print('training time: ', time() - t0)
        print('training: ', '-' * 60)
        #print('trained weights are saved to %s/%s/pretrain/ae_weights.h5' % (save_dir, self.dataset))
        #print('trained weights are saved to %s/%s/critic_weights.h5' % (save_dir, self.dataset))
        #print('training: ', '-' * 60)
    
    

    
        
        
    def train_dynAE_New(self, x, y=None, kappa=1, n_clusters=10, n_overclusters=40, maxiter=1e5, batch_size=256, tol=1e-1, validate_interval=140, show_interval=None, save_interval=2800, save_dir=PATH_RESULT, aug_train=True):
    #def train_dynAE_New(self, x, y=None, kappa=3, n_clusters=10, n_overclusters=40, maxiter=1e5, batch_size=256, tol=1e-2, validate_interval=140, show_interval=2000, save_interval=2800, save_dir=PATH_RESULT, aug_train=True):
        #init
        number_of_samples = x.shape[0]
        img_h = int(math.sqrt(x.shape[1]))
        img_w = int(math.sqrt(x.shape[1]))

        #logging file
        import csv, os
        if not os.path.exists(save_dir):
            os.makedirs(save_dir)
        logfile = open(save_dir + '/' + self.dataset + '/cluster/train_dynAE_gamma=' + str(self.gamma) + '_logآNew.csv', 'w')
        #logwriter = csv.DictWriter(logfile, fieldnames=['iter', 'acc', 'nmi', 'ari', 'acc_unconf', 'nmi_unconf', 'acc_conf', 'nmi_conf', 'nb_unconf', 'nb_conf', 'fr', 'fd', 'loss'])
        logwriter = csv.DictWriter(logfile, fieldnames=['iter', 'acc', 'nmi', 'ari', 'acc_unconf', 'nmi_unconf', 'acc_conf', 'nmi_conf', 'nb_unconf', 'nb_conf', 'loss'])
        logwriter.writeheader()

        #intervals config
        print('Begin clustering:', '-' * 60)
        if save_interval is None: 
            save_interval = int(maxiter)  # only save the initial and final model
        print('Save interval ', save_interval)
        if show_interval is None:
            show_interval = int(np.ceil(number_of_samples/batch_size))*20
        print('show interval ', show_interval)  
        
        # Step 1: initialize cluster centers using k-means
        t1 = time()
        print('Initializing cluster centers with k-means.')
        
        #new just for the centroid
        #x_emb = self.predict_encoder(x)
        #km = KMeans(n_clusters=self.n_clusters, n_init=20)
        #y_pred = km.fit_predict(x_emb)
        #new
        
        centers_emb, centers_img, y_pred, _ = self.generate_centers(x, n_clusters)
        overcenters_emb, overcenters_img, _, _ = self.generate_centers(x, n_overclusters)

        # Step 2: beta1 and beta2
        #beta1, beta2 = self.generate_beta(kappa, n_clusters)
        beta1 = 0.
        #beta2 = 0.25
        beta2 = 0.01
        beta1_overclustering = 0.
       # beta2_overclustering = 0.15
        beta2_overclustering = 0.15
        print ('beta1 beta2 b1ove b2ove n_overclusters',beta1,beta2,beta1_overclustering,beta2_overclustering,n_overclusters)            
        # Step 3: deep clustering
        loss = 0
        index = 0
        nb_conf_prev = x.shape[0]
        index_array = np.arange(x.shape[0])
        delta_kappa = 0.1 * kappa
        #sess = tf.keras.backend.get_session()
        #first_time = 1
        validate_interval = 2
        
        print("Centroids : ")
        print("----------------------------------------------------------------------------------")
        draw_centers(n_clusters, centers_img, img_h=img_h, img_w=img_w)
        
        
        
        for ite in range(int(maxiter)):
            print ('range' , range(int(maxiter)))
            if ite % validate_interval == 0:
                x_emb = self.encoder.predict(x)
                q = q_mat(x_emb, centers_emb)
                y_pred = q.argmax(1) 
                avg_loss = loss / validate_interval
                loss = 0.
                if ite > 0:
                    nb_conf_prev = nb_conf 
                nb_unconf, nb_conf = self.compute_nb_conflicted_data(x, centers_emb, beta1, beta2)
                #update centers
                if nb_conf >= nb_conf_prev:
                    #centers_emb, centers_img, _, _ = self.generate_centers(x, n_clusters)
                    print("update centers", nb_conf , nb_conf_prev )
                    beta1 = beta1 - (delta_kappa / n_clusters)
                    beta2 = beta2 - (delta_kappa / n_clusters)
                    #beta1 = 0.5
                    #beta2 = 0.25
                    delta_kappa = 0.1 * kappa
                    kappa = delta_kappa
                    #first_time = 0
                    #print("update confidences")

                if y is not None:
                    #y_mapped = map_vector_to_clusters(y, y_pred)
                    #x_emb = self.predict_encoder(x)
                    #y_encoder, y_autoencoder = generate_supervisory_signals(x_emb, x, centers_emb, centers_img, beta1, beta2)
                    #y_encoder_true = centers_emb[y_mapped]
                    #grad_loss_dynAE = sess.run(self.grad_loss_dynAE, feed_dict={'input_dynAE:0': x, 'target1_dynAE:0': y_encoder, 'target2_dynAE:0': y_autoencoder})
                    #grad_loss_pseudo_supervised = sess.run(self.grad_loss_pseudo_supervised, feed_dict={'input_dynAE:0': x, 'target1_dynAE:0': y_encoder})
                    #grad_loss_self_supervised = sess.run(self.grad_loss_self_supervised, feed_dict={'input_dynAE:0': x, 'target2_dynAE:0': y_autoencoder})
                    #grad_loss_supervised = sess.run(self.grad_loss_supervised, feed_dict={'input_dynAE:0': x, 'target3_dynAE:0': y_encoder_true})
                    
                    acc = np.round(metrics.acc(y, y_pred), 5)
                    nmi = np.round(metrics.nmi(y, y_pred), 5)
                    ari = np.round(metrics.ari(y, y_pred), 5) 
                    #fr = np.round(metrics.cos_grad(grad_loss_supervised, grad_loss_dynAE), 5)
                    #fd = np.round(metrics.cos_grad(grad_loss_self_supervised, grad_loss_pseudo_supervised), 5)
                    acc_unconf, nmi_unconf, acc_conf, nmi_conf = self.compute_acc_and_nmi_conflicted_data(x, y, centers_emb, beta1, beta2)
                    #logdict = dict(iter=ite, acc=acc, nmi=nmi, ari=ari, acc_unconf=acc_unconf, nmi_unconf=nmi_unconf, acc_conf=acc_conf, nmi_conf=nmi_conf, nb_unconf=nb_unconf, nb_conf=nb_conf, fr=fr, fd=fd, loss=avg_loss)
                    logdict = dict(iter=ite, acc=acc, nmi=nmi, ari=ari, acc_unconf=acc_unconf, nmi_unconf=nmi_unconf, acc_conf=acc_conf, nmi_conf=nmi_conf, nb_unconf=nb_unconf, nb_conf=nb_conf, loss=avg_loss)
                    logwriter.writerow(logdict)
                    logfile.flush()
                    #print('Iter %d: acc=%.5f, nmi=%.5f, ari=%.5f, acc_unconf=%.5f, nmi_unconf=%.5f, acc_conf=%.5f, nmi_conf=%.5f, nb_unconf=%d, nb_conf=%d, fr=%.5f, fd=%.5f, loss=%.5f' % (ite, acc, nmi, ari, acc_unconf, nmi_unconf, acc_conf, nmi_conf, nb_unconf, nb_conf, fr, fd, avg_loss))
                    print('Iter %d: acc=%.5f, nmi=%.5f, ari=%.5f, acc_unconf=%.5f, nmi_unconf=%.5f, acc_conf=%.5f, nmi_conf=%.5f, nb_unconf=%d, nb_conf=%d, loss=%.5f' % (ite, acc, nmi, ari, acc_unconf, nmi_unconf, acc_conf, nmi_conf, nb_unconf, nb_conf, avg_loss))
                    print("The number of unconflicted data points is : " + str(nb_unconf))
                    print("The number of conflicted data points is : " + str(nb_conf))
                else:
                    x_embed = self.encoder.predict(x)
                    silhouette_avg = silhouette_score(x_embed, y_pred) 
                    print("For number of clusters = " +  str(self.n_clusters) + " The average silhouette_score is :", silhouette_avg) 
                    #logdict = dict(iter=ite, silhouette_avg=silhouette_avg, loss=avg_loss)
                    #logwriter.writerow(logdict)
                    #logfile.flush()
                if(nb_conf / x.shape[0]) < tol:
                    logfile.close()
                    break

            if ite % show_interval == 0:
               
                print("")
                print("----------------------------------------------------------------------------------")
                print("Centroids : ")
                print("----------------------------------------------------------------------------------")
               # draw_centers(n_clusters, centers_img, img_h=img_h, img_w=img_w)
                
                # save intermediate model
            
            if ite % save_interval == 0:
                
                z = self.predict_encoder(x)
                q1 = q_mat(z, centers_emb)
                y1_pred = q1.argmax(1)
               

                pca = PCA(n_components=2).fit(z)
                z_2d = pca.transform(z)
                centers_2d = pca.transform(centers_emb)

                # save states for visualization
                np.save(self.visualisation_dir + '/embeddings/' + self.dataset + '/vis_' + str(ite) + '.npy', {'z_2d': z_2d, 'centers_2d': centers_2d, 'y_pred': y1_pred})

                print('saving model to: ', save_dir + '/' + self.dataset + '/cluster/ae_' + str(ite) + '_weights.h5')
                self.ae.save_weights(save_dir + '/' + self.dataset + '/cluster/ae_' + str(ite) + '_weights.h5')

            # train on batch
            if ite % 2 == 1:
                idx = index_array[index * batch_size: min((index+1) * batch_size, x.shape[0])]
                X_img = x[idx]
                X_emb = self.predict_encoder(X_img)
                Y_encoder, Y_autoencoder = generate_supervisory_signals(X_emb, X_img, centers_emb, centers_img, beta1, beta2)
                X_transformed = self.random_transform(X_img, ws=self.ws, hs=self.hs, rot=self.rot, scale=self.scale) if aug_train else X_img
                losses = self.train_on_batch_dynAE(X_transformed, Y_encoder, Y_autoencoder)
                loss = loss + losses
                index = index + 1 if (index + 1) * batch_size <= x.shape[0] else 0
            else:
                idx = index_array[index * batch_size: min((index+1) * batch_size, x.shape[0])]
                X_img = x[idx]
                X_emb = self.predict_encoder(X_img)
                Y_encoder, Y_autoencoder = generate_supervisory_signals(X_emb, X_img, overcenters_emb, overcenters_img, beta1_overclustering, beta2_overclustering)
                X_transformed = self.random_transform(X_img, ws=self.ws, hs=self.hs, rot=self.rot, scale=self.scale) if aug_train else X_img
                losses = self.train_on_batch_dynAE(X_transformed, Y_encoder, Y_autoencoder)
                loss = loss + losses
                index = index + 1 if (index + 1) * batch_size <= x.shape[0] else 0
                
        logfile.close()
        print('saving model to:', save_dir + '/' + self.dataset + '/cluster/ae_weights.h5')
        self.ae.save_weights(save_dir + '/' + self.dataset + '/cluster/ae_weights.h5')
        print('Clustering time: %ds' % (time() - t1))
        print('End clustering:', '-' * 60)

        return y_pred
        
        
 
     
    def train_dynAE(self, x, y=None, kappa=3, n_clusters=10, maxiter=1e5, batch_size=256, tol=1e-2, validate_interval=140, show_interval=None, save_interval=2800, save_dir=PATH_RESULT, aug_train=True):
        #init
        number_of_samples = x.shape[0]
        img_h = int(math.sqrt(x.shape[1]))
        img_w = int(math.sqrt(x.shape[1]))

        #logging file
        import csv, os
        if not os.path.exists(save_dir):
            os.makedirs(save_dir)
        logfile = open(save_dir + '/' + self.dataset + '/cluster/train_dynAE_gamma=' + str(self.gamma) + '_log.csv', 'w')
        #logwriter = csv.DictWriter(logfile, fieldnames=['iter', 'acc', 'nmi', 'ari', 'acc_unconf', 'nmi_unconf', 'acc_conf', 'nmi_conf', 'nb_unconf', 'nb_conf', 'fr', 'fd', 'loss'])
        logwriter = csv.DictWriter(logfile, fieldnames=['iter', 'acc', 'nmi', 'ari', 'acc_unconf', 'nmi_unconf', 'acc_conf', 'nmi_conf', 'nb_unconf', 'nb_conf', 'loss'])
        logwriter.writeheader()

        #intervals config
        print('Begin clustering:', '-' * 60)
        if save_interval is None: 
            save_interval = int(maxiter)  # only save the initial and final model
        print('Save interval ', save_interval)
        if show_interval is None:
            show_interval = int(np.ceil(number_of_samples/batch_size))*20
        print('show interval ', show_interval)  

        # Step 1: initialize cluster centers using k-means
        t1 = time()
        print('Initializing cluster centers with k-means.')
        centers_emb, centers_img, y_pred, _ = self.generate_centers(x, n_clusters)

        # Step 2: beta1 and beta2
        #beta1, beta2 = self.generate_beta(kappa, n_clusters)
        beta1 = 0.
        beta2 = 0.3
        # Step 3: deep clustering
        loss = 0
        index = 0
        nb_conf_prev = x.shape[0]
        index_array = np.arange(x.shape[0])
        delta_kappa = 0.3 * kappa
        #sess = tf.keras.backend.get_session()
        first_time = 1
        for ite in range(int(maxiter)):
            if ite % validate_interval == 0:
                x_emb = self.encoder.predict(x)
                q = q_mat(x_emb, centers_emb)
                y_pred = q.argmax(1) 
                avg_loss = loss / validate_interval
                loss = 0.
                if ite > 0:
                    nb_conf_prev = nb_conf 
                nb_unconf, nb_conf = self.compute_nb_conflicted_data(x, centers_emb, beta1, beta2)
                #update centers
                if nb_conf >= nb_conf_prev and first_time == 1:
                    centers_emb, centers_img, _, _ = self.generate_centers(x, n_clusters)
                    #print("update centers")
                    beta1 = beta1 - (delta_kappa / n_clusters)
                    beta2 = beta2 - (delta_kappa / n_clusters)
                    delta_kappa = 0.3 * kappa
                    kappa = delta_kappa
                    first_time = 0
                    #print("update confidences")

                if y is not None:
                    y_mapped = map_vector_to_clusters(y, y_pred)
                    x_emb = self.predict_encoder(x)
                    y_encoder, y_autoencoder = generate_supervisory_signals(x_emb, x, centers_emb, centers_img, beta1, beta2)
                    y_encoder_true = centers_emb[y_mapped]
                    #grad_loss_dynAE = sess.run(self.grad_loss_dynAE, feed_dict={'input_dynAE:0': x, 'target1_dynAE:0': y_encoder, 'target2_dynAE:0': y_autoencoder})
                    #grad_loss_pseudo_supervised = sess.run(self.grad_loss_pseudo_supervised, feed_dict={'input_dynAE:0': x, 'target1_dynAE:0': y_encoder})
                    #grad_loss_self_supervised = sess.run(self.grad_loss_self_supervised, feed_dict={'input_dynAE:0': x, 'target2_dynAE:0': y_autoencoder})
                    #grad_loss_supervised = sess.run(self.grad_loss_supervised, feed_dict={'input_dynAE:0': x, 'target3_dynAE:0': y_encoder_true})
                    acc = np.round(metrics.acc(y, y_pred), 5)
                    nmi = np.round(metrics.nmi(y, y_pred), 5)
                    ari = np.round(metrics.ari(y, y_pred), 5) 
                    #fr = np.round(metrics.cos_grad(grad_loss_supervised, grad_loss_dynAE), 5)
                    #fd = np.round(metrics.cos_grad(grad_loss_self_supervised, grad_loss_pseudo_supervised), 5)
                    acc_unconf, nmi_unconf, acc_conf, nmi_conf = self.compute_acc_and_nmi_conflicted_data(x, y, centers_emb, beta1, beta2)
                    #logdict = dict(iter=ite, acc=acc, nmi=nmi, ari=ari, acc_unconf=acc_unconf, nmi_unconf=nmi_unconf, acc_conf=acc_conf, nmi_conf=nmi_conf, nb_unconf=nb_unconf, nb_conf=nb_conf, fr=fr, fd=fd, loss=avg_loss)
                    logdict = dict(iter=ite, acc=acc, nmi=nmi, ari=ari, acc_unconf=acc_unconf, nmi_unconf=nmi_unconf, acc_conf=acc_conf, nmi_conf=nmi_conf, nb_unconf=nb_unconf, nb_conf=nb_conf, loss=avg_loss)
                    logwriter.writerow(logdict)
                    logfile.flush()
                    #print('Iter %d: acc=%.5f, nmi=%.5f, ari=%.5f, acc_unconf=%.5f, nmi_unconf=%.5f, acc_conf=%.5f, nmi_conf=%.5f, nb_unconf=%d, nb_conf=%d, fr=%.5f, fd=%.5f, loss=%.5f' % (ite, acc, nmi, ari, acc_unconf, nmi_unconf, acc_conf, nmi_conf, nb_unconf, nb_conf, fr, fd, avg_loss))
                    print('Iter %d: acc=%.5f, nmi=%.5f, ari=%.5f, acc_unconf=%.5f, nmi_unconf=%.5f, acc_conf=%.5f, nmi_conf=%.5f, nb_unconf=%d, nb_conf=%d, loss=%.5f' % (ite, acc, nmi, ari, acc_unconf, nmi_unconf, acc_conf, nmi_conf, nb_unconf, nb_conf, avg_loss))
                    print("The number of unconflicted data points is : " + str(nb_unconf))
                    print("The number of conflicted data points is : " + str(nb_conf))
                else:
                    x_embed = self.encoder.predict(x)
                    silhouette_avg = silhouette_score(x_embed, y_pred) 
                    print("For number of clusters = " +  str(self.n_clusters) + " The average silhouette_score is :", silhouette_avg) 
                    #logdict = dict(iter=ite, silhouette_avg=silhouette_avg, loss=avg_loss)
                    #logwriter.writerow(logdict)
                    #logfile.flush()
                if(nb_conf / x.shape[0]) < tol:
                    logfile.close()
                    break

            if ite % show_interval == 0 and ite!=0:
                print("")
                print("----------------------------------------------------------------------------------")
                print("Centroids : ")
                print("----------------------------------------------------------------------------------")
                draw_centers(n_clusters, centers_img, img_h=img_h, img_w=img_w)
                
            # save intermediate model
            if ite % save_interval == 0:
                print("")
                print("----------------------------------------------------------------------------------")
                print("Save embeddings for visualization : ")
                print("----------------------------------------------------------------------------------")
                z = self.predict_encoder(x)
                q1 = q_mat(z, centers_emb)
                y1_pred = q1.argmax(1)

                pca = PCA(n_components=2).fit(z)
                z_2d = pca.transform(z)
                centers_2d = pca.transform(centers_emb)

                # save states for visualization
                np.save(self.visualisation_dir + '/embeddings/' + self.dataset + '/vis_' + str(ite) + '.npy', {'z_2d': z_2d, 'centers_2d': centers_2d, 'y_pred': y1_pred})

                print('saving model to: ', save_dir + '/' + self.dataset + '/cluster/ae_' + str(ite) + '_weights.h5')
                self.ae.save_weights(save_dir + '/' + self.dataset + '/cluster/ae_' + str(ite) + '_weights.h5')

            # train on batch
            idx = index_array[index * batch_size: min((index+1) * batch_size, x.shape[0])]
            X_img = x[idx]
            X_emb = self.predict_encoder(X_img)

            Y_encoder, Y_autoencoder = generate_supervisory_signals(X_emb, X_img, centers_emb, centers_img, beta1, beta2)
            X_transformed = self.random_transform(X_img, ws=self.ws, hs=self.hs, rot=self.rot, scale=self.scale) if aug_train else X_img
            losses = self.train_on_batch_dynAE(X_transformed, Y_encoder, Y_autoencoder)
            loss = loss + losses
            index = index + 1 if (index + 1) * batch_size <= x.shape[0] else 0

        # save the trained model
        logfile.close()
        print('saving model to:', save_dir + '/' + self.dataset + '/cluster/ae_weights.h5')
        self.ae.save_weights(save_dir + '/' + self.dataset + '/cluster/ae_weights.h5')
        print('Clustering time: %ds' % (time() - t1))
        print('End clustering:', '-' * 60)

        return y_pred
        
    def train_second_phase(self, x, y=None, kappa=3, n_clusters=7, maxiter=1e5, batch_size=256, tol=1e-1, validate_interval=140, show_interval=None, save_interval=2800, save_dir=PATH_RESULT, aug_train=True):
        

        number_of_samples = x.shape[0]
        img_h = int(math.sqrt(x.shape[1]))
        img_w = int(math.sqrt(x.shape[1]))
        print ('Num of sample', number_of_samples )
        print('img_h  ',  img_h)
        print ('img_w', img_w )
        #logging file
        import csv, os
        if not os.path.exists(save_dir):
            os.makedirs(save_dir)
        logfile = open(save_dir + '/' + self.dataset + '/phase_2/train_dynAE_gamma=' + str(self.gamma) + '_log.csv', 'w')
        logfile1 = open(save_dir + '/' + self.dataset + '/phase_2/train_dynAE_gamma=' + str(self.gamma) + '_log_MAX.csv', 'w')
        #logwriter = csv.DictWriter(logfile, fieldnames=['iter', 'acc', 'nmi', 'ari', 'acc_unconf', 'nmi_unconf', 'acc_conf', 'nmi_conf', 'nb_unconf', 'nb_conf', 'fr', 'fd', 'loss'])
        logwriter = csv.DictWriter(logfile, fieldnames=['iter', 'acc', 'nmi', 'ari', 'loss', 'trueN', 'FalseN', 'trueAF', 'FalseAF', 'accN','accF', 'ID_global_mean', 'ID_local_mean', 'PC_ID_mean', 'ID_global_error', 'ID_local_error', 'FT'])
        logwriter1 = csv.DictWriter(logfile1, fieldnames=['iter', 'accMax','nmi'])
        logwriter.writeheader()
        logwriter1.writeheader()
        #intervals config
        print('begin clustering:', '-' * 60)
        if save_interval is None: 
            save_interval = int(maxiter)  # only save the initial and final model
        print('Save interval ', save_interval)
        if show_interval is None:
            show_interval = int(np.ceil(number_of_samples/batch_size))*20
        print('show interval ', show_interval)  

        # Step 1: initialize cluster centers using k-means
        t1 = time()
        #print('Initializing cluster centers with k-means.')
        centers_emb, centers_img, y_pred, _ = self.generate_centers(x, n_clusters)
        
        
        #print("Centroids : ")
        #print("----------------------------------------------------------------------------------")
        #draw_centers(n_clusters, centers_img, img_h=img_h, img_w=img_w)
        
       # dists, nn_idx = torch.topk(x, k=5)
        
        # Step 2: beta1 and beta2
        #beta1, beta2 = self.generate_beta(kappa, n_clusters)
    
        # Step 3: deep clustering
        loss = 0
        index = 0
        nb_conf_prev = x.shape[0]
        index_array = np.arange(x.shape[0])
        #delta_kappa = 0.3 * kappa
        #sess = tf.keras.backend.get_session()
        first_time = 1
      #  print ('index_array', index_array)
        
  #      Alpha = 0.6--  best best 634
    #    Alpha = 0.5   -- best
  #      Beta = 0.6   -- best
     #   Beta = 0.6    best best 634
    
        Alpha = 0.6 
        Beta = 0.6
        
    #   Beta1 = 0.9
        #Beta2 = 0.6
        
      #  Beta2 = 0.2
    #    number_neighbours = 5  --best
        number_neighbours = 5
        #smallest_d = 0.0001
        #d = 0.0001
        #dd = 0
        acc_prev = 0
        acc_prev_sc = 0
       # wcss = []
    #    x_emb = self.encoder.predict(x)
     #   for i in range(1,11):
      #    kmeans = KMeans(n_clusters= i, n_init=20, random_state=0)
       #   kmeans = kmeans.fit(x_emb)
          
        #  wcss.append(kmeans.inertia_)
        #plt.plot(range(1,11), wcss)
        #plt.title('T')
        #plt.xlabel('T')
        #plt.ylabel('T')
        #plt.show()
        
        print('N_B1_B2_Alpha', number_neighbours, Beta, Alpha)
        accuracy_unconflicted_point = 0
        accuracy_conflicted_point = 0
        unconflicted_point = 0
        conflicted_point = 0
        
        for ite in range(int(maxiter)):
            if ite % validate_interval == 0:
                x_emb = self.encoder.predict(x)
                centers_embs = x_emb
                nearest_neighbours = NearestNeighbors(n_neighbors=number_neighbours, algorithm='ball_tree').fit(x_emb)
                _, indices = nearest_neighbours.kneighbors(x_emb)
                x_emb_neighbours = x_emb[indices]
                
                # Selecting the unconflicted points
                unconflicted_point_indices = []
                conflicted_point_indices = []
                for i in range(len(x_emb)):
                    biggest_d = 0
                    smallest_d = 10000
                    smallest_dd = 0
                    for j in range(number_neighbours - 1):
                        d = distance.euclidean(x_emb[i], x_emb_neighbours[i, j + 1, :])
                        if biggest_d < d:
                            biggest_d = d
                        if smallest_d > d:
                            smallest_d = d
                    if (smallest_d / biggest_d) > Alpha:
                        unconflicted_point_indices.append(i)
                    else:
                        conflicted_point_indices.append(i)
                print('unconflicted_point_indices ', len(unconflicted_point_indices))
                print('conflicted_point_indices ', len(conflicted_point_indices))
                unconflicted_point = len(unconflicted_point_indices)
                conflicted_point = len(conflicted_point_indices)
                #smallest_d = 0.0001       
                smallest_dd = 0
                for i in range(len(x_emb)):
                    sum_neighbours = 0
                    number_nn = 0
                  #  smallest_dd = 0
                    if i in unconflicted_point_indices:
                        for j in range(number_neighbours - 1):
                            d = distance.euclidean(x_emb[i], x_emb_neighbours[i, j + 1, :])
                            #print ('d', d)
                            if smallest_d > d:
                                smallest_d = d 
                            dd = d - smallest_d
                            #print ('smallest_d', smallest_d)
                            #dd = smallest_d / d
                            #print ('dd', dd)
                           # if smallest_dd < d:
                            #    smallest_dd = d
                            #else:
                            #    smallest_dd = smallest_dd 
                            #dd = smallest_dd / d
                            # dd = d  /  (smallest_d * smallest_d )
                           
                           # print ('dd ', dd)
                            if dd < Beta:
                                #if dd <= Beta2:
                                sum_neighbours = sum_neighbours + x_emb_neighbours[i, j + 1, :] 
                                number_nn = number_nn + 1
                        if number_nn != 0:
                            centers_embs[i] = sum_neighbours / number_nn
                        else:
                            centers_embs[i] = x_emb[i]
               
                x_emb = self.predict_encoder(x)
                #print ('x_emb' , x_emb )
                #x_emb_u = self.predict_encoder(unconflicted_point_indices)
               # print ('x_emb' , x_emb )
               # print ('unconflicted_point_indices' , unconflicted_point_indices )
                accuracy_unconflicted_point = unconflicted_point / (unconflicted_point + conflicted_point)
                accuracy_conflicted_point = conflicted_point / (unconflicted_point + conflicted_point)
                print ('accuracy_unconflicted_point : ', accuracy_unconflicted_point)
                print ('accuracy_conflicted_point : ', accuracy_conflicted_point)
                
                #print ('x_emb_u' , x_emb_u )
                km = KMeans(n_clusters=self.n_clusters, n_init=20)
                y_pred = km.fit_predict(x_emb)
                #print ('ypred' , y_pred )
                #print('uncon00')
                
                
                #y_pred_sc = DBSCAN(eps=0.25, min_samples=9).fit_predict(x_emb)
              #  y_pred_sc= Birch(branching_factor = 50, n_clusters = self.n_clusters, threshold = 1.5).fit_predict(x_emb)
                #y_pred_sc= GaussianMixture(self.n_clusters, covariance_type='full', random_state=0).fit_predict(x_emb)
               # y_pred_sc = SpectralClustering(n_clusters=self.n_clusters).fit_predict(x_emb)
                #y_pred_sc = AgglomerativeClustering(n_clusters=self.n_clusters, affinity='euclidean', linkage='ward').fit_predict(x_emb)
                avg_loss = loss / validate_interval
                loss = 0
               # acc_prev = 0
                acc = 0
                if y is not None:
                 
                    km = KMeans(n_clusters=self.n_clusters, n_init=20)
                #km = kmedoids(n_clusters=self.n_clusters, n_init=20)
                    y_pred = km.fit_predict(x_emb)
                    
                   
                  #y_pred_sc = DBSCAN(eps=0.25, min_samples=9).fit_predict(x_emb)
                  #  y_pred_sc = SpectralClustering(n_clusters=self.n_clusters).fit_predict(x_emb)
                   # y_pred_sc= Birch(branching_factor = 50, n_clusters = self.n_clusters, threshold = 1.5).fit_predict(x_emb)
                   # y_pred_sc= GaussianMixture(self.n_clusters, covariance_type='full', random_state=0).fit_predict(x_emb)
                    #y_pred_sc = AgglomerativeClustering(n_clusters=self.n_clusters, affinity='euclidean', linkage='ward').fit_predict(x_emb)
# ##  New Part                    
                    true_neighbours = 0
                    false_neighbours = 0
                    true_neighbours_after_filtering = 0
                    false_neighbours_after_filtering = 0
                    #smallest_d = 0.0001
                    smallest_dd = 0
                    
                    # Selecting the neighbours of the unconflicted points
                    for i in range(len(x_emb)):
                      #  biggest_d = 0
                      #  smallest_d = 10000
                    #    smallest_dd = 0
                        
                        if i in unconflicted_point_indices:
                            for j in range(number_neighbours - 1):
                                d = distance.euclidean(x_emb[i], x_emb_neighbours[i, j + 1, :])
                                if smallest_d > d:
                                    smallest_d = d 
                                dd = d - smallest_d    
                               # print ('dd : ', dd)
                                #if smallest_dd < d:
                                 #   smallest_dd = d
                                  #  print ('smallest_dd d ' , smallest_dd)
                                #else:
                                 #   smallest_dd = smallest_dd 
                                   # print ('smallest_dd smallest_dd' , smallest_dd)
                                #dd = smallest_dd   /  d
                                    #print ('ddevaluation', dd)
                            
                                if dd < Beta:
                                    #if dd <= Beta2:
                                    if y[i] == y[indices[i, j]]:
                                        true_neighbours_after_filtering = true_neighbours_after_filtering + 1
                                    else:
                                        false_neighbours_after_filtering = false_neighbours_after_filtering + 1
                                
                                if y[i] == y[indices[i, j]]:
                                    true_neighbours = true_neighbours + 1
                                else: 
                                    false_neighbours = false_neighbours + 1
                        
                    
                                    
                    #if prev_acc <= acc: 
                    if (true_neighbours_after_filtering + false_neighbours_after_filtering) != 0:
                        acc_neighbours = true_neighbours / (false_neighbours + true_neighbours)
                        acc_filtered_neighbours = true_neighbours_after_filtering / (true_neighbours_after_filtering + false_neighbours_after_filtering)
                    else:
                        acc_neighbours = 0
                        acc_filtered_neighbours = 0
                    
                    
                   # acc_filtered_neighbours = true_neighbours_after_filtering / (true_neighbours_after_filtering + false_neighbours_after_filtering) 
                    print ('true_neighbours : ', true_neighbours)
                    print ('false_neighbours : ', false_neighbours)
                    print ('true_neighbours_after_filtering : ', true_neighbours_after_filtering)
                    print ('false_neighbours_after_filtering : ', false_neighbours_after_filtering)
                    print ('acc_neighbours : ', acc_neighbours)
                    print ('acc_filtered_neighbours : ', acc_filtered_neighbours)
                   
                    km = KMeans(n_clusters=self.n_clusters, n_init=20)
                    y_pred = km.fit_predict(x_emb)
                    
                    
                    
                    #y_pred_sc = DBSCAN(eps=0.25, min_samples=9).fit_predict(x_emb)
                #    y_pred_sc = SpectralClustering(n_clusters=self.n_clusters).fit_predict(x_emb)
                 #   y_pred_sc= Birch(branching_factor = 50, n_clusters = self.n_clusters, threshold = 1.5).fit_predict(x_emb)
                  #  y_pred_sc= GaussianMixture(self.n_clusters, covariance_type='full', random_state=0).fit_predict(x_emb)
                  #  y_pred_sc = AgglomerativeClustering(n_clusters=self.n_clusters, affinity='euclidean', linkage='ward').fit_predict(x_emb)
                    #y_mapped = map_vector_to_clusters(y, y_neighbours)
                   
                    ## End of new part
                    acc = np.round(metrics.acc(y, y_pred), 5)
                    nmi = np.round(metrics.nmi(y, y_pred), 5)
                    ari = np.round(metrics.ari(y, y_pred), 5)
                    
                 
                  #  acc_sc = np.round(metrics.acc(y, y_pred_sc), 5)
                  #  nmi_sc = np.round(metrics.nmi(y, y_pred_sc), 5)
                #    ari_sc = np.round(metrics.ari(y, y_pred_sc), 5)
                    
                   # acc_prev = acc
                    ID = [] 
                    PC_ID = []
                    for k in range(self.n_clusters):
                      ID.append(computeID(x_emb[y_pred==k]))
                      PC_ID.append(computePC_ID(x_emb[y_pred==k]))
                    ID = np.asarray(ID)
                    PC_ID = np.asarray(PC_ID)
                    ID_global_mean = np.mean(ID)
                    ID_local_mean = np.mean(ID, axis=1) 
                    ID_global_error = np.std(ID)
                    ID_local_error = np.std(ID, axis=1)
                    PC_ID_mean = np.mean(PC_ID)
                    FT = PC_ID_mean - ID_global_mean
                    #print ("FT", FT)
                    print ("ID_local_mean = ", ID_local_mean)
                    print ("ID = ", ID_global_mean)
                    print ("LID = ", PC_ID_mean)
                    
                    
                    logdict = dict(iter=ite, acc=acc, nmi=nmi, ari=ari,  loss=avg_loss, trueN=true_neighbours, FalseN=false_neighbours, trueAF=true_neighbours_after_filtering, FalseAF=false_neighbours_after_filtering,  accN=acc_neighbours, accF=acc_filtered_neighbours, ID_global_mean=ID_global_mean, ID_local_mean=ID_local_mean.tolist(), PC_ID_mean=PC_ID_mean, ID_global_error=ID_global_error, ID_local_error=ID_local_error.tolist(), FT=FT)
                    logwriter.writerow(logdict)
                    logfile.flush()
                    
                    print('K-means: iter %d, acc=%.5f, nmi=%.5f, ari=%.5f, loss=%.5f' % (ite, acc, nmi, ari,  avg_loss))
                #    print('Spectral clustering: iter %d, acc=%.5f, nmi=%.5f, ari=%.5f, loss=%.5f' % (ite, acc_sc, nmi_sc, ari_sc, avg_loss))
                  
                    #print("")
                    #print("----------------------------------------------------------------------------------")
                    print("Centroids : ")
                    print("----------------------------------------------------------------------------------")
                    centers_emb, centers_img, y_pred, _ = self.generate_centers(x, n_clusters)
                    draw_centers(n_clusters, centers_img, img_h=img_h, img_w=img_w)
                    print('out')     
                else:
                    x_embed = self.encoder.predict(x)
                    silhouette_avg = silhouette_score(x_embed, y_pred) 
                    print("For number of clusters = " +  str(self.nearest_neighbours) + " The average silhouette_score is :", silhouette_avg) 
         
            idx = index_array[index * batch_size: min((index+1) * batch_size, x.shape[0])]
           
            X_img = x[idx]
            X_emb = self.predict_encoder(X_img)
            Y_encoder = centers_embs[idx]
            Y_autoencoder = self.decoder.predict(centers_embs[idx])
            #Y_autoencoder = self.decoder.predict(X_emb)
            X_transformed = self.random_transform(X_img, ws=self.ws, hs=self.hs, rot=self.rot, scale=self.scale) if aug_train else X_img
           
            losses = self.train_on_batch_dynAE(X_transformed, Y_encoder, Y_autoencoder)
           
   
            loss = loss + losses
            index = index + 1 if (index + 1) * batch_size <= x.shape[0] else 0

          #  print('acc', acc)
           # print('acc_prev1', acc_prev)
            
            if acc > acc_prev:
                acc_prev = acc
           #     print('acc_prev2', acc_prev)
                print('saving model to ::', save_dir + '/' + self.dataset + '/phase_2/ae_weights.h5')
                self.ae.save_weights(save_dir + '/' + self.dataset + '/phase_2/ae_weights.h5')
                logdict1 = dict(iter=ite, accMax=acc_prev, nmi=nmi)
                logwriter1.writerow(logdict1)
                logfile1.flush()
         #   if acc_sc > acc_prev:
          #      acc_prev_sc = acc_sc
           #     print('saving model to ::', save_dir + '/' + self.dataset + '/phase_2/ae_weights.h5')
            #    self.ae.save_weights(save_dir + '/' + self.dataset + '/phase_2/ae_weights.h5')
             #   logdict1 = dict(iter=ite, accMax=acc_prev, nmi_sc=nmi_sc)
              #  logwriter1.writerow(logdict1)
               # logfile1.flush()
        
    
                
                print('Clustering time: %ds' % (time() - t1))
                print('End clustering:', '-' * 60)
                print("Centroids : ")
                print("----------------------------------------------------------------------------------")
                centers_emb, centers_img, y_pred, _ = self.generate_centers(x, n_clusters)
                draw_centers(n_clusters, centers_img, img_h=img_h, img_w=img_w)
        logfile.close()
       # print('training time: ', time() - t0)
    #    print('training: ', '-' * 60)
        print('Clustering time: %ds' % (time() - t1))
        print('End clustering:', '-' * 60)
        print("Centroids : ")
        print("----------------------------------------------------------------------------------")
        centers_emb, centers_img, y_pred, _ = self.generate_centers(x, n_clusters)
        draw_centers(n_clusters, centers_img, img_h=img_h, img_w=img_w)
        
        return y_pred
    
    
    
    #  breastmnist
# pretraining 
    
    def train_ae_breastmnist(self, x, y=None, optimizer='adam', epochs=10, batch_size=50, save_dir=PATH_RESULT, verbose=1, aug_train=True):
    #def train_ae_bloodmnist(self, x, y=None, optimizer='adam', epochs=500, batch_size=256, save_dir=PATH_RESULT, verbose=1, aug_train=True):
        #print('Begin pretraining: breastmnist', '-' * 60)
        self.ae.compile(optimizer=optimizer, loss='mse')
        #csv_logger = callbacks.CSVLogger(save_dir + '/' + self.dataset + '/pretrain/log_ae.csv')
        #cb = [csv_logger]
        #Prepare log file
        import csv, os
        if not os.path.exists(save_dir):
            os.makedirs(save_dir)
        logfile = open(save_dir + '/' + self.dataset + '/pretrain/pretrain.csv', 'w')
        logwriter = csv.DictWriter(logfile, fieldnames=['iter', 'ID_global_mean', 'ID_local_mean', 'PC_ID_mean', 'ID_global_error', 'ID_local_error', 'FT','acc','nmi'])
        logwriter.writeheader()
       
      
        cb = []
        acc_prev = 0
        
        if y is not None and verbose > 0:
            class PrintACC(callbacks.Callback):
                def __init__(self, x, y, encoder):
                    self.x = x
                    self.y = y
                    self.encoder = encoder
                    super(PrintACC, self).__init__()

                def on_epoch_end(self, epoch, logs=None):
                    if epoch % 20 != 0:
                        return
                    features = self.encoder.predict(self.x)
                    km = KMeans(n_clusters=len(np.unique(y)), n_init=20)
                   
                   
                    y_pred = km.fit_predict(features)
                    
                  
                  #  print(' '*8 + '|==>  acc: %.4f,  nmi: %.4f  <==|'% (metrics.acc(y, y_pred), metrics.nmi(y, y_pred)))
            
            class PrintID_LID(callbacks.Callback):
                def __init__(self, x, y, encoder):
                    self.x = x
                    self.y = y
                    self.encoder = encoder
                    super(PrintID_LID, self).__init__()

                def on_epoch_end(self, epoch, logs=None):
                    features = self.encoder.predict(self.x)
                    km = KMeans(n_clusters=len(np.unique(y)), n_init=20)
                    y_pred = km.fit_predict(features)
                    ID = [] 
                    PC_ID = []
                    for k in range(len(np.unique(y))):
                       ID.append(computeID(features[y_pred==k]))
                       PC_ID.append(computePC_ID(features[y_pred==k]))
                    ID = np.asarray(ID)
                    PC_ID = np.asarray(PC_ID)
                    ID_global_mean = np.mean(ID)
                    ID_local_mean = np.mean(ID, axis=1) 
                    ID_global_error = np.std(ID)
                    ID_local_error = np.std(ID, axis=1)
                    PC_ID_mean = np.mean(PC_ID)
                    FT = PC_ID_mean - ID_global_mean
                   # print ("ID = ", ID_global_mean)
                #    print ("LID = ", PC_ID_mean)
                    logdict = dict(iter=epoch, ID_global_mean=ID_global_mean, ID_local_mean=ID_local_mean, PC_ID_mean=PC_ID_mean, ID_global_error=ID_global_error, ID_local_error=ID_local_error, FT=FT, acc=metrics.acc(y, y_pred) ,nmi=metrics.nmi(y, y_pred) )
                    logwriter.writerow(logdict)
                    logfile.flush()
                    
                    
            cb.append(PrintACC(x, y, self.encoder))
            cb.append(PrintID_LID(x, y, self.encoder))
        # begin pretraining
        t0 = time()
        if not aug_train:
            self.ae.fit(x, y, batch_size=batch_size, epochs=epochs, callbacks=cb, verbose=verbose)
        else:
            
           # print('-=*'*20)
            #print('Using augmentation for ae')
            #print('-=*'*20)
            def gen(x, batch_size):
                if len(x.shape) > 2:  # image
                   
                    gen0 = self.datagen.flow(y, shuffle=True, batch_size=batch_size)
                    while True:
                        batch_x = gen0.next()
                        yield (batch_x, batch_x)
                else:
                   
                    width = int(np.sqrt(x.shape[-1]))
                  
                    
                    if width * width == x.shape[-1]:  # gray
                        
                        im_shape = [-1, width, width, 1]
                       
                    else:  # RGB
                        width = int(np.sqrt(x.shape[-1] / 3.0))
                        width = int(np.sqrt(x.shape[-1] / 3.0))
                      
                        im_shape = [-1, width, width, 3]
                        im_shape = [-1, width, width, 3]
                       
                      
                    xreshape = np.reshape(x, im_shape)
                    gen0 = self.datagen.flow(np.reshape(x, im_shape), shuffle=True, batch_size=batch_size)
               
                  
                    while True:
                     
                        batch_x = gen0.next()
                        batch_x = np.reshape(batch_x, [batch_x.shape[0], x.shape[-1]])
                        yield (batch_x, batch_x)
                       
                       
            self.ae.fit_generator(gen(x, batch_size), steps_per_epoch=int(x.shape[0]/batch_size), epochs=epochs, callbacks=cb, verbose=verbose)
            
        exec_time = time() - t0
        print('Pretraining time: %ds' % (exec_time))
        if y is not None:
            features = self.encoder.predict(x)
            km = KMeans(n_clusters=len(np.unique(y)), n_init=20)
            y_pred = km.fit_predict(features)
            f = open(save_dir + '/' + self.dataset + '/pretrain/results_train_ae.txt', 'w+')
            f.write('|==> ACC: %.4f,  NMI: %.4f  <==| \n' % (metrics.acc(y, y_pred), metrics.nmi(y, y_pred)))
            f.write('|==> Execution time: %.5f <==| \n' % (exec_time))
            f.close()
        logfile.close()
        #if metrics.acc(y, y_pred) > acc_prev:
         #       acc_prev = metrics.acc(y, y_pred)
        self.ae.save_weights(save_dir + '/' + self.dataset + '/pretrain/ae_weights.h5')
        print('Pretrained weights are saved tkk %s/%s/pretrain/ae_weights.h5' % (save_dir, self.dataset))
        print('End pretraining: ', '-' * 60)
        
        
    
      #best def train_second_phase_medmnist(self, x, y=None, kappa=3, n_clusters=2, maxiter=4, batch_size=256, tol=0.0, validate_interval=140, show_interval=None, save_interval=800, save_dir=PATH_RESULT, aug_train=True):
    def train_second_phase_medmnist(self, x, y=None, kappa=3, n_clusters=2, maxiter=4, batch_size=256, tol=0.0, validate_interval=140, show_interval=None, save_interval=800, save_dir=PATH_RESULT, aug_train=True):    
    # def train_second_phase(self, x, y=None, kappa=3, n_clusters=3, maxiter=1e5, batch_size=256, tol=1e-1, validate_interval=140, show_interval=None, save_interval=2800, save_dir=PATH_RESULT, aug_train=True):  --best
                
    #def train_second_phase(self, x, y=None, kappa=3, n_clusters=10, maxiter=10, batch_size=, tol=1e-2, validate_interval=140, show_interval=None, save_interval=2800, save_dir=PATH_RESULT, aug_train=True):
        #init
        number_of_samples = x.shape[0]
        print(number_of_samples)
        img_h = int(math.sqrt(x.shape[0]))
        img_w = int(math.sqrt(x.shape[0]))
        print ('Num of sample', number_of_samples )
        print('img_h  ',  img_h)
        print ('img_w', img_w )
        #logging file
        import csv, os
        if not os.path.exists(save_dir):
            os.makedirs(save_dir)
        logfile = open(save_dir + '/' + self.dataset + '/phase_2/train_dynAE_gamma=' + str(self.gamma) + '_log.csv', 'w')
        logfile1 = open(save_dir + '/' + self.dataset + '/phase_2/train_dynAE_gamma=' + str(self.gamma) + '_log_MAX.csv', 'w')
        #logwriter = csv.DictWriter(logfile, fieldnames=['iter', 'acc', 'nmi', 'ari', 'acc_unconf', 'nmi_unconf', 'acc_conf', 'nmi_conf', 'nb_unconf', 'nb_conf', 'fr', 'fd', 'loss'])
        logwriter = csv.DictWriter(logfile, fieldnames=['iter', 'acc', 'nmi', 'ari', 'loss', 'trueN', 'FalseN', 'trueAF', 'FalseAF', 'accN','accF', 'ID_global_mean', 'ID_local_mean', 'PC_ID_mean', 'ID_global_error', 'ID_local_error', 'FT'])
        logwriter1 = csv.DictWriter(logfile1, fieldnames=['iter', 'accMax','nmi'])
        logwriter.writeheader()
        logwriter1.writeheader()
        #intervals config
        print('begin clustering:', '-' * 60)
        if save_interval is None: 
            save_interval = int(maxiter)  # only save the initial and final model
        print('Save interval ', save_interval)
        if show_interval is None:
            show_interval = int(np.ceil(number_of_samples/batch_size))*20
        print('show interval ', show_interval)  

        # Step 1: initialize cluster centers using k-means
        t1 = time()
        #print('Initializing cluster centers with k-means.')
        centers_emb, centers_img, y_pred, _ = self.generate_centers(x, n_clusters)
       
        
        #print("Centroids : ")
        #print("----------------------------------------------------------------------------------")
        #draw_centers(n_clusters, centers_img, img_h=img_h, img_w=img_w)
        
       # dists, nn_idx = torch.topk(x, k=5)
        
        # Step 2: beta1 and beta2
        #beta1, beta2 = self.generate_beta(kappa, n_clusters)
    
        # Step 3: deep clustering
        loss = 0
        index = 0
        nb_conf_prev = x.shape[0]
        index_array = np.arange(x.shape[0])
        #delta_kappa = 0.3 * kappa
        #sess = tf.keras.backend.get_session()
        first_time = 1
      #  print ('index_array', index_array)
   #      Alpha = 0.6--  best best 634
    #    Alpha = 0.5   -- best
  #      Beta = 0.6   -- best
     #   Beta = 0.6    best best 634
    
       # Alpha = 0.6   --- best medmnisr
    #    Beta = 0.6   --- best medmnit
        
        
        #Alpha = 0.6   breastmnist 73 nmi 4
        #Beta = 0.7    breastmnist 73 nmi 4
        
       ## Alpha = 0.4
    ##    Beta = 0.5
        
        Alpha = 0.6 
        Beta  = 0.6
    #   Beta1 = 0.9
        #Beta2 = 0.6
        
      #  Beta2 = 0.2
    #    number_neighbours = 5  --best
        number_neighbours = 5
        #smallest_d = 0.0001
        #d = 0.0001
        #dd = 0
        acc_prev = 0
        acc_prev_sc = 0
       # batch_size = 256
       # wcss = []
    #    x_emb = self.encoder.predict(x)
     #   for i in range(1,11):
      #    kmeans = KMeans(n_clusters= i, n_init=20, random_state=0)
       #   kmeans = kmeans.fit(x_emb)
          
        #  wcss.append(kmeans.inertia_)
        #plt.plot(range(1,11), wcss)
        #plt.title('T')
        #plt.xlabel('T')
        #plt.ylabel('T')
        #plt.show()
       # validate_interval = 1
        print('N_B1_B2_Alpha', number_neighbours, Beta, Alpha)
        accuracy_unconflicted_point = 0
        accuracy_conflicted_point = 0
        unconflicted_point = 0
        conflicted_point = 0
        
        for ite in range(int(maxiter)):
           
            if ite % validate_interval == 0:
                
               # x_emb = self.encoder.predict(x)
                x_emb = self.predict_encoder(x)
                
                centers_embs = x_emb
                
                centers_embs = centers_embs
               
                x_emb = x_emb.astype(np.float64)
               
                
                nearest_neighbours = NearestNeighbors(n_neighbors=number_neighbours, algorithm='ball_tree').fit(x_emb)
              
                _, indices = nearest_neighbours.kneighbors(x_emb)
                #print('ddd-333')
                x_emb_neighbours = x_emb[indices]
                #print('ddd-3')
                # Selecting the unconflicted points
                unconflicted_point_indices = []
                conflicted_point_indices = []
                for i in range(len(x_emb)):
                    biggest_d = 0
                    smallest_d = 10000
                    smallest_dd = 0
                    for j in range(number_neighbours - 1):
                      #  print('ddd-4')
                        d = distance.euclidean(x_emb[i], x_emb_neighbours[i, j + 1, :])
                        if biggest_d < d:
                            biggest_d = d
                        if smallest_d > d:
                            smallest_d = d
                    if (smallest_d / biggest_d) > Alpha:
                        unconflicted_point_indices.append(i)
                    else:
                        conflicted_point_indices.append(i)
                print('unconflicted_point_indices ', len(unconflicted_point_indices))
                print('conflicted_point_indices ', len(conflicted_point_indices))
                unconflicted_point = len(unconflicted_point_indices)
                conflicted_point = len(conflicted_point_indices)
                #smallest_d = 0.0001       
                smallest_dd = 0
                for i in range(len(x_emb)):
                    sum_neighbours = 0
                    number_nn = 0
                  #  smallest_dd = 0
                    if i in unconflicted_point_indices:
                        for j in range(number_neighbours - 1):
                            d = distance.euclidean(x_emb[i], x_emb_neighbours[i, j + 1, :])
                            #print ('d', d)
                            if smallest_d > d:
                                smallest_d = d 
                            dd = d - smallest_d
                            #print ('smallest_d', smallest_d)
                            #dd = smallest_d / d
                            #print ('dd', dd)
                           # if smallest_dd < d:
                            #    smallest_dd = d
                            #else:
                            #    smallest_dd = smallest_dd 
                            #dd = smallest_dd / d
                            # dd = d  /  (smallest_d * smallest_d )
                           
                           # print ('dd ', dd)
                            if dd < Beta:
                                #if dd <= Beta2:
                                sum_neighbours = sum_neighbours + x_emb_neighbours[i, j + 1, :] 
                                number_nn = number_nn + 1
                        if number_nn != 0:
                            centers_embs[i] = sum_neighbours / number_nn
                        else:
                            centers_embs[i] = x_emb[i]
               
                x_emb = self.predict_encoder(x)
                
                accuracy_unconflicted_point = unconflicted_point / (unconflicted_point + conflicted_point)
                accuracy_conflicted_point = conflicted_point / (unconflicted_point + conflicted_point)
                print ('accuracy_unconflicted_point : ', accuracy_unconflicted_point)
                print ('accuracy_conflicted_point : ', accuracy_conflicted_point)
                
                #print ('x_emb_u' , x_emb_u )
                km = KMeans(n_clusters=self.n_clusters, n_init=20)
                y_pred = km.fit_predict(x_emb)
               
             
                avg_loss = loss / validate_interval
                loss = 0
               # acc_prev = 0
                acc = 0
                if y is not None:
                 
                    km = KMeans(n_clusters=self.n_clusters, n_init=20)
              
                    y_pred = km.fit_predict(x_emb)
             
                    true_neighbours = 0
                    false_neighbours = 0
                    true_neighbours_after_filtering = 0
                    false_neighbours_after_filtering = 0
                   
                    smallest_dd = 0
                    
                    # Selecting the neighbours of the unconflicted points
                    for i in range(len(x_emb)):
                  
                        if i in unconflicted_point_indices:
                            for j in range(number_neighbours - 1):
                                d = distance.euclidean(x_emb[i], x_emb_neighbours[i, j + 1, :])
                                if smallest_d > d:
                                    smallest_d = d 
                                dd = d - smallest_d    
                             
                                if dd < Beta:
                                 
                                    if y[i] == y[indices[i, j]]:
                                        true_neighbours_after_filtering = true_neighbours_after_filtering + 1
                                    else:
                                        false_neighbours_after_filtering = false_neighbours_after_filtering + 1
                                
                                if y[i] == y[indices[i, j]]:
                                    true_neighbours = true_neighbours + 1
                                else: 
                                    false_neighbours = false_neighbours + 1
                        
                    
                                    
                    #if prev_acc <= acc: 
                    if (true_neighbours_after_filtering + false_neighbours_after_filtering) != 0:
                        acc_neighbours = true_neighbours / (false_neighbours + true_neighbours)
                        acc_filtered_neighbours = true_neighbours_after_filtering / (true_neighbours_after_filtering + false_neighbours_after_filtering)
                    else:
                        acc_neighbours = 0
                        acc_filtered_neighbours = 0
                    
                    
                   # acc_filtered_neighbours = true_neighbours_after_filtering / (true_neighbours_after_filtering + false_neighbours_after_filtering) 
                    print ('true_neighbours : ', true_neighbours)
                    print ('false_neighbours : ', false_neighbours)
                    print ('true_neighbours_after_filtering : ', true_neighbours_after_filtering)
                    print ('false_neighbours_after_filtering : ', false_neighbours_after_filtering)
                    print ('acc_neighbours : ', acc_neighbours)
                    print ('acc_filtered_neighbours : ', acc_filtered_neighbours)
                   
                    km = KMeans(n_clusters=self.n_clusters, n_init=20)
                    y_pred = km.fit_predict(x_emb)
                    acc = np.round(metrics.acc(y, y_pred), 5)
                    nmi = np.round(metrics.nmi(y, y_pred), 5)
                    ari = np.round(metrics.ari(y, y_pred), 5)
               
                    ID = [] 
                    PC_ID = []
                    for k in range(self.n_clusters):
                      ID.append(computeID(x_emb[y_pred==k]))
                      PC_ID.append(computePC_ID(x_emb[y_pred==k]))
                    ID = np.asarray(ID)
                    PC_ID = np.asarray(PC_ID)
                    ID_global_mean = np.mean(ID)
                    ID_local_mean = np.mean(ID, axis=1) 
                    ID_global_error = np.std(ID)
                    ID_local_error = np.std(ID, axis=1)
                    PC_ID_mean = np.mean(PC_ID)
                    FT = PC_ID_mean - ID_global_mean
                    #print ("FT", FT)
                    print ("ID_local_mean = ", ID_local_mean)
                    print ("ID = ", ID_global_mean)
                    print ("LID = ", PC_ID_mean)
                    
                    if acc > acc_prev:
                        #acc_prev = acc
                        logdict = dict(iter=ite, acc=acc, nmi=nmi, ari=ari,  loss=avg_loss, trueN=true_neighbours, FalseN=false_neighbours, trueAF=true_neighbours_after_filtering, FalseAF=false_neighbours_after_filtering,  accN=acc_neighbours, accF=acc_filtered_neighbours, ID_global_mean=ID_global_mean, ID_local_mean=ID_local_mean.tolist(), PC_ID_mean=PC_ID_mean, ID_global_error=ID_global_error, ID_local_error=ID_local_error.tolist(), FT=FT)
                        logwriter.writerow(logdict)
                        logfile.flush()
                    
                    print('K-means: iter %d, acc=%.5f, nmi=%.5f, ari=%.5f, loss=%.5f' % (ite, acc, nmi, ari,  avg_loss))
                    print("Centroids : ")
                    print("----------------------------------------------------------------------------------")
                    #centers_emb, centers_img, y_pred, _ = self.generate_centers(x, n_clusters)
                    #draw_centers(n_clusters, centers_img, img_h=img_h, img_w=img_w)
              #      print('out')     
                else:
                    x_embed = self.encoder.predict(x)
                    silhouette_avg = silhouette_score(x_embed, y_pred) 
                    print("For number of clusters = " +  str(self.nearest_neighbours) + " The average silhouette_score is :", silhouette_avg) 
               #     print('x_embeddddd',x_embed) 
            
           
            idx = index_array[index * batch_size: min((index+1) * batch_size, x.shape[0])]
            
          #wit best  idx = index_array[index * 10: min((index+1) * 10, x.shape[0])]
            
            X_img = x[idx]
            
            
            x_emb = self.predict_encoder(x)
            Y_encoder = centers_embs[idx]
           
                        #print('Y_encoder',Y_encoder)
            Y_autoencoder = self.decoder.predict(centers_embs[idx])
            
              # Y_autoencoder = self.decoder.predict(x_emb)
            X_transformed = self.random_transform(X_img, ws=self.ws, hs=self.hs, rot=self.rot, scale=self.scale) if aug_train else X_img
           
            
            losses = self.train_on_batch_dynAE(X_transformed, Y_encoder, Y_autoencoder)
           
            
            
   
            loss = loss + losses
           
       
            
            index = index + 1 if (index + 1) * batch_size <= int(x.shape[0]) else 0
            
        print('saving model to ::', save_dir + '/' + self.dataset + '/phase_2/ae_weights.h5')
        self.ae.save_weights(save_dir + '/' + self.dataset + '/phase_2/ae_weights.h5')
              #  print('save the best acc in W', acc )
        logdict1 = dict(iter=ite, accMax=acc_prev, nmi=nmi)
        logwriter1.writerow(logdict1)
        logfile1.flush()
         #   if acc_sc > acc_prev:
          #      acc_prev_sc = acc_sc
           #     print('saving model to ::', save_dir + '/' + self.dataset + '/phase_2/ae_weights.h5')
            #    self.ae.save_weights(save_dir + '/' + self.dataset + '/phase_2/ae_weights.h5')
             #   logdict1 = dict(iter=ite, accMax=acc_prev, nmi_sc=nmi_sc)
              #  logwriter1.writerow(logdict1)
               # logfile1.flush()
        
    
                
      
                #centers_emb, centers_img, y_pred, _ = self.generate_centers(x, n_clusters)
                #draw_centers(n_clusters, centers_img, img_h=img_h, img_w=img_w)
                
        logfile.close()
       # print('training time: ', time() - t0)
    #    print('training: ', '-' * 60)
        print('Clustering time: %ds' % (time() - t1))
        print('End clustering:', '-' * 60)
        print("Centroids : ")
        print('end')
        print("----------------------------------------------------------------------------------")
     
        
        return y_pred
    
    
    def train_dynAE_New_meadmnist(self, x, y=None, kappa=1, n_clusters=2, n_overclusters=10, maxiter=1e2, batch_size=100, tol=1e-2, validate_interval=140, show_interval=None, save_interval=2800, save_dir=PATH_RESULT, aug_train=True):
    #def train_dynAE_New(self, x, y=None, kappa=3, n_clusters=10, n_overclusters=40, maxiter=1e5, batch_size=256, tol=1e-2, validate_interval=140, show_interval=2000, save_interval=2800, save_dir=PATH_RESULT, aug_train=True):
        #init
        number_of_samples = x.shape[0]
        img_h = int(math.sqrt(x.shape[1]))
        img_w = int(math.sqrt(x.shape[1]))
       
        import csv, os
        if not os.path.exists(save_dir):
            os.makedirs(save_dir)
        logfile = open(save_dir + '/' + self.dataset + '/cluster/train_dynAE_gamma=' + str(self.gamma) + '_logآNew.csv', 'w')
        #logwriter = csv.DictWriter(logfile, fieldnames=['iter', 'acc', 'nmi', 'ari', 'acc_unconf', 'nmi_unconf', 'acc_conf', 'nmi_conf', 'nb_unconf', 'nb_conf', 'fr', 'fd', 'loss'])
        logwriter = csv.DictWriter(logfile, fieldnames=['iter', 'acc', 'nmi', 'ari', 'acc_unconf', 'nmi_unconf', 'acc_conf', 'nmi_conf', 'nb_unconf', 'nb_conf', 'loss'])
        logwriter.writeheader()

        #intervals config
        print('Begin clustering:', '-' * 60)
        if save_interval is None: 
            save_interval = int(maxiter)  # only save the initial and final model
        print('Save interval ', save_interval)
        if show_interval is None:
            show_interval = int(np.ceil(number_of_samples/batch_size))*20
        print('show interval ', show_interval)  
        
        # Step 1: initialize cluster centers using k-means
        t1 = time()
        print('Initializing cluster centers with k-means.')
        
        #new just for the centroid
        #x_emb = self.predict_encoder(x)
        #km = KMeans(n_clusters=self.n_clusters, n_init=20)
        #y_pred = km.fit_predict(x_emb)
        #new
        
        centers_emb, centers_img, y_pred, _ = self.generate_centers(x, n_clusters)
        overcenters_emb, overcenters_img, _, _ = self.generate_centers(x, n_overclusters)

        # Step 2: beta1 and beta2
        #beta1, beta2 = self.generate_beta(kappa, n_clusters)
        beta1 = 0.0
      #  beta1 = .4
        #beta2 = 0.25
       # beta2 = .3
       #719 beta1 = .5
        beta2 = .25
        beta1_overclustering = 0.0
       # beta2_overclustering = 0.15
        # 719 beta2_overclustering = 0.25
        beta2_overclustering = 0.15
        n_clusters = 2
      #719  n_overclusters = 100
        n_overclusters = 10
       ## 719 kappa = 3
       
        kappa = 1
        acc_prev = 0
        print ('beta1 beta2 b1ove b2ove n_overclusters',beta1,beta2,beta1_overclustering,beta2_overclustering,n_overclusters)            
        # Step 3: deep clustering
        loss = 0
        index = 0
        nb_conf_prev = x.shape[0]
        index_array = np.arange(x.shape[0])
        delta_kappa = 0.1 * kappa
        #sess = tf.keras.backend.get_session()
        #first_time = 1
       # validate_interval = 1
        print ('validate_interval tol ',validate_interval,tol)
        print("----------------------------------------------------------------------------------")
        draw_centers(n_clusters, centers_img, img_h=img_h, img_w=img_w)
        
        
        
        for ite in range(int(maxiter)):
           # print ('range inside ite  ite' , range(int(maxiter))  , ite )
            if ite % validate_interval == 0:
            #    print('inside if ite')
                x_emb = self.encoder.predict(x)
              
              #  print('xemb   ',x_emb)
                q = q_mat(x_emb, centers_emb)
               # print('change ypred ', ite % validate_interval)
                y_pred = q.argmax(1) 
                #print('y_pred after q', y_pred)
                
                avg_loss = loss / validate_interval
                loss = 0.
                if ite > 0:
                    nb_conf_prev = nb_conf 
                nb_unconf, nb_conf = self.compute_nb_conflicted_data(x, centers_emb, beta1, beta2)
                #update centers
                if nb_conf >= nb_conf_prev:
                    #centers_emb, centers_img, _, _ = self.generate_centers(x, n_clusters)
                  #  print("update centers", nb_conf , nb_conf_prev )
                    beta1 = beta1 - (delta_kappa / n_clusters)
                    beta2 = beta2 - (delta_kappa / n_clusters)
                    #beta1 = 0.5
                    #beta2 = 0.25
                    delta_kappa = 0.1 * kappa
                    kappa = delta_kappa
                    #first_time = 0
                    #print("update confidences")
                
                #print(' before y')    

                if y is not None:
                    #y_mapped = map_vector_to_clusters(y, y_pred)
                    #x_emb = self.predict_encoder(x)
                    #y_encoder, y_autoencoder = generate_supervisory_signals(x_emb, x, centers_emb, centers_img, beta1, beta2)
                    #y_encoder_true = centers_emb[y_mapped]
                    #grad_loss_dynAE = sess.run(self.grad_loss_dynAE, feed_dict={'input_dynAE:0': x, 'target1_dynAE:0': y_encoder, 'target2_dynAE:0': y_autoencoder})
                    #grad_loss_pseudo_supervised = sess.run(self.grad_loss_pseudo_supervised, feed_dict={'input_dynAE:0': x, 'target1_dynAE:0': y_encoder})
                    #grad_loss_self_supervised = sess.run(self.grad_loss_self_supervised, feed_dict={'input_dynAE:0': x, 'target2_dynAE:0': y_autoencoder})
                    #grad_loss_supervised = sess.run(self.grad_loss_supervised, feed_dict={'input_dynAE:0': x, 'target3_dynAE:0': y_encoder_true})
                    
                    acc = np.round(metrics.acc(y, y_pred), 5)
                 #   print('accccccc',acc)
                    nmi = np.round(metrics.nmi(y, y_pred), 5)
                    ari = np.round(metrics.ari(y, y_pred), 5) 
                    #fr = np.round(metrics.cos_grad(grad_loss_supervised, grad_loss_dynAE), 5)
                    #fd = np.round(metrics.cos_grad(grad_loss_self_supervised, grad_loss_pseudo_supervised), 5)
                  #  print('x', x)
                   # print('y',y)
                    #print('centers_emb',centers_emb)
                    acc_unconf, nmi_unconf, acc_conf, nmi_conf = self.compute_acc_and_nmi_conflicted_data(x, y, centers_emb, beta1, beta2)
                    ## acc_unconf, nmi_unconf, acc_conf, nmi_conf = self.compute_acc_and_nmi_conflicted_data(x, y, centers_emb, beta1, beta2)
                    #logdict = dict(iter=ite, acc=acc, nmi=nmi, ari=ari, acc_unconf=acc_unconf, nmi_unconf=nmi_unconf, acc_conf=acc_conf, nmi_conf=nmi_conf, nb_unconf=nb_unconf, nb_conf=nb_conf, fr=fr, fd=fd, loss=avg_loss)
                    logdict = dict(iter=ite, acc=acc, nmi=nmi, ari=ari, acc_unconf=acc_unconf, nmi_unconf=nmi_unconf, acc_conf=acc_conf, nmi_conf=nmi_conf, nb_unconf=nb_unconf, nb_conf=nb_conf, loss=avg_loss)
                    logwriter.writerow(logdict)
                    logfile.flush()
                    #print('Iter %d: acc=%.5f, nmi=%.5f, ari=%.5f, acc_unconf=%.5f, nmi_unconf=%.5f, acc_conf=%.5f, nmi_conf=%.5f, nb_unconf=%d, nb_conf=%d, fr=%.5f, fd=%.5f, loss=%.5f' % (ite, acc, nmi, ari, acc_unconf, nmi_unconf, acc_conf, nmi_conf, nb_unconf, nb_conf, fr, fd, avg_loss))
                    print('Iter %d: acc=%.5f, nmi=%.5f, ari=%.5f, acc_unconf=%.5f, nmi_unconf=%.5f, acc_conf=%.5f, nmi_conf=%.5f, nb_unconf=%d, nb_conf=%d, loss=%.5f' % (ite, acc, nmi, ari, acc_unconf, nmi_unconf, acc_conf, nmi_conf, nb_unconf, nb_conf, avg_loss))
                    print("The number of unconflicted data points is : " + str(nb_unconf))
                    print("The number of conflicted data points is : " + str(nb_conf))
                    
                    if acc > acc_prev:
                        acc_prev = acc
                        
                        self.ae.save_weights(save_dir + '/' + self.dataset + '/cluster/ae_N_weights.h5')
                       # print('save w ', acc )
                    
                else:
                    x_embed = self.encoder.predict(x)
                    silhouette_avg = silhouette_score(x_embed, y_pred) 
                    print("For number of clusters = " +  str(self.n_clusters) + " The average silhouette_score is :", silhouette_avg) 
                    #logdict = dict(iter=ite, silhouette_avg=silhouette_avg, loss=avg_loss)
                    #logwriter.writerow(logdict)
                    #logfile.flush()
                #print('nb_conf', nb_conf )
                #print('xshape', x.shape[0])
                #print('tol', tol)
                    
                if(nb_conf / x.shape[0]) < tol:
                 #   print('clooooooose')
                    logfile.close()
                    break

            if ite % show_interval == 0:
               
                print("")
                print("----------------------------------------------------------------------------------")
                print("Centroids : ")
                print("----------------------------------------------------------------------------------")
               # draw_centers(n_clusters, centers_img, img_h=img_h, img_w=img_w)
                
                # save intermediate model
            
            if ite % save_interval == 0:
                
                z = self.predict_encoder(x)
                q1 = q_mat(z, centers_emb)
               # y1_pred = q1.argmax(1)
                y1_pred = q1.argmax(1)
               

                pca = PCA(n_components=2).fit(z)
                z_2d = pca.transform(z)
                centers_2d = pca.transform(centers_emb)

                # save states for visualization
                np.save(self.visualisation_dir + '/embeddings/' + self.dataset + '/vis_' + str(ite) + '.npy', {'z_2d': z_2d, 'centers_2d': centers_2d, 'y_pred': y1_pred})

                print('saving model to: ', save_dir + '/' + self.dataset + '/cluster/ae_' + str(ite) + '_weights.h5')
                self.ae.save_weights(save_dir + '/' + self.dataset + '/cluster/ae_' + str(ite) + '_weights.h5')

            # train on batch
            #if ite % 2 == 2:
            if ite % 2 == 2:    
                idx = index_array[index * batch_size: min((index+1) * batch_size, x.shape[0])]
                X_img = x[idx]
                X_emb = self.predict_encoder(X_img)
                Y_encoder, Y_autoencoder = generate_supervisory_signals(X_emb, X_img, centers_emb, centers_img, beta1, beta2)
                X_transformed = self.random_transform(X_img, ws=self.ws, hs=self.hs, rot=self.rot, scale=self.scale) if aug_train else X_img
                losses = self.train_on_batch_dynAE(X_transformed, Y_encoder, Y_autoencoder)
                loss = loss + losses
                #print('loooose')
                index = index + 1 if (index + 1) * batch_size <= x.shape[0] else 0
            else:
                idx = index_array[index * batch_size: min((index+1) * batch_size, x.shape[0])]
                X_img = x[idx]
                X_emb = self.predict_encoder(X_img)
                Y_encoder, Y_autoencoder = generate_supervisory_signals(X_emb, X_img, overcenters_emb, overcenters_img, beta1_overclustering, beta2_overclustering)
                X_transformed = self.random_transform(X_img, ws=self.ws, hs=self.hs, rot=self.rot, scale=self.scale) if aug_train else X_img
                losses = self.train_on_batch_dynAE(X_transformed, Y_encoder, Y_autoencoder)
                loss = loss + losses
                index = index + 1 if (index + 1) * batch_size <= x.shape[0] else 0
                
        logfile.close()
        
      #  if acc > acc_prev:
       #     acc_prev = acc
        print('saving model to:', save_dir + '/' + self.dataset + '/cluster/ae_all_weights.h5')
        self.ae.save_weights(save_dir + '/' + self.dataset + '/cluster/ae_all_weights.h5')
        
        print('Clustering time: %ds' % (time() - t1))
        print('End clustering:', '-' * 60)

        return y_pred
    

## dataset vesselmnist

    def train_ae_vesselmnist(self, x, y=None, optimizer='adam', epochs=10, batch_size=50, save_dir=PATH_RESULT, verbose=1, aug_train=True):
    #def train_ae_bloodmnist(self, x, y=None, optimizer='adam', epochs=500, batch_size=256, save_dir=PATH_RESULT, verbose=1, aug_train=True):
     #   print('Begin pretraining: vesselmnist', '-' * 60)
        self.ae.compile(optimizer=optimizer, loss='mse')
        #csv_logger = callbacks.CSVLogger(save_dir + '/' + self.dataset + '/pretrain/log_ae.csv')
        #cb = [csv_logger]
        
        #Prepare log file
        import csv, os
        if not os.path.exists(save_dir):
            os.makedirs(save_dir)
        logfile = open(save_dir + '/' + self.dataset + '/pretrain/pretrain.csv', 'w')
        logwriter = csv.DictWriter(logfile, fieldnames=['iter', 'ID_global_mean', 'ID_local_mean', 'PC_ID_mean', 'ID_global_error', 'ID_local_error', 'FT','acc','nmi'])
        logwriter.writeheader()
        filepath = '/content/drive/My Drive/Colab/DynAE_Amal/results/vesselmnist3d/pretrain/ae_weightsC.h5'
        model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(filepath=filepath, save_weights_only=True, monitor='val_accuracy', verbose=1, mode='max')
        callbacks_list = [model_checkpoint_callback]
        cb = []
        acc_prev = 0
        if y is not None and verbose > 0:
            class PrintACC(callbacks.Callback):
                def __init__(self, x, y, encoder):
                    self.x = x
                    self.y = y
                    self.encoder = encoder
                    super(PrintACC, self).__init__()

                def on_epoch_end(self, epoch, logs=None):
                    if epoch % 20 != 0:
                        return
                    features = self.encoder.predict(self.x)
                    km = KMeans(n_clusters=len(np.unique(y)), n_init=20)
                   
                    y_pred = km.fit_predict(features,callbacks_list)
      #              print(' '*8 + '|==>  acc: %.4f,  nmi: %.4f  <==|'% (metrics.acc(y, y_pred), metrics.nmi(y, y_pred)))
            
            class PrintID_LID(callbacks.Callback):
                def __init__(self, x, y, encoder):
                    self.x = x
                    self.y = y
                    self.encoder = encoder
                    super(PrintID_LID, self).__init__()

                def on_epoch_end(self, epoch, logs=None):
                    features = self.encoder.predict(self.x)
                    km = KMeans(n_clusters=len(np.unique(y)), n_init=20)
                    y_pred = km.fit_predict(features,callbacks_list)
                    ID = [] 
                    PC_ID = []
                    for k in range(len(np.unique(y))):
                       ID.append(computeID(features[y_pred==k]))
                       PC_ID.append(computePC_ID(features[y_pred==k]))
                    ID = np.asarray(ID)
                    PC_ID = np.asarray(PC_ID)
                    ID_global_mean = np.mean(ID)
                    ID_local_mean = np.mean(ID, axis=1) 
                    ID_global_error = np.std(ID)
                    ID_local_error = np.std(ID, axis=1)
                    PC_ID_mean = np.mean(PC_ID)
                    FT = PC_ID_mean - ID_global_mean
                  #  print ("ID = ", ID_global_mean)
                   # print ("LID = ", PC_ID_mean)
                    logdict = dict(iter=epoch, ID_global_mean=ID_global_mean, ID_local_mean=ID_local_mean, PC_ID_mean=PC_ID_mean, ID_global_error=ID_global_error, ID_local_error=ID_local_error, FT=FT, acc=metrics.acc(y, y_pred) ,nmi=metrics.nmi(y, y_pred) )
                    logwriter.writerow(logdict)
                    logfile.flush()
                    
                    
            cb.append(PrintACC(x, y, self.encoder))
            cb.append(PrintID_LID(x, y, self.encoder))
        # begin pretraining
        t0 = time()
        #print('2--')
        if not aug_train:
         #   print('3--')
            self.ae.fit(x, y, batch_size=batch_size, epochs=epochs, callbacks=cb, verbose=verbose)
        else:
          #  print('4--')
       #     print('-=*'*20)
        #    print('Using augmentation for ae')
         #   print('-=*'*20)
            def gen(x, batch_size):
                if len(x.shape) > 2:  # image
                #    print('5--')
                    gen0 = self.datagen.flow(y, shuffle=True, batch_size=batch_size)
                    while True:
                        batch_x = gen0.next()
                        yield (batch_x, batch_x)
                else:
                 #   print('6--')
                    width = int(np.sqrt(x.shape[-1]))
          
                  #  print('7--')
                    if width * width == x.shape[-1]:  # gray
                        
                        im_shape = [-1, width, width, 1]
                   #     print('8--')
                    else:  # RGB
                        width = int(np.sqrt(x.shape[-1] / 3.0))
                        width = int(np.sqrt(x.shape[-1] / 3.0))
                       
                        im_shape = [-1, width, width, 3]
                        im_shape = [-1, width, width, 3]
                    xreshape = np.reshape(x, im_shape)
                    gen0 = self.datagen.flow(np.reshape(x, im_shape), shuffle=True, batch_size=batch_size)
                    while True:
                        batch_x = gen0.next()
                        batch_x = np.reshape(batch_x, [batch_x.shape[0], x.shape[-1]])
                        yield (batch_x, batch_x)
                       
                       
            self.ae.fit_generator(gen(x, batch_size), steps_per_epoch=int(x.shape[0]/batch_size), epochs=epochs, callbacks=cb, verbose=verbose)
        
        exec_time = time() - t0
        print('Pretraining time: %ds' % (exec_time))
        if y is not None:
            features = self.encoder.predict(x)
            km = KMeans(n_clusters=len(np.unique(y)), n_init=20)
            y_pred = km.fit_predict(features,callbacks_list)
            f = open(save_dir + '/' + self.dataset + '/pretrain/results_train_ae.txt', 'w+')
            f.write('|==> ACC: %.4f,  NMI: %.4f  <==| \n' % (metrics.acc(y, y_pred), metrics.nmi(y, y_pred)))
            f.write('|==> Execution time: %.5f <==| \n' % (exec_time))
            f.close()
            logfile.close()
        
        
        if metrics.acc(y, y_pred) > acc_prev:
                acc_prev = metrics.acc(y, y_pred)
                self.ae.save_weights(save_dir + '/' + self.dataset + '/pretrain/ae_weights.h5')
        print('Pretrained weights are saved tkk %s/%s/pretrain/ae_weights.h5' % (save_dir, self.dataset))
        
        print('End pretraining: ', '-' * 60)


###  phase  2 vesselmnist

    def train_second_phase_vesselmnist(self, x, y=None, kappa=1, n_clusters=2, maxiter=1e5, batch_size=50, tol=1e-1, validate_interval=140, show_interval=None, save_interval=800, save_dir=PATH_RESULT, aug_train=True, callbacks=callbacks_list ):    
    # def train_second_phase(self, x, y=None, kappa=3, n_clusters=3, maxiter=1e5, batch_size=256, tol=1e-1, validate_interval=140, show_interval=None, save_interval=2800, save_dir=PATH_RESULT, aug_train=True):  --best
                
    #def train_second_phase(self, x, y=None, kappa=3, n_clusters=10, maxiter=10, batch_size=, tol=1e-2, validate_interval=140, show_interval=None, save_interval=2800, save_dir=PATH_RESULT, aug_train=True):
        #init
        number_of_samples = x.shape[0]
    #    print(number_of_samples)
        img_h = int(math.sqrt(x.shape[0]))
        img_w = int(math.sqrt(x.shape[0]))
     #   print ('Num of sample', number_of_samples )
      #  print('img_h  ',  img_h)
       # print ('img_w', img_w )
        #logging file
        import csv, os
        if not os.path.exists(save_dir):
            os.makedirs(save_dir)
        logfile = open(save_dir + '/' + self.dataset + '/phase_2/train_dynAE_gamma=' + str(self.gamma) + '_log.csv', 'w')
        logfile1 = open(save_dir + '/' + self.dataset + '/phase_2/train_dynAE_gamma=' + str(self.gamma) + '_log_MAX.csv', 'w')
        #logwriter = csv.DictWriter(logfile, fieldnames=['iter', 'acc', 'nmi', 'ari', 'acc_unconf', 'nmi_unconf', 'acc_conf', 'nmi_conf', 'nb_unconf', 'nb_conf', 'fr', 'fd', 'loss'])
        logwriter = csv.DictWriter(logfile, fieldnames=['iter', 'acc', 'nmi', 'ari', 'loss',  'trueN', 'FalseN', 'trueAF', 'FalseAF', 'accN','accF', 'ID_global_mean', 'ID_local_mean', 'PC_ID_mean', 'ID_global_error', 'ID_local_error', 'FT'])
        logwriter1 = csv.DictWriter(logfile1, fieldnames=['iter', 'accMax','nmi'])
        logwriter.writeheader()
        logwriter1.writeheader()
        #intervals config
    #    print('begin clustering:', '-' * 60)
        if save_interval is None: 
            save_interval = int(maxiter)  # only save the initial and final model
     #   print('Save interval ', save_interval)
        if show_interval is None:
            show_interval = int(np.ceil(number_of_samples/batch_size))*20
      #  print('show interval ', show_interval)  

        # Step 1: initialize cluster centers using k-means
        t1 = time()
        #print('Initializing cluster centers with k-means.')
        centers_emb, centers_img, y_pred, _ = self.generate_centers(x, n_clusters)
     
        # Step 3: deep clustering
        loss = 0
        index = 0
        nb_conf_prev = x.shape[0]
        index_array = np.arange(x.shape[0])
        #delta_kappa = 0.3 * kappa
        #sess = tf.keras.backend.get_session()
        first_time = 1
      #  print ('index_array', index_array)
   #      Alpha = 0.6--  best best 634
    #    Alpha = 0.5   -- best
  #      Beta = 0.6   -- best
     #   Beta = 0.6    best best 634
    
       # Alpha = 0.6   --- best medmnisr
    #    Beta = 0.6   --- best medmnit
        
        
        #Alpha = 0.6   breastmnist 73 nmi 4
        #Beta = 0.7    breastmnist 73 nmi 4
        
      ## good for poneu  Alpha = 0.6
      ##good for ponue  Beta = 0.7
        Alpha = 0.9
        Beta = 1
        
    #   Beta1 = 0.9
        #Beta2 = 0.6
        
      #  Beta2 = 0.2
    #    number_neighbours = 5  --best
        number_neighbours = 3
        #smallest_d = 0.0001
        #d = 0.0001
        #dd = 0
        acc_prev = 0
        acc_prev_sc = 0
       # batch_size = 256
       # wcss = []
    #    x_emb = self.encoder.predict(x)
     #   for i in range(1,11):
      #    kmeans = KMeans(n_clusters= i, n_init=20, random_state=0)
       #   kmeans = kmeans.fit(x_emb)
          
        #  wcss.append(kmeans.inertia_)
        #plt.plot(range(1,11), wcss)
        #plt.title('T')
        #plt.xlabel('T')
        #plt.ylabel('T')
        #plt.show()
       # validate_interval = 1
        print('N_B1_B2_Alpha', number_neighbours, Beta, Alpha)
        accuracy_unconflicted_point = 0
        accuracy_conflicted_point = 0
        unconflicted_point = 0
        conflicted_point = 0
        
        
        
         
        for ite in range(int(maxiter)):
           
            if ite % validate_interval == 0:
                
               # x_emb = self.encoder.predict(x)
                x_emb = self.predict_encoder(x)
                
                centers_embs = x_emb
                
                centers_embs = centers_embs
              
                nearest_neighbours = NearestNeighbors(n_neighbors=number_neighbours, algorithm='ball_tree').fit(x_emb)
          #      print('nnnnnnnnnnnnn',nearest_neighbours)
              
                    
               # print('ddd-33')
                _, indices = nearest_neighbours.kneighbors(x_emb)
                
                #print('ddd-333')
                x_emb_neighbours = x_emb[indices]
            #    print('x_emb_neighbours' , x_emb_neighbours)
                # Selecting the unconflicted points
                unconflicted_point_indices = []
                conflicted_point_indices = []
                for i in range(len(x_emb)):
                    biggest_d = 0
                    smallest_d = 10000
                    smallest_dd = 0
                    for j in range(number_neighbours - 1):
                     #   print('ddd-4', x_emb[i])
                      
                        d = distance.euclidean(x_emb[i], x_emb_neighbours[i, j + 1, :])
                      #  print('ddd',d)
                        if biggest_d < d:
                            biggest_d = d
                        if smallest_d > d:
                            smallest_d = d
               #     print('biggestd', biggest_d)      
                    
                    if biggest_d != 0:
                        
                        if (smallest_d / biggest_d) < Alpha:
                            unconflicted_point_indices.append(i)
                        else:
                            conflicted_point_indices.append(i)
                print('unconflicted_point_indices ', len(unconflicted_point_indices))
                print('conflicted_point_indices ', len(conflicted_point_indices))
                unconflicted_point = len(unconflicted_point_indices)
                conflicted_point = len(conflicted_point_indices)
                #smallest_d = 0.0001       
                smallest_dd = 0
                for i in range(len(x_emb)):
                    sum_neighbours = 0
                    number_nn = 0
                  #  smallest_dd = 0
                    if i in unconflicted_point_indices:
                        for j in range(number_neighbours - 1):
                            d = distance.euclidean(x_emb[i], x_emb_neighbours[i, j + 1, :])
                            #print ('d', d)
                            if smallest_d > d:
                                smallest_d = d 
                            dd = d - smallest_d
                            #print ('smallest_d', smallest_d)
                            #dd = smallest_d / d
                            #print ('dd', dd)
                           # if smallest_dd < d:
                            #    smallest_dd = d
                            #else:
                            #    smallest_dd = smallest_dd 
                            #dd = smallest_dd / d
                            # dd = d  /  (smallest_d * smallest_d )
                           
                           # print ('dd ', dd)
                            if dd < Beta:
                                #if dd <= Beta2:
                                sum_neighbours = sum_neighbours + x_emb_neighbours[i, j + 1, :] 
                                number_nn = number_nn + 1
                        if number_nn != 0:
                            centers_embs[i] = sum_neighbours / number_nn
                        else:
                            centers_embs[i] = x_emb[i]
               
                x_emb = self.predict_encoder(x)
                ##
                #act='relu'
                #dims=[x.shape[-1], 500, 500, 2000, 2]
                
                #n_stacks = len(dims) - 1
                #init = VarianceScaling(scale=1. / 3., mode='fan_in', distribution='uniform')
                # input
                
                #x = Input(shape=(dims[0],), name='input_encoder')
                #print('xxxxxxx',x)
                #h = x
                # internal layers in encoder
                #for i in range(n_stacks-1):
                #    h = Dense(dims[i + 1], activation=act, kernel_initializer=init, name='encoder_%d' % i)(h)
                # hidden layer
                #h = Dense(dims[-1], kernel_initializer=init, name='encoder_%d' % (n_stacks - 1))(h)  # hidden layer, features are extracted from here
                #encoder = Model(inputs=x, outputs=h, name='encoder')
                #plot_model(encoder, show_shapes=True, show_layer_names=True, to_file=self.visualisation_dir + '/graph/FcEncoder.png')
                
                ##
               
                #print ('x_emb' , x_emb )
                #x_emb_u = self.predict_encoder(unconflicted_point_indices)
               # print ('x_emb' , x_emb )
               # print ('unconflicted_point_indices' , unconflicted_point_indices )
                accuracy_unconflicted_point = unconflicted_point / (unconflicted_point + conflicted_point)
                accuracy_conflicted_point = conflicted_point / (unconflicted_point + conflicted_point)
                print ('accuracy_unconflicted_point : ', accuracy_unconflicted_point)
                print ('accuracy_conflicted_point : ', accuracy_conflicted_point)
                
                #print ('x_emb_u' , x_emb_u )
                km = KMeans(n_clusters=self.n_clusters, n_init=20)
                y_pred = km.fit_predict(x_emb, callbacks)
               
                #print('uncon00')
                
                
                #y_pred_sc = DBSCAN(eps=0.25, min_samples=9).fit_predict(x_emb)
              #  y_pred_sc= Birch(branching_factor = 50, n_clusters = self.n_clusters, threshold = 1.5).fit_predict(x_emb)
                #y_pred_sc= GaussianMixture(self.n_clusters, covariance_type='full', random_state=0).fit_predict(x_emb)
               # y_pred_sc = SpectralClustering(n_clusters=self.n_clusters).fit_predict(x_emb)
                #y_pred_sc = AgglomerativeClustering(n_clusters=self.n_clusters, affinity='euclidean', linkage='ward').fit_predict(x_emb)
                avg_loss = loss / validate_interval
                loss = 0
               # acc_prev = 0
                acc = 0
                if y is not None:
                 
                    km = KMeans(n_clusters=self.n_clusters, n_init=20)
                #km = kmedoids(n_clusters=self.n_clusters, n_init=20)
                    y_pred = km.fit_predict(x_emb, callbacks)
                    
                   
                  #y_pred_sc = DBSCAN(eps=0.25, min_samples=9).fit_predict(x_emb)
                  #  y_pred_sc = SpectralClustering(n_clusters=self.n_clusters).fit_predict(x_emb)
                   # y_pred_sc= Birch(branching_factor = 50, n_clusters = self.n_clusters, threshold = 1.5).fit_predict(x_emb)
                   # y_pred_sc= GaussianMixture(self.n_clusters, covariance_type='full', random_state=0).fit_predict(x_emb)
                    #y_pred_sc = AgglomerativeClustering(n_clusters=self.n_clusters, affinity='euclidean', linkage='ward').fit_predict(x_emb)
# ##  New Part                    
                    true_neighbours = 0
                    false_neighbours = 0
                    true_neighbours_after_filtering = 0
                    false_neighbours_after_filtering = 0
                    #smallest_d = 0.0001
                    smallest_dd = 0
                    
                    # Selecting the neighbours of the unconflicted points
                    for i in range(len(x_emb)):
                      #  biggest_d = 0
                      #  smallest_d = 10000
                    #    smallest_dd = 0
                        
                        if i in unconflicted_point_indices:
                            for j in range(number_neighbours - 1):
                                d = distance.euclidean(x_emb[i], x_emb_neighbours[i, j + 1, :])
                                if smallest_d > d:
                                    smallest_d = d 
                                dd = d - smallest_d    
                               # print ('dd : ', dd)
                                #if smallest_dd < d:
                                 #   smallest_dd = d
                                  #  print ('smallest_dd d ' , smallest_dd)
                                #else:
                                 #   smallest_dd = smallest_dd 
                                   # print ('smallest_dd smallest_dd' , smallest_dd)
                                #dd = smallest_dd   /  d
                                    #print ('ddevaluation', dd)
                            
                                if dd < Beta:
                                    #if dd <= Beta2:
                                    if y[i] == y[indices[i, j]]:
                                        true_neighbours_after_filtering = true_neighbours_after_filtering + 1
                                    else:
                                        false_neighbours_after_filtering = false_neighbours_after_filtering + 1
                                
                                if y[i] == y[indices[i, j]]:
                                    true_neighbours = true_neighbours + 1
                                else: 
                                    false_neighbours = false_neighbours + 1
                        
                    
                                    
                    #if prev_acc <= acc: 
                    if (true_neighbours_after_filtering + false_neighbours_after_filtering) != 0:
                        acc_neighbours = true_neighbours / (false_neighbours + true_neighbours)
                        acc_filtered_neighbours = true_neighbours_after_filtering / (true_neighbours_after_filtering + false_neighbours_after_filtering)
                    else:
                        acc_neighbours = 0
                        acc_filtered_neighbours = 0
                    
                    
                   # acc_filtered_neighbours = true_neighbours_after_filtering / (true_neighbours_after_filtering + false_neighbours_after_filtering) 
                    print ('true_neighbours : ', true_neighbours)
                    print ('false_neighbours : ', false_neighbours)
                    print ('true_neighbours_after_filtering : ', true_neighbours_after_filtering)
                    print ('false_neighbours_after_filtering : ', false_neighbours_after_filtering)
                    print ('acc_neighbours : ', acc_neighbours)
                    print ('acc_filtered_neighbours : ', acc_filtered_neighbours)
                   
                    km = KMeans(n_clusters=self.n_clusters, n_init=20)
                    
                    y_pred = km.fit_predict(x_emb, callbacks)
                   
                    
                    
                    
                    #y_pred_sc = DBSCAN(eps=0.25, min_samples=9).fit_predict(x_emb)
                #    y_pred_sc = SpectralClustering(n_clusters=self.n_clusters).fit_predict(x_emb)
                 #   y_pred_sc= Birch(branching_factor = 50, n_clusters = self.n_clusters, threshold = 1.5).fit_predict(x_emb)
                  #  y_pred_sc= GaussianMixture(self.n_clusters, covariance_type='full', random_state=0).fit_predict(x_emb)
                  #  y_pred_sc = AgglomerativeClustering(n_clusters=self.n_clusters, affinity='euclidean', linkage='ward').fit_predict(x_emb)
                    #y_mapped = map_vector_to_clusters(y, y_neighbours)
                   
                    ## End of new part
                    
                    acc = np.round(metrics.acc(y, y_pred), 5)
                    nmi = np.round(metrics.nmi(y, y_pred), 5)
                    ari = np.round(metrics.ari(y, y_pred), 5)
                    #f1score = np.round(sklearn.metrics.f1_score(y, y_pred, labels=None, pos_label=1, average='weighted'),5)   
                   # f1score = f1_score(y,y_pred)
                   # f1scoremicro= f1_score(y, y_pred, average='micro')
                    #f1scoremacro= f1_score(y, y_pred, average='macro')
                    
                    #print('f1scoremicro', f1scoremicro)
                    #print('f1scoremacro', f1scoremacro)
                    
                    
                    
                  #  acc_sc = np.round(metrics.acc(y, y_pred_sc), 5)
                  #  nmi_sc = np.round(metrics.nmi(y, y_pred_sc), 5)
                #    ari_sc = np.round(metrics.ari(y, y_pred_sc), 5)
                    
                   # acc_prev = acc
                    ID = [] 
                    PC_ID = []
                    for k in range(self.n_clusters):
                      ID.append(computeID(x_emb[y_pred==k]))
                      PC_ID.append(computePC_ID(x_emb[y_pred==k]))
                    ID = np.asarray(ID)
                    PC_ID = np.asarray(PC_ID)
                    ID_global_mean = np.mean(ID)
                    ID_local_mean = np.mean(ID, axis=1) 
                    ID_global_error = np.std(ID)
                    ID_local_error = np.std(ID, axis=1)
                    PC_ID_mean = np.mean(PC_ID)
                    FT = PC_ID_mean - ID_global_mean
                    #print ("FT", FT)
                    print ("ID_local_mean = ", ID_local_mean)
                    print ("ID = ", ID_global_mean)
                    print ("LID = ", PC_ID_mean)
                    
                    if acc > acc_prev:
                        #acc_prev = acc
                        logdict = dict(iter=ite, acc=acc, nmi=nmi, ari=ari,  loss=avg_loss,  trueN=true_neighbours, FalseN=false_neighbours, trueAF=true_neighbours_after_filtering, FalseAF=false_neighbours_after_filtering,  accN=acc_neighbours, accF=acc_filtered_neighbours, ID_global_mean=ID_global_mean, ID_local_mean=ID_local_mean.tolist(), PC_ID_mean=PC_ID_mean, ID_global_error=ID_global_error, ID_local_error=ID_local_error.tolist(), FT=FT)
                        logwriter.writerow(logdict)
                        logfile.flush()
                    
                    print('K-means: iter %d, acc=%.5f, nmi=%.5f, ari=%.5f, loss=%.5f' % (ite, acc, nmi, ari,  avg_loss))
                #    print('Spectral clustering: iter %d, acc=%.5f, nmi=%.5f, ari=%.5f, loss=%.5f' % (ite, acc_sc, nmi_sc, ari_sc, avg_loss))
                  
                    #print("")
                    #print("----------------------------------------------------------------------------------")
                    print("Centroids : ")
                    print("----------------------------------------------------------------------------------")
                    #centers_emb, centers_img, y_pred, _ = self.generate_centers(x, n_clusters)
                    #draw_centers(n_clusters, centers_img, img_h=img_h, img_w=img_w)
                       
                else:
                    x_embed = self.encoder.predict(x)
                    silhouette_avg = silhouette_score(x_embed, y_pred) 
                    print("For number of clusters = " +  str(self.nearest_neighbours) + " The average silhouette_score is :", silhouette_avg) 
                   
            
           
          
            idx = index_array[index * batch_size: min((index+1) * batch_size, x.shape[0])]
            
          #wit best  idx = index_array[index * 10: min((index+1) * 10, x.shape[0])]
           
            X_img = x[idx]
            
           
            
          #  x_emb = self.predict_encoder(x)
            x_emb = self.predict_encoder(X_img)
            Y_encoder = centers_embs[idx]
           
                        #print('Y_encoder',Y_encoder)
            Y_autoencoder = self.decoder.predict(centers_embs[idx])
            
              # Y_autoencoder = self.decoder.predict(x_emb)
            X_transformed = self.random_transform(X_img, ws=self.ws, hs=self.hs, rot=self.rot, scale=self.scale) if aug_train else X_img
           
            
            losses = self.train_on_batch_dynAE(X_transformed, Y_encoder, Y_autoencoder)
           
            
            
   
            loss = loss + losses
           
          
            
            index = index + 1 if (index + 1) * batch_size <= int(x.shape[0]) else 0
            

         
            if acc > acc_prev:
                acc_prev = acc
           #     print('acc_prev2', acc_prev)
                if os.path.isfile(save_dir + '/' + self.dataset + '/phase_2/ae_weights.h5'):
                   os.remove (save_dir + '/' + self.dataset + '/phase_2/ae_weights.h5')
                   print('remove file' )
                   print('saving model to ::', save_dir + '/' + self.dataset + '/phase_2/ae_weights.h5')
                   self.ae.save_weights(save_dir + '/' + self.dataset + '/phase_2/ae_weights.h5')
                   print('save the best acc in W', acc )
                   logdict1 = dict(iter=ite, accMax=acc_prev, nmi=nmi)
                   logwriter1.writerow(logdict1)
                   logfile1.flush()
                else: ## Show an error ##
                   print('no file' )
                   print('saving model to ::', save_dir + '/' + self.dataset + '/phase_2/ae_weights.h5')
                   self.ae.save_weights(save_dir + '/' + self.dataset + '/phase_2/ae_weights.h5')
                   print('save the best acc in W', acc )
                   logdict1 = dict(iter=ite, accMax=acc_prev, nmi=nmi)
                   logwriter1.writerow(logdict1)
                   logfile1.flush()
  
  
                
         #   if acc_sc > acc_prev:
          #      acc_prev_sc = acc_sc
           #     print('saving model to ::', save_dir + '/' + self.dataset + '/phase_2/ae_weights.h5')
            #    self.ae.save_weights(save_dir + '/' + self.dataset + '/phase_2/ae_weights.h5')
             #   logdict1 = dict(iter=ite, accMax=acc_prev, nmi_sc=nmi_sc)
              #  logwriter1.writerow(logdict1)
               # logfile1.flush()
                          
                
        #model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(filepath=save_dir + '/' + self.dataset + '/phase_2/ae_weightsC.h5',save_weights_only=True,monitor='val_accuracy',mode='max',save_best_only=True)
                
                
             #   tf.keras.callbacks.ModelCheckpoint(filepath=save_dir + '/' + self.dataset + '/phase_2/ae_weights.h5',save_weights_only=True,monitor='val_accuracy',mode='max',save_best_only=True)
                #self.fit(epochs=epochs, callbacks=[model_checkpoint_callback])
        #print( 'mmmm',model_checkpoint_callback)
              ##  self.ae.save_weights(save_dir + '/' + self.dataset + '/phase_2/ae_weights.h5')

              
          
          
        print('Clustering time: %ds' % (time() - t1))
        print('End clustering:', '-' * 60)
        print("Centroids : ")
        print("----------------------------------------------------------------------------------")
                #centers_emb, centers_img, y_pred, _ = self.generate_centers(x, n_clusters)
                #draw_centers(n_clusters, centers_img, img_h=img_h, img_w=img_w)
                
        logfile.close()
       # print('training time: ', time() - t0)
    #    print('training: ', '-' * 60)
        print('Clustering time: %ds' % (time() - t1))
        print('End clustering:', '-' * 60)
        print("Centroids : ")
        print('end')
        print("----------------------------------------------------------------------------------")
       # centers_emb, centers_img, y_pred, _ = self.generate_centers(x, n_clusters)
    #    draw_centers(n_clusters, centers_img, img_h=img_h, img_w=img_w)
       
        
        return y_pred
    

#####  cluster vesselmnist

    def train_dynAE_New_vesselmnist(self, x, y=None, kappa=1, n_clusters=7, n_overclusters=10, maxiter=1e9, batch_size=256, tol=1e-2, validate_interval=140, show_interval=None, save_interval=2800, save_dir=PATH_RESULT, aug_train=True, callbacks=callbacks_list):
    #def train_dynAE_New(self, x, y=None, kappa=3, n_clusters=10, n_overclusters=40, maxiter=1e5, batch_size=256, tol=1e-2, validate_interval=140, show_interval=2000, save_interval=2800, save_dir=PATH_RESULT, aug_train=True):
        #init
        number_of_samples = x.shape[0]
        img_h = int(math.sqrt(x.shape[1]))
        img_w = int(math.sqrt(x.shape[1]))
       # print('maxiter',maxiter)
        #logging file
        import csv, os
        if not os.path.exists(save_dir):
            os.makedirs(save_dir)
        logfile = open(save_dir + '/' + self.dataset + '/cluster/train_dynAE_gamma=' + str(self.gamma) + '_logآNew.csv', 'w')
        #logwriter = csv.DictWriter(logfile, fieldnames=['iter', 'acc', 'nmi', 'ari', 'acc_unconf', 'nmi_unconf', 'acc_conf', 'nmi_conf', 'nb_unconf', 'nb_conf', 'fr', 'fd', 'loss'])
        logwriter = csv.DictWriter(logfile, fieldnames=['iter', 'acc', 'nmi', 'ari', 'acc_unconf', 'nmi_unconf', 'acc_conf', 'nmi_conf', 'nb_unconf', 'nb_conf', 'loss'])
        logwriter.writeheader()

        #intervals config
        print('Begin clustering:', '-' * 60)
        if save_interval is None: 
            save_interval = int(maxiter)  # only save the initial and final model
        print('Save interval ', save_interval)
        if show_interval is None:
            show_interval = int(np.ceil(number_of_samples/batch_size))*20
        print('show interval ', show_interval)  
        
        # Step 1: initialize cluster centers using k-means
        t1 = time()
        print('Initializing cluster centers with k-means.')
        
        #new just for the centroid
        #x_emb = self.predict_encoder(x)
        #km = KMeans(n_clusters=self.n_clusters, n_init=20)
        #y_pred = km.fit_predict(x_emb)
        #new
        
        

        # Step 2: beta1 and beta2
        #beta1, beta2 = self.generate_beta(kappa, n_clusters)
      #  beta1 = .7
        #beta2 = 0.25
       # beta2 = .5
        beta1 =0.5
        beta2=0.5
        beta1_overclustering = 0.0
       # beta2_overclustering = 0.15
        beta2_overclustering = 0.15
        n_clusters = 2
        n_overclusters = 200
        kappa = 2
        
        acc_prev = 0
        
        centers_emb, centers_img, y_pred, _ = self.generate_centers(x, n_clusters)
        overcenters_emb, overcenters_img, _, _ = self.generate_centers(x, n_overclusters)
       # print('cenemp', centers_emb )
        
        
        print ('beta1 beta2 b1ove b2ove n_overclusters n_clusters',beta1,beta2,beta1_overclustering,beta2_overclustering,n_overclusters, n_clusters)            
        # Step 3: deep clustering
        loss = 0
        index = 0
        nb_conf_prev = x.shape[0]
        index_array = np.arange(x.shape[0])
        delta_kappa = 0.1 * kappa
        #sess = tf.keras.backend.get_session()
        #first_time = 1
        #validate_interval = 1
        
        print("Centroids : ")
        print("----------------------------------------------------------------------------------")
        #draw_centers(n_clusters, centers_img, img_h=img_h, img_w=img_w)
        
        
        
        for ite in range(int(maxiter)):
          #  print ('range inside ite  ite' , range(int(maxiter))  , ite )
            if ite % validate_interval == 0:
              
                x_emb = self.encoder.predict(x)
              
              #  print('xemb   ',x_emb)
                q = q_mat(x_emb, centers_emb)
               # print('change ypred ', ite % validate_interval)
                y_pred = q.argmax(1) 
                #print('y_pred after q', y_pred)
                
                avg_loss = loss / validate_interval
                loss = 0.
                if ite > 0:
                    nb_conf_prev = nb_conf 
               # print('nb_conf_prev', nb_conf_prev )
                nb_unconf, nb_conf = self.compute_nb_conflicted_data(x, centers_emb, beta1, beta2)
    ##   from the fun         
                x_emb = self.encoder.predict(x)
                unconf_indices = []
                conf_indices = []
                q = q_mat(x_emb, centers_emb, alpha=3)
                confidence1 = q.max(1)
                confidence2 = np.zeros((q.shape[0],))
                a = np.argsort(q, axis=1)[:,-2]  
                for i in range(x.shape[0]):
                    confidence2[i] = q[i,a[i]]
              #  print('confidence1', confidence1[i])
               # print('confidence',confidence1[i] - confidence2[i])
                #print('beta1',beta1 ,beta2 )
                if (confidence1[i]) > beta1 and (confidence1[i] - confidence2[i]) > beta2:
                    
                    unconf_indices.append(i)
                else:
                    conf_indices.append(i)
                unconf_indices = np.asarray(unconf_indices, dtype=int)
                conf_indices = np.asarray(conf_indices, dtype=int)
    ###end the part -- remove 
                #nb_unconf, nb_conf = self.compute_nb_conflicted_data(x, centers_emb, beta1, beta2)
                print('nb_unconf', unconf_indices )
                print('nb_conf', conf_indices )
                
                #update centers
                if nb_conf >= nb_conf_prev:
                    #centers_emb, centers_img, _, _ = self.generate_centers(x, n_clusters) 00
                  #  print("update centers", nb_conf , nb_conf_prev )
                    beta1 = beta1 - (delta_kappa / n_clusters)
                    beta2 = beta2 - (delta_kappa / n_clusters)
                    #beta1 = 0.5
                    #beta2 = 0.25
                    delta_kappa = 0.1 * kappa
                    kappa = delta_kappa
                    #first_time = 0
                    #print("update confidences")
                
                print(' before y', y )    

                if y is not None:
                    #y_mapped = map_vector_to_clusters(y, y_pred)
                    #x_emb = self.predict_encoder(x)
                    #y_encoder, y_autoencoder = generate_supervisory_signals(x_emb, x, centers_emb, centers_img, beta1, beta2)
                    #y_encoder_true = centers_emb[y_mapped]
                    #grad_loss_dynAE = sess.run(self.grad_loss_dynAE, feed_dict={'input_dynAE:0': x, 'target1_dynAE:0': y_encoder, 'target2_dynAE:0': y_autoencoder})
                    #grad_loss_pseudo_supervised = sess.run(self.grad_loss_pseudo_supervised, feed_dict={'input_dynAE:0': x, 'target1_dynAE:0': y_encoder})
                    #grad_loss_self_supervised = sess.run(self.grad_loss_self_supervised, feed_dict={'input_dynAE:0': x, 'target2_dynAE:0': y_autoencoder})
                    #grad_loss_supervised = sess.run(self.grad_loss_supervised, feed_dict={'input_dynAE:0': x, 'target3_dynAE:0': y_encoder_true})
                    
                    acc = np.round(metrics.acc(y, y_pred), 5)
                  #  print('accccccc',acc)
                    nmi = np.round(metrics.nmi(y, y_pred), 5)
                    ari = np.round(metrics.ari(y, y_pred), 5) 
                    #fr = np.round(metrics.cos_grad(grad_loss_supervised, grad_loss_dynAE), 5)
                    #fd = np.round(metrics.cos_grad(grad_loss_self_supervised, grad_loss_pseudo_supervised), 5)
                    acc_unconf, nmi_unconf, acc_conf, nmi_conf = self.compute_acc_and_nmi_conflicted_data(x, y, centers_emb, beta1, beta2)
                    #logdict = dict(iter=ite, acc=acc, nmi=nmi, ari=ari, acc_unconf=acc_unconf, nmi_unconf=nmi_unconf, acc_conf=acc_conf, nmi_conf=nmi_conf, nb_unconf=nb_unconf, nb_conf=nb_conf, fr=fr, fd=fd, loss=avg_loss)
                    logdict = dict(iter=ite, acc=acc, nmi=nmi, ari=ari, acc_unconf=acc_unconf, nmi_unconf=nmi_unconf, acc_conf=acc_conf, nmi_conf=nmi_conf, nb_unconf=nb_unconf, nb_conf=nb_conf, loss=avg_loss)
                    logwriter.writerow(logdict)
                    logfile.flush()
                    #print('Iter %d: acc=%.5f, nmi=%.5f, ari=%.5f, acc_unconf=%.5f, nmi_unconf=%.5f, acc_conf=%.5f, nmi_conf=%.5f, nb_unconf=%d, nb_conf=%d, fr=%.5f, fd=%.5f, loss=%.5f' % (ite, acc, nmi, ari, acc_unconf, nmi_unconf, acc_conf, nmi_conf, nb_unconf, nb_conf, fr, fd, avg_loss))
                    print('Iter %d: acc=%.5f, nmi=%.5f, ari=%.5f, acc_unconf=%.5f, nmi_unconf=%.5f, acc_conf=%.5f, nmi_conf=%.5f, nb_unconf=%d, nb_conf=%d, loss=%.5f' % (ite, acc, nmi, ari, acc_unconf, nmi_unconf, acc_conf, nmi_conf, nb_unconf, nb_conf, avg_loss))
                    print("The number of unconflicted data points is : " + str(nb_unconf))
                    print("The number of conflicted data points is : " + str(nb_conf))
                    if acc > acc_prev:
                        acc_prev = acc
                        self.ae.save_weights(save_dir + '/' + self.dataset + '/cluster/ae_N_weights.h5')
                        
                else:
                    x_embed = self.encoder.predict(x)
                    silhouette_avg = silhouette_score(x_embed, y_pred) 
                    print("For number of clusters = " +  str(self.n_clusters) + " The average silhouette_score is :", silhouette_avg) 
                    #logdict = dict(iter=ite, silhouette_avg=silhouette_avg, loss=avg_loss)
                    #logwriter.writerow(logdict)
                    #logfile.flush()
              #  print('nb_conf', nb_conf )
               # print('xshape', x.shape[0])
                #print('tol', tol)
                    
                if(nb_conf / x.shape[0]) < tol:
                   # print('clooooooose')
                    logfile.close()
                    break

            if ite % show_interval == 0:
               
                print("")
                print("----------------------------------------------------------------------------------")
                print("Centroids : ")
                print("----------------------------------------------------------------------------------")
               # draw_centers(n_clusters, centers_img, img_h=img_h, img_w=img_w)
                
                # save intermediate model
            
            if ite % save_interval == 0:
                
                z = self.predict_encoder(x)
                q1 = q_mat(z, centers_emb)
               # y1_pred = q1.argmax(1)
                y1_pred = q1.argmax(1)
               

                pca = PCA(n_components=2).fit(z)
                z_2d = pca.transform(z)
                centers_2d = pca.transform(centers_emb)

                # save states for visualization
                np.save(self.visualisation_dir + '/embeddings/' + self.dataset + '/vis_' + str(ite) + '.npy', {'z_2d': z_2d, 'centers_2d': centers_2d, 'y_pred': y1_pred})

                print('saving model to: ', save_dir + '/' + self.dataset + '/cluster/ae_' + str(ite) + '_weights.h5')
                self.ae.save_weights(save_dir + '/' + self.dataset + '/cluster/ae_' + str(ite) + '_weights.h5')

            # train on batch
            if ite % 2 == 2:
                idx = index_array[index * batch_size: min((index+1) * batch_size, x.shape[0])]
                X_img = x[idx]
                X_emb = self.predict_encoder(X_img)
                Y_encoder, Y_autoencoder = generate_supervisory_signals(X_emb, X_img, centers_emb, centers_img, beta1, beta2)
                X_transformed = self.random_transform(X_img, ws=self.ws, hs=self.hs, rot=self.rot, scale=self.scale) if aug_train else X_img
                losses = self.train_on_batch_dynAE(X_transformed, Y_encoder, Y_autoencoder)
                loss = loss + losses
               # print('loooose')
                index = index + 1 if (index + 1) * batch_size <= x.shape[0] else 0
            else:
                idx = index_array[index * batch_size: min((index+1) * batch_size, x.shape[0])]
                X_img = x[idx]
                X_emb = self.predict_encoder(X_img)
                Y_encoder, Y_autoencoder = generate_supervisory_signals(X_emb, X_img, overcenters_emb, overcenters_img, beta1_overclustering, beta2_overclustering)
                X_transformed = self.random_transform(X_img, ws=self.ws, hs=self.hs, rot=self.rot, scale=self.scale) if aug_train else X_img
                losses = self.train_on_batch_dynAE(X_transformed, Y_encoder, Y_autoencoder)
                loss = loss + losses
                index = index + 1 if (index + 1) * batch_size <= x.shape[0] else 0
                
        
        
          #  if acc >= acc_prev:
          #      acc_prev = acc
           #     print('acc_prev2', acc_prev)
           #     if os.path.isfile(save_dir + '/' + self.dataset + '/cluster/ae_weights.h5'):
            #        os.remove (save_dir + '/' + self.dataset + '/cluster/ae_weights.h5')
             #       print('remove file' )
              #      print('saving model to ::', save_dir + '/' + self.dataset + '/cluster/ae_weights.h5')
               #     self.ae.save_weights(save_dir + '/' + self.dataset + '/cluster/ae_weights.h5')
                #    print('save the best acc in W', acc )
                 #   self.ae.save_weights(save_dir + '/' + self.dataset + '/cluster/ae_' + str(acc) + '_weights.h5')
                 #   logfile.close()
               # logdict1 = dict(iter=ite, accMax=acc_prev, nmi=nmi)
            #    logwriter1.writerow(logdict1)
             #   logfile1.flush()
        #        else: ## Show an error ##
         #           print('no file' )
          #          print('saving model to ::', save_dir + '/' + self.dataset + '/cluster/ae_weights.h5')
           #         self.ae.save_weights(save_dir + '/' + self.dataset + '/cluster/ae_weights.h5')
            #        print('save the best acc in W', acc )
                    
            #    logdict1 = dict(iter=ite, accMax=acc_prev, nmi=nmi)
             #   logwriter1.writerow(logdict1)
              #  logfile1.flush()
                   
        
            #    logfile.close()
        print('Clustering time: %ds' % (time() - t1))
        print('End clustering:', '-' * 60)
        
        return y_pred

### end dataset vesselmnist


#  Dataset pneumoniamnist  
# training 
    
   # def train_ae_pneumoniamnist(self, x, y=None, optimizer='adam', epochs=500, batch_size=256, save_dir=PATH_RESULT, verbose=1, aug_train=True):
    def train_ae_pneumoniamnist(self, x, y=None, optimizer='adam', epochs=10, batch_size=50, save_dir=PATH_RESULT, verbose=1, aug_train=True): 
        print('Begin pretraining: pneum', '-' * 60)
        self.ae.compile(optimizer=optimizer, loss='mse')
        #csv_logger = callbacks.CSVLogger(save_dir + '/' + self.dataset + '/pretrain/log_ae.csv')
        #cb = [csv_logger]
        
        #Prepare log file
        import csv, os
        if not os.path.exists(save_dir):
            os.makedirs(save_dir)
        logfile = open(save_dir + '/' + self.dataset + '/pretrain/pretrain.csv', 'w')
        logwriter = csv.DictWriter(logfile, fieldnames=['iter', 'ID_global_mean', 'ID_local_mean', 'PC_ID_mean', 'ID_global_error', 'ID_local_error', 'FT','acc','nmi'])
        logwriter.writeheader()
       
      
        cb = []
        acc_prev = 0
        if y is not None and verbose > 0:
            class PrintACC(callbacks.Callback):
                def __init__(self, x, y, encoder):
                    self.x = x
                    self.y = y
                    self.encoder = encoder
                    super(PrintACC, self).__init__()

                def on_epoch_end(self, epoch, logs=None):
                    if epoch % 20 != 0:
                        return
                    features = self.encoder.predict(self.x)
                    km = KMeans(n_clusters=len(np.unique(y)), n_init=20)
                   
                    y_pred = km.fit_predict(features)
                   
                    acc = metrics.acc(y, y_pred)
                    print(' '*8 + '|==>  acc: %.4f,  nmi: %.4f  <==|'% (metrics.acc(y, y_pred), metrics.nmi(y, y_pred)))
            
            class PrintID_LID(callbacks.Callback):
                def __init__(self, x, y, encoder):
                    self.x = x
                    self.y = y
                    self.encoder = encoder
                    super(PrintID_LID, self).__init__()

                def on_epoch_end(self, epoch, logs=None):
                    features = self.encoder.predict(self.x)
                    km = KMeans(n_clusters=len(np.unique(y)), n_init=20)
                    y_pred = km.fit_predict(features)
                    ID = [] 
                    PC_ID = []
                    for k in range(len(np.unique(y))):
                       ID.append(computeID(features[y_pred==k]))
                       PC_ID.append(computePC_ID(features[y_pred==k]))
                    ID = np.asarray(ID)
                    PC_ID = np.asarray(PC_ID)
                    ID_global_mean = np.mean(ID)
                    ID_local_mean = np.mean(ID, axis=1) 
                    ID_global_error = np.std(ID)
                    ID_local_error = np.std(ID, axis=1)
                    PC_ID_mean = np.mean(PC_ID)
                    FT = PC_ID_mean - ID_global_mean
                    print ("ID = ", ID_global_mean)
                    print ("LID = ", PC_ID_mean)
                    logdict = dict(iter=epoch, ID_global_mean=ID_global_mean, ID_local_mean=ID_local_mean, PC_ID_mean=PC_ID_mean, ID_global_error=ID_global_error, ID_local_error=ID_local_error, FT=FT, acc=metrics.acc(y, y_pred) ,nmi=metrics.nmi(y, y_pred) )
                    logwriter.writerow(logdict)
                    logfile.flush()
                    
                    
            cb.append(PrintACC(x, y, self.encoder))
            cb.append(PrintID_LID(x, y, self.encoder))
        # begin pretraining
        t0 = time()
     
        if not aug_train:
            
            self.ae.fit(x, y, batch_size=batch_size, epochs=epochs, callbacks=cb, verbose=verbose)
        else:
            
            print('-=*'*20)
            print('Using augmentation for ae')
            print('-=*'*20)
            def gen(x, batch_size):
                if len(x.shape) > 2:  # image
                   
                    gen0 = self.datagen.flow(y, shuffle=True, batch_size=batch_size)
                    while True:
                        batch_x = gen0.next()
                        yield (batch_x, batch_x)
                else:
                   
                    width = int(np.sqrt(x.shape[-1]))
                    print(width)
                    
                    if width * width == x.shape[-1]:  # gray
                        
                        im_shape = [-1, width, width, 1]
                       
                    else:  # RGB
                        width = int(np.sqrt(x.shape[-1] / 3.0))
                        width = int(np.sqrt(x.shape[-1] / 3.0))
                        print(width)
                        im_shape = [-1, width, width, 3]
                        im_shape = [-1, width, width, 3]
                        print(im_shape)
                        
                    print(x.shape[-1])
                    print(im_shape)
                    #gen0 = self.datagen.flow(np.reshape(x, im_shape), shuffle=True, batch_size=batch_size)
            
            #        x_test_gray = x_test_gray.reshape([-1, 28, 28,1]) / 255.0
            #        x_test_gray= x_test_gray.reshape([x_test_gray.shape[0], -1])
            #        x_train_gray = x_train_gray.reshape([-1, 28, 28,1]) / 255.0
            #        x_train_gray=x_train_gray.reshape([x_train_gray.shape[0], -1])
                 ##   dataGen = ImageDataGenerator(rotation_range=15,width_shift_range=0.2,height_shift_range=0.2,shear_range=0.15,zoom_range=[0.5,2],validation_split=0.2)
                   ## dataGen.fit(x)
                    ##x_generator = dataGen.flow(x.shape(0), y.shape(0), batch_size=64, shuffle=True,seed=2, save_to_dir=None, subset='training')
                    
                    print('reshape')
                    xreshape = np.reshape(x, im_shape)
                    print(xreshape)
                    gen0 = self.datagen.flow(np.reshape(x, im_shape), shuffle=True, batch_size=batch_size)
                    #np.array(x).shape[-1]
                  
                    while True:
                      ##  batch_x = x_generator.next()
                        batch_x = gen0.next()
                       
                        batch_x = np.reshape(batch_x, [batch_x.shape[0], x.shape[-1]])
                        ##batch_x = np.reshape(batch_x, [batch_x.shape[0], x.shape[-1]])
                       
                        yield (batch_x, batch_x)
                       
                       
            self.ae.fit_generator(gen(x, batch_size), steps_per_epoch=int(x.shape[0]/batch_size), epochs=epochs, callbacks=cb, verbose=verbose)
            
        exec_time = time() - t0
        print('Pretraining time: %ds' % (exec_time))
        if y is not None:
            features = self.encoder.predict(x)
            km = KMeans(n_clusters=len(np.unique(y)), n_init=20)
            y_pred = km.fit_predict(features)
            f = open(save_dir + '/' + self.dataset + '/pretrain/results_train_ae.txt', 'w+')
            f.write('|==> ACC: %.4f,  NMI: %.4f  <==| \n' % (metrics.acc(y, y_pred), metrics.nmi(y, y_pred)))
            f.write('|==> Execution time: %.5f <==| \n' % (exec_time))
            f.close()
            logfile.close()
        
        
        if metrics.acc(y, y_pred) > acc_prev:
                acc_prev = metrics.acc(y, y_pred)
                self.ae.save_weights(save_dir + '/' + self.dataset + '/pretrain/ae_weights.h5')
        print('Pretrained weights are saved to %s/%s/pretrain/ae_weights.h5' % (save_dir, self.dataset))
        
        print('End pretraining: ', '-' * 60)
        
# Phase 2
# pneumoniamnist
    def train_second_phase_pneumoniamnist(self, x, y=None, kappa=1, n_clusters=2, maxiter=50, batch_size=50, tol=1e-1, validate_interval=140, show_interval=None, save_interval=800, save_dir=PATH_RESULT, aug_train=True):    
    # def train_second_phase(self, x, y=None, kappa=3, n_clusters=3, maxiter=1e5, batch_size=256, tol=1e-1, validate_interval=140, show_interval=None, save_interval=2800, save_dir=PATH_RESULT, aug_train=True):  --best
                
    #def train_second_phase(self, x, y=None, kappa=3, n_clusters=10, maxiter=10, batch_size=, tol=1e-2, validate_interval=140, show_interval=None, save_interval=2800, save_dir=PATH_RESULT, aug_train=True):
        #init
        number_of_samples = x.shape[0]
        print(number_of_samples)
        img_h = int(math.sqrt(x.shape[1]))
        img_w = int(math.sqrt(x.shape[1]))
        print ('Num of sample', number_of_samples )
        print('img_h  ',  img_h)
        print ('img_w', img_w )
        #logging file
        import csv, os
        if not os.path.exists(save_dir):
            os.makedirs(save_dir)
        logfile = open(save_dir + '/' + self.dataset + '/phase_2/train_dynAE_gamma=' + str(self.gamma) + '_log.csv', 'w')
        logfile1 = open(save_dir + '/' + self.dataset + '/phase_2/train_dynAE_gamma=' + str(self.gamma) + '_log_MAX.csv', 'w')
        #logwriter = csv.DictWriter(logfile, fieldnames=['iter', 'acc', 'nmi', 'ari', 'acc_unconf', 'nmi_unconf', 'acc_conf', 'nmi_conf', 'nb_unconf', 'nb_conf', 'fr', 'fd', 'loss'])
        logwriter = csv.DictWriter(logfile, fieldnames=['iter', 'acc', 'nmi', 'ari', 'loss', 'trueN', 'FalseN', 'trueAF', 'FalseAF', 'accN','accF', 'ID_global_mean', 'ID_local_mean', 'PC_ID_mean', 'ID_global_error', 'ID_local_error', 'FT'])
        logwriter1 = csv.DictWriter(logfile1, fieldnames=['iter', 'accMax','nmi'])
        logwriter.writeheader()
        logwriter1.writeheader()
        #intervals config
        print('begin clustering:', '-' * 60)
        if save_interval is None: 
            save_interval = int(maxiter)  # only save the initial and final model
        print('Save interval ', save_interval)
        if show_interval is None:
            show_interval = int(np.ceil(number_of_samples/batch_size))*20
        print('show interval ', show_interval)  

        # Step 1: initialize cluster centers using k-means
        t1 = time()
        #print('Initializing cluster centers with k-means.')
        centers_emb, centers_img, y_pred, _ = self.generate_centers(x, n_clusters)
      #  print(x)
        
        #print("Centroids : ")
        #print("----------------------------------------------------------------------------------")
        #draw_centers(n_clusters, centers_img, img_h=img_h, img_w=img_w)
        
       # dists, nn_idx = torch.topk(x, k=5)
        
        # Step 2: beta1 and beta2
        #beta1, beta2 = self.generate_beta(kappa, n_clusters)
    
        # Step 3: deep clustering
        loss = 0
        index = 0
        nb_conf_prev = x.shape[0]
        index_array = np.arange(x.shape[0])
        #delta_kappa = 0.3 * kappa
        #sess = tf.keras.backend.get_session()
        first_time = 1
      #  print ('index_array', index_array)
   #      Alpha = 0.6--  best best 634
    #    Alpha = 0.5   -- best
  #      Beta = 0.6   -- best
     #   Beta = 0.6    best best 634
    
       # Alpha = 0.6   --- best medmnisr
    #    Beta = 0.6   --- best medmnit
        
        
        #Alpha = 0.6   breastmnist 73 nmi 4
        #Beta = 0.7    breastmnist 73 nmi 4
        
      ## good for poneu  Alpha = 0.6      #.5
      ##good for ponue  Beta = 0.7  #.5
       ##  gooooooooooooooooood Alpha = 0.6
       ## goooooooood  Beta = 0.7
        
        Alpha = 0.6
        Beta = 0.1
        
      #  Beta2 = 0.2
    #    number_neighbours = 5  --best
      #  number_neighbours = 3  ---   best 81
        number_neighbours = 3
        #smallest_d = 0.0001
        #d = 0.0001
        #dd = 0
        acc_prev = 0
        acc_prev_sc = 0
       # batch_size = 256
       # wcss = []
    #    x_emb = self.encoder.predict(x)
     #   for i in range(1,11):
      #    kmeans = KMeans(n_clusters= i, n_init=20, random_state=0)
       #   kmeans = kmeans.fit(x_emb)
          
        #  wcss.append(kmeans.inertia_)
        #plt.plot(range(1,11), wcss)
        #plt.title('T')
        #plt.xlabel('T')
        #plt.ylabel('T')
        #plt.show()
      # good 77  validate_interval = 1
       # validate_interval = 1
        print('N_B1_B2_Alpha', number_neighbours, Beta, Alpha)
        accuracy_unconflicted_point = 0
        accuracy_conflicted_point = 0
        unconflicted_point = 0
        conflicted_point = 0
        
        for ite in range(int(maxiter)):
           
            if ite % validate_interval == 0:
                
               # x_emb = self.encoder.predict(x)
                x_emb = self.predict_encoder(x)
                
                centers_embs = x_emb
                
                centers_embs = centers_embs
               
                
                #nearest_neighbours = NearestNeighbors(n_neighbors=number_neighbours, algorithm='ball_tree').fit(x_emb)
                #x_emb = x_emb.astype(np.float32)
             #   x_emb = x_emb.astype(np.float64)
               
                
                nearest_neighbours = NearestNeighbors(n_neighbors=number_neighbours, algorithm='ball_tree').fit(x_emb)
                
              
                    
               # print('ddd-33')
                _, indices = nearest_neighbours.kneighbors(x_emb)
                #print('ddd-333')
                x_emb_neighbours = x_emb[indices]
                #print('ddd-3')
                # Selecting the unconflicted points
                unconflicted_point_indices = []
                conflicted_point_indices = []
                for i in range(len(x_emb)):
                    biggest_d = 0
                    smallest_d = 10000
                    smallest_dd = 0
                    for j in range(number_neighbours - 1):
                      #  print('ddd-4')
                        d = distance.euclidean(x_emb[i], x_emb_neighbours[i, j + 1, :])
                        if biggest_d < d:
                            biggest_d = d
                        if smallest_d > d:
                            smallest_d = d
                    if (smallest_d / biggest_d) > Alpha:  
                        unconflicted_point_indices.append(i)
                    else:
                        conflicted_point_indices.append(i)
                print('unconflicted_point_indices ', len(unconflicted_point_indices))
                print('conflicted_point_indices ', len(conflicted_point_indices))
                unconflicted_point = len(unconflicted_point_indices)
                conflicted_point = len(conflicted_point_indices)
                #smallest_d = 0.0001       
                smallest_dd = 0
                for i in range(len(x_emb)):
                    sum_neighbours = 0
                    number_nn = 0
                  #  smallest_dd = 0
                    if i in unconflicted_point_indices:
                        for j in range(number_neighbours - 1):
                            d = distance.euclidean(x_emb[i], x_emb_neighbours[i, j + 1, :])
                            #print ('d', d)
                            if smallest_d > d:
                                smallest_d = d 
                            dd = d - smallest_d
                            #print ('smallest_d', smallest_d)
                            #dd = smallest_d / d
                            #print ('dd', dd)
                           # if smallest_dd < d:
                            #    smallest_dd = d
                            #else:
                            #    smallest_dd = smallest_dd 
                            #dd = smallest_dd / d
                            # dd = d  /  (smallest_d * smallest_d )
                           
                           # print ('dd ', dd)
                            if dd < Beta:
                                #if dd <= Beta2:
                                sum_neighbours = sum_neighbours + x_emb_neighbours[i, j + 1, :] 
                                number_nn = number_nn + 1
                        if number_nn != 0:
                            centers_embs[i] = sum_neighbours / number_nn
                        else:
                            centers_embs[i] = x_emb[i]
               
                x_emb = self.predict_encoder(x)
                ##
                #act='relu'
                #dims=[x.shape[-1], 500, 500, 2000, 2]
                
                #n_stacks = len(dims) - 1
                #init = VarianceScaling(scale=1. / 3., mode='fan_in', distribution='uniform')
                # input
                
                #x = Input(shape=(dims[0],), name='input_encoder')
                #print('xxxxxxx',x)
                #h = x
                # internal layers in encoder
                #for i in range(n_stacks-1):
                #    h = Dense(dims[i + 1], activation=act, kernel_initializer=init, name='encoder_%d' % i)(h)
                # hidden layer
                #h = Dense(dims[-1], kernel_initializer=init, name='encoder_%d' % (n_stacks - 1))(h)  # hidden layer, features are extracted from here
                #encoder = Model(inputs=x, outputs=h, name='encoder')
                #plot_model(encoder, show_shapes=True, show_layer_names=True, to_file=self.visualisation_dir + '/graph/FcEncoder.png')
                
                ##
               
                #print ('x_emb' , x_emb )
                #x_emb_u = self.predict_encoder(unconflicted_point_indices)
               # print ('x_emb' , x_emb )
               # print ('unconflicted_point_indices' , unconflicted_point_indices )
                accuracy_unconflicted_point = unconflicted_point / (unconflicted_point + conflicted_point)
                accuracy_conflicted_point = conflicted_point / (unconflicted_point + conflicted_point)
                print ('accuracy_unconflicted_point : ', accuracy_unconflicted_point)
                print ('accuracy_conflicted_point : ', accuracy_conflicted_point)
                
                #print ('x_emb_u' , x_emb_u )
                km = KMeans(n_clusters=self.n_clusters, n_init=20)
                y_pred = km.fit_predict(x_emb)
               
                #print('uncon00')
                
                
                #y_pred_sc = DBSCAN(eps=0.25, min_samples=9).fit_predict(x_emb)
              #  y_pred_sc= Birch(branching_factor = 50, n_clusters = self.n_clusters, threshold = 1.5).fit_predict(x_emb)
                #y_pred_sc= GaussianMixture(self.n_clusters, covariance_type='full', random_state=0).fit_predict(x_emb)
               # y_pred_sc = SpectralClustering(n_clusters=self.n_clusters).fit_predict(x_emb)
                #y_pred_sc = AgglomerativeClustering(n_clusters=self.n_clusters, affinity='euclidean', linkage='ward').fit_predict(x_emb)
                avg_loss = loss / validate_interval
                loss = 0
               # acc_prev = 0
                acc = 0
                if y is not None:
                 
                    km = KMeans(n_clusters=self.n_clusters, n_init=20)
                #km = kmedoids(n_clusters=self.n_clusters, n_init=20)
                    y_pred = km.fit_predict(x_emb)
                    
                   
                  #y_pred_sc = DBSCAN(eps=0.25, min_samples=9).fit_predict(x_emb)
                  #  y_pred_sc = SpectralClustering(n_clusters=self.n_clusters).fit_predict(x_emb)
                   # y_pred_sc= Birch(branching_factor = 50, n_clusters = self.n_clusters, threshold = 1.5).fit_predict(x_emb)
                   # y_pred_sc= GaussianMixture(self.n_clusters, covariance_type='full', random_state=0).fit_predict(x_emb)
                    #y_pred_sc = AgglomerativeClustering(n_clusters=self.n_clusters, affinity='euclidean', linkage='ward').fit_predict(x_emb)
# ##  New Part                    
                    true_neighbours = 0
                    false_neighbours = 0
                    true_neighbours_after_filtering = 0
                    false_neighbours_after_filtering = 0
                    #smallest_d = 0.0001
                    smallest_dd = 0
                    
                    # Selecting the neighbours of the unconflicted points
                    for i in range(len(x_emb)):
                      #  biggest_d = 0
                      #  smallest_d = 10000
                    #    smallest_dd = 0
                        
                        if i in unconflicted_point_indices:
                            for j in range(number_neighbours - 1):
                                d = distance.euclidean(x_emb[i], x_emb_neighbours[i, j + 1, :])
                                if smallest_d > d:
                                    smallest_d = d 
                                dd = d - smallest_d    
                               # print ('dd : ', dd)
                                #if smallest_dd < d:
                                 #   smallest_dd = d
                                  #  print ('smallest_dd d ' , smallest_dd)
                                #else:
                                 #   smallest_dd = smallest_dd 
                                   # print ('smallest_dd smallest_dd' , smallest_dd)
                                #dd = smallest_dd   /  d
                                    #print ('ddevaluation', dd)
                            
                                if dd < Beta:
                                    #if dd <= Beta2:
                                    if y[i] == y[indices[i, j]]:
                                        true_neighbours_after_filtering = true_neighbours_after_filtering + 1
                                    else:
                                        false_neighbours_after_filtering = false_neighbours_after_filtering + 1
                                
                                if y[i] == y[indices[i, j]]:
                                    true_neighbours = true_neighbours + 1
                                else: 
                                    false_neighbours = false_neighbours + 1
                        
                    
                                    
                    #if prev_acc <= acc: 
                    if (true_neighbours_after_filtering + false_neighbours_after_filtering) != 0:
                        acc_neighbours = true_neighbours / (false_neighbours + true_neighbours)
                        acc_filtered_neighbours = true_neighbours_after_filtering / (true_neighbours_after_filtering + false_neighbours_after_filtering)
                    else:
                        acc_neighbours = 0
                        acc_filtered_neighbours = 0
                    
                    
                   # acc_filtered_neighbours = true_neighbours_after_filtering / (true_neighbours_after_filtering + false_neighbours_after_filtering) 
                    print ('true_neighbours : ', true_neighbours)
                    print ('false_neighbours : ', false_neighbours)
                    print ('true_neighbours_after_filtering : ', true_neighbours_after_filtering)
                    print ('false_neighbours_after_filtering : ', false_neighbours_after_filtering)
                    print ('acc_neighbours : ', acc_neighbours)
                    print ('acc_filtered_neighbours : ', acc_filtered_neighbours)
                   
                    km = KMeans(n_clusters=self.n_clusters, n_init=20)
                    y_pred = km.fit_predict(x_emb)
                    
                    
                    
                    #y_pred_sc = DBSCAN(eps=0.25, min_samples=9).fit_predict(x_emb)
                #    y_pred_sc = SpectralClustering(n_clusters=self.n_clusters).fit_predict(x_emb)
                 #   y_pred_sc= Birch(branching_factor = 50, n_clusters = self.n_clusters, threshold = 1.5).fit_predict(x_emb)
                  #  y_pred_sc= GaussianMixture(self.n_clusters, covariance_type='full', random_state=0).fit_predict(x_emb)
                  #  y_pred_sc = AgglomerativeClustering(n_clusters=self.n_clusters, affinity='euclidean', linkage='ward').fit_predict(x_emb)
                    #y_mapped = map_vector_to_clusters(y, y_neighbours)
                   
                    ## End of new part
                    
                    acc = np.round(metrics.acc(y, y_pred), 5)
                    nmi = np.round(metrics.nmi(y, y_pred), 5)
                    ari = np.round(metrics.ari(y, y_pred), 5)
                    
                 
                  #  acc_sc = np.round(metrics.acc(y, y_pred_sc), 5)
                  #  nmi_sc = np.round(metrics.nmi(y, y_pred_sc), 5)
                #    ari_sc = np.round(metrics.ari(y, y_pred_sc), 5)
                    
                   # acc_prev = acc
                    ID = [] 
                    PC_ID = []
                    for k in range(self.n_clusters):
                      ID.append(computeID(x_emb[y_pred==k]))
                      PC_ID.append(computePC_ID(x_emb[y_pred==k]))
                    ID = np.asarray(ID)
                    PC_ID = np.asarray(PC_ID)
                    ID_global_mean = np.mean(ID)
                    ID_local_mean = np.mean(ID, axis=1) 
                    ID_global_error = np.std(ID)
                    ID_local_error = np.std(ID, axis=1)
                    PC_ID_mean = np.mean(PC_ID)
                    FT = PC_ID_mean - ID_global_mean
                    #print ("FT", FT)
                    print ("ID_local_mean = ", ID_local_mean)
                    print ("ID = ", ID_global_mean)
                    print ("LID = ", PC_ID_mean)
                    
                    if acc > acc_prev:
                        #acc_prev = acc
                        logdict = dict(iter=ite, acc=acc, nmi=nmi, ari=ari,  loss=avg_loss, trueN=true_neighbours, FalseN=false_neighbours, trueAF=true_neighbours_after_filtering, FalseAF=false_neighbours_after_filtering,  accN=acc_neighbours, accF=acc_filtered_neighbours, ID_global_mean=ID_global_mean, ID_local_mean=ID_local_mean.tolist(), PC_ID_mean=PC_ID_mean, ID_global_error=ID_global_error, ID_local_error=ID_local_error.tolist(), FT=FT)
                        logwriter.writerow(logdict)
                        logfile.flush()
                    
                    print('K-means: iter %d, acc=%.5f, nmi=%.5f, ari=%.5f, loss=%.5f' % (ite, acc, nmi, ari,  avg_loss))
                #    print('Spectral clustering: iter %d, acc=%.5f, nmi=%.5f, ari=%.5f, loss=%.5f' % (ite, acc_sc, nmi_sc, ari_sc, avg_loss))
                  
                    #print("")
                    #print("----------------------------------------------------------------------------------")
                    print("Centroids : ")
                    print("----------------------------------------------------------------------------------")
                    #centers_emb, centers_img, y_pred, _ = self.generate_centers(x, n_clusters)
                    #draw_centers(n_clusters, centers_img, img_h=img_h, img_w=img_w)
                    print('out')     
                else:
                    x_embed = self.encoder.predict(x)
                    silhouette_avg = silhouette_score(x_embed, y_pred) 
                    print("For number of clusters = " +  str(self.nearest_neighbours) + " The average silhouette_score is :", silhouette_avg) 
                    print('x_embeddddd',x_embed) 
            
           
           # s = min((index+1) * batch_size, x.shape[0])
            #print('s',s)
            #print('nnjbj',index * batch_size)
            #idx = index_array[(index + 1) * batch_size: min((index+1) * batch_size, x.shape[-1])]
            #idx = index_array[index * batch_size: min((index+1) * batch_size, x.shape[0])]
           # idx = index_array[index * 10: min((index+1) * 10, x.shape[0])]
            idx = index_array[index * batch_size: min((index+1) * batch_size, x.shape[0])]
            
          #wit best  idx = index_array[index * 10: min((index+1) * 10, x.shape[0])]
           
            X_img = x[idx]
            
            #X_emb = self.predict_encoder(X_img)
            #X_emb = self.predict_encoder(X_img)
            
            #X_emb = self.predict_encoder(X_img)
            #    x_emb = self.predict_encoder(x[idx])
            
            x_emb = self.predict_encoder(x)
            Y_encoder = centers_embs[idx]
           
                        #print('Y_encoder',Y_encoder)
            Y_autoencoder = self.decoder.predict(centers_embs[idx])
            
              # Y_autoencoder = self.decoder.predict(x_emb)
            X_transformed = self.random_transform(X_img, ws=self.ws, hs=self.hs, rot=self.rot, scale=self.scale) if aug_train else X_img
           
            
            losses = self.train_on_batch_dynAE(X_transformed, Y_encoder, Y_autoencoder)
           
            
            
   
            loss = loss + losses
           
            #index = index + 1 if (index + 1) * batch_size <= x.shape[0] else 0
            
            index = index + 1 if (index + 1) * batch_size <= int(x.shape[0]) else 0
            

          #  print('acc', acc)
           # print('acc_prev1', acc_prev)
            
            if acc > acc_prev:
                acc_prev = acc
           #     print('acc_prev2', acc_prev)
                print('saving model to ::', save_dir + '/' + self.dataset + '/phase_2/ae_weights.h5')
                self.ae.save_weights(save_dir + '/' + self.dataset + '/phase_2/ae_weights.h5')
                print('save the best acc in W', acc )
                logdict1 = dict(iter=ite, accMax=acc_prev, nmi=nmi)
                logwriter1.writerow(logdict1)
                logfile1.flush()
         #   if acc_sc > acc_prev:
          #      acc_prev_sc = acc_sc
           #     print('saving model to ::', save_dir + '/' + self.dataset + '/phase_2/ae_weights.h5')
            #    self.ae.save_weights(save_dir + '/' + self.dataset + '/phase_2/ae_weights.h5')
             #   logdict1 = dict(iter=ite, accMax=acc_prev, nmi_sc=nmi_sc)
              #  logwriter1.writerow(logdict1)
               # logfile1.flush()
        
    
                
                print('Clustering time: %ds' % (time() - t1))
                print('End clustering:', '-' * 60)
                print("Centroids : ")
                print("----------------------------------------------------------------------------------")
                #centers_emb, centers_img, y_pred, _ = self.generate_centers(x, n_clusters)
                #draw_centers(n_clusters, centers_img, img_h=img_h, img_w=img_w)
                
        
        logfile.close()
       # print('training time: ', time() - t0)
    #    print('training: ', '-' * 60)
        print('Clustering time: %ds' % (time() - t1))
        print('End clustering:', '-' * 60)
        print("Centroids : ")
        print('end')
        print("----------------------------------------------------------------------------------")
       # centers_emb, centers_img, y_pred, _ = self.generate_centers(x, n_clusters)
    #    draw_centers(n_clusters, centers_img, img_h=img_h, img_w=img_w)
       
        
        return y_pred
    


# phase 3
##pneumoniamnist

    def train_dynAE_New_pneumoniamnist(self, x, y=None, kappa=3, n_clusters=2, n_overclusters=10, maxiter=50, batch_size=256, tol=1e-2, validate_interval=140, show_interval=None, save_interval=2800, save_dir=PATH_RESULT, aug_train=True):
    #def train_dynAE_New(self, x, y=None, kappa=3, n_clusters=10, n_overclusters=40, maxiter=1e5, batch_size=256, tol=1e-2, validate_interval=140, show_interval=2000, save_interval=2800, save_dir=PATH_RESULT, aug_train=True):
        #init
        number_of_samples = x.shape[0]
        img_h = int(math.sqrt(x.shape[1]))
        img_w = int(math.sqrt(x.shape[1]))
        print('maxiter',maxiter)
        #logging file
        import csv, os
        if not os.path.exists(save_dir):
            os.makedirs(save_dir)
        logfile = open(save_dir + '/' + self.dataset + '/cluster/train_dynAE_gamma=' + str(self.gamma) + '_logآNew.csv', 'w')
        #logwriter = csv.DictWriter(logfile, fieldnames=['iter', 'acc', 'nmi', 'ari', 'acc_unconf', 'nmi_unconf', 'acc_conf', 'nmi_conf', 'nb_unconf', 'nb_conf', 'fr', 'fd', 'loss'])
        logwriter = csv.DictWriter(logfile, fieldnames=['iter', 'acc', 'nmi', 'ari', 'acc_unconf', 'nmi_unconf', 'acc_conf', 'nmi_conf', 'nb_unconf', 'nb_conf', 'loss'])
        logwriter.writeheader()

        #intervals config
        print('Begin clustering:', '-' * 60)
        if save_interval is None: 
            save_interval = int(maxiter)  # only save the initial and final model
        print('Save interval ', save_interval)
        if show_interval is None:
            show_interval = int(np.ceil(number_of_samples/batch_size))*20
        print('show interval ', show_interval)  
        
        # Step 1: initialize cluster centers using k-means
        t1 = time()
        print('Initializing cluster centers with k-means.')
        
        #new just for the centroid
        #x_emb = self.predict_encoder(x)
        #km = KMeans(n_clusters=self.n_clusters, n_init=20)
        #y_pred = km.fit_predict(x_emb)
        #new
        
        centers_emb, centers_img, y_pred, _ = self.generate_centers(x, n_clusters)
        overcenters_emb, overcenters_img, _, _ = self.generate_centers(x, n_overclusters)

        # Step 2: beta1 and beta2
        #beta1, beta2 = self.generate_beta(kappa, n_clusters)
        beta1 = 0.0
        #beta2 = 0.25
        beta2 = 0.25
        beta1_overclustering = 0.0
       # beta2_overclustering = 0.15
        beta2_overclustering = 0.15
        n_clusters = 2
        n_overclusters = 10
        kappa = 1
        
        print ('beta1 beta2 b1ove b2ove n_overclusters',beta1,beta2,beta1_overclustering,beta2_overclustering,n_overclusters)            
        # Step 3: deep clustering
        loss = 0
        index = 0
        nb_conf_prev = x.shape[0]
        index_array = np.arange(x.shape[0])
        delta_kappa = 0.1 * kappa
        #sess = tf.keras.backend.get_session()
        #first_time = 1
       #### validate_interval = 1  #### best and maxiter =1e5
      #  validate_interval = 1
        
        print("Centroids : ")
        print("----------------------------------------------------------------------------------")
        draw_centers(n_clusters, centers_img, img_h=img_h, img_w=img_w)
        
        
        
        for ite in range(int(maxiter)):
         #   print ('range inside ite  ite' , range(int(maxiter))  , ite )
            if ite % validate_interval == 0:
            #    print('inside if ite')
                x_emb = self.encoder.predict(x)
              
              #  print('xemb   ',x_emb)
                q = q_mat(x_emb, centers_emb)
               # print('change ypred ', ite % validate_interval)
                y_pred = q.argmax(1) 
                #print('y_pred after q', y_pred)
                
                avg_loss = loss / validate_interval
                loss = 0.
                if ite > 0:
                    nb_conf_prev = nb_conf 
                nb_unconf, nb_conf = self.compute_nb_conflicted_data(x, centers_emb, beta1, beta2)
                #update centers
                if nb_conf >= nb_conf_prev:
                    #centers_emb, centers_img, _, _ = self.generate_centers(x, n_clusters)
                   # print("update centers", nb_conf , nb_conf_prev )
                    beta1 = beta1 - (delta_kappa / n_clusters)
                    beta2 = beta2 - (delta_kappa / n_clusters)
                    #beta1 = 0.5
                    #beta2 = 0.25
                    delta_kappa = 0.1 * kappa
                    kappa = delta_kappa
                    #first_time = 0
                    #print("update confidences")
                
              #  print(' before y')    

                if y is not None:
                    #y_mapped = map_vector_to_clusters(y, y_pred)
                    #x_emb = self.predict_encoder(x)
                    #y_encoder, y_autoencoder = generate_supervisory_signals(x_emb, x, centers_emb, centers_img, beta1, beta2)
                    #y_encoder_true = centers_emb[y_mapped]
                    #grad_loss_dynAE = sess.run(self.grad_loss_dynAE, feed_dict={'input_dynAE:0': x, 'target1_dynAE:0': y_encoder, 'target2_dynAE:0': y_autoencoder})
                    #grad_loss_pseudo_supervised = sess.run(self.grad_loss_pseudo_supervised, feed_dict={'input_dynAE:0': x, 'target1_dynAE:0': y_encoder})
                    #grad_loss_self_supervised = sess.run(self.grad_loss_self_supervised, feed_dict={'input_dynAE:0': x, 'target2_dynAE:0': y_autoencoder})
                    #grad_loss_supervised = sess.run(self.grad_loss_supervised, feed_dict={'input_dynAE:0': x, 'target3_dynAE:0': y_encoder_true})
                    
                    acc = np.round(metrics.acc(y, y_pred), 5)
                    print('accccccc',acc)
                    nmi = np.round(metrics.nmi(y, y_pred), 5)
                    ari = np.round(metrics.ari(y, y_pred), 5) 
                    #fr = np.round(metrics.cos_grad(grad_loss_supervised, grad_loss_dynAE), 5)
                    #fd = np.round(metrics.cos_grad(grad_loss_self_supervised, grad_loss_pseudo_supervised), 5)
                    acc_unconf, nmi_unconf, acc_conf, nmi_conf = self.compute_acc_and_nmi_conflicted_data(x, y, centers_emb, beta1, beta2)
                    #logdict = dict(iter=ite, acc=acc, nmi=nmi, ari=ari, acc_unconf=acc_unconf, nmi_unconf=nmi_unconf, acc_conf=acc_conf, nmi_conf=nmi_conf, nb_unconf=nb_unconf, nb_conf=nb_conf, fr=fr, fd=fd, loss=avg_loss)
                    logdict = dict(iter=ite, acc=acc, nmi=nmi, ari=ari, acc_unconf=acc_unconf, nmi_unconf=nmi_unconf, acc_conf=acc_conf, nmi_conf=nmi_conf, nb_unconf=nb_unconf, nb_conf=nb_conf, loss=avg_loss)
                    logwriter.writerow(logdict)
                    logfile.flush()
                    #print('Iter %d: acc=%.5f, nmi=%.5f, ari=%.5f, acc_unconf=%.5f, nmi_unconf=%.5f, acc_conf=%.5f, nmi_conf=%.5f, nb_unconf=%d, nb_conf=%d, fr=%.5f, fd=%.5f, loss=%.5f' % (ite, acc, nmi, ari, acc_unconf, nmi_unconf, acc_conf, nmi_conf, nb_unconf, nb_conf, fr, fd, avg_loss))
                    print('Iter %d: acc=%.5f, nmi=%.5f, ari=%.5f, acc_unconf=%.5f, nmi_unconf=%.5f, acc_conf=%.5f, nmi_conf=%.5f, nb_unconf=%d, nb_conf=%d, loss=%.5f' % (ite, acc, nmi, ari, acc_unconf, nmi_unconf, acc_conf, nmi_conf, nb_unconf, nb_conf, avg_loss))
                    print("The number of unconflicted data points is : " + str(nb_unconf))
                    print("The number of conflicted data points is : " + str(nb_conf))
                else:
                    x_embed = self.encoder.predict(x)
                    silhouette_avg = silhouette_score(x_embed, y_pred) 
                    print("For number of clusters = " +  str(self.n_clusters) + " The average silhouette_score is :", silhouette_avg) 
                    #logdict = dict(iter=ite, silhouette_avg=silhouette_avg, loss=avg_loss)
                    #logwriter.writerow(logdict)
                    #logfile.flush()
               # print('nb_conf', nb_conf )
        #        print('xshape', x.shape[0])
         #       print('tol', tol)
                    
                if(nb_conf / x.shape[0]) < tol:
                #    print('clooooooose')
                    logfile.close()
                    break

            if ite % show_interval == 0:
               
                print("")
                print("----------------------------------------------------------------------------------")
                print("Centroids : ")
                print("----------------------------------------------------------------------------------")
               # draw_centers(n_clusters, centers_img, img_h=img_h, img_w=img_w)
                
                # save intermediate model
            
            if ite % save_interval == 0:
                
                z = self.predict_encoder(x)
                q1 = q_mat(z, centers_emb)
               # y1_pred = q1.argmax(1)
                y1_pred = q1.argmax(1)
               

                pca = PCA(n_components=2).fit(z)
                z_2d = pca.transform(z)
                centers_2d = pca.transform(centers_emb)

                # save states for visualization
                np.save(self.visualisation_dir + '/embeddings/' + self.dataset + '/vis_' + str(ite) + '.npy', {'z_2d': z_2d, 'centers_2d': centers_2d, 'y_pred': y1_pred})

                print('saving model to: ', save_dir + '/' + self.dataset + '/cluster/ae_' + str(ite) + '_weights.h5')
                self.ae.save_weights(save_dir + '/' + self.dataset + '/cluster/ae_' + str(ite) + '_weights.h5')

            # train on batch
            if ite % 2 == 2:
                idx = index_array[index * batch_size: min((index+1) * batch_size, x.shape[0])]
                X_img = x[idx]
                X_emb = self.predict_encoder(X_img)
                Y_encoder, Y_autoencoder = generate_supervisory_signals(X_emb, X_img, centers_emb, centers_img, beta1, beta2)
                X_transformed = self.random_transform(X_img, ws=self.ws, hs=self.hs, rot=self.rot, scale=self.scale) if aug_train else X_img
                losses = self.train_on_batch_dynAE(X_transformed, Y_encoder, Y_autoencoder)
                loss = loss + losses
            #    print('loooose')
                index = index + 1 if (index + 1) * batch_size <= x.shape[0] else 0
            else:
                idx = index_array[index * batch_size: min((index+1) * batch_size, x.shape[0])]
                X_img = x[idx]
                X_emb = self.predict_encoder(X_img)
                Y_encoder, Y_autoencoder = generate_supervisory_signals(X_emb, X_img, overcenters_emb, overcenters_img, beta1_overclustering, beta2_overclustering)
                X_transformed = self.random_transform(X_img, ws=self.ws, hs=self.hs, rot=self.rot, scale=self.scale) if aug_train else X_img
                losses = self.train_on_batch_dynAE(X_transformed, Y_encoder, Y_autoencoder)
                loss = loss + losses
                index = index + 1 if (index + 1) * batch_size <= x.shape[0] else 0
                
        logfile.close()
        print('saving model to:', save_dir + '/' + self.dataset + '/cluster/ae_weights.h5')
        self.ae.save_weights(save_dir + '/' + self.dataset + '/cluster/ae_weights.h5')
        print('Clustering time: %ds' % (time() - t1))
        print('End clustering:', '-' * 60)

        return y_pred
    
    

# bloodmnist
#pretrain
        
    def train_ae_bloodmnist(self, x, y=None, optimizer='adam', epochs=20, batch_size=50, save_dir=PATH_RESULT, verbose=1, aug_train=True):
      #  print('Begin pretraining: bloodmnist', '-' * 60)
        self.ae.compile(optimizer=optimizer, loss='mse')
        import csv, os
        if not os.path.exists(save_dir):
            os.makedirs(save_dir)
        logfile = open(save_dir + '/' + self.dataset + '/pretrain/pretrain.csv', 'w')
        logwriter = csv.DictWriter(logfile, fieldnames=['iter', 'ID_global_mean', 'ID_local_mean', 'PC_ID_mean', 'ID_global_error', 'ID_local_error', 'FT','acc','nmi'])
        logwriter.writeheader()
       
   
        cb = []
        acc_prev = 0
      
        if y is not None and verbose > 0:
            class PrintACC(callbacks.Callback):
                def __init__(self, x, y, encoder):
                    self.x = x
                    self.y = y
                    self.encoder = encoder
                    super(PrintACC, self).__init__()

                def on_epoch_end(self, epoch, logs=None):
                    if epoch % 20 != 0:
                        return
                    features = self.encoder.predict(self.x)
                    km = KMeans(n_clusters=len(np.unique(y)), n_init=20)
                    y_pred = km.fit_predict(features)
                   
               #     print(' '*8 + '|==>  acc: %.4f,  nmi: %.4f  <==|'% (metrics.acc(y, y_pred), metrics.nmi(y, y_pred)))
            class PrintID_LID(callbacks.Callback):
                def __init__(self, x, y, encoder):
                    self.x = x
                    self.y = y
                    self.encoder = encoder
                    super(PrintID_LID, self).__init__()

                def on_epoch_end(self, epoch, logs=None):
                    features = self.encoder.predict(self.x)
                    km = KMeans(n_clusters=len(np.unique(y)), n_init=20)
                    y_pred = km.fit_predict(features)
                    ID = [] 
                    PC_ID = []
                    for k in range(len(np.unique(y))):
                       ID.append(computeID(features[y_pred==k]))
                       PC_ID.append(computePC_ID(features[y_pred==k]))
                    ID = np.asarray(ID)
                    PC_ID = np.asarray(PC_ID)
                    ID_global_mean = np.mean(ID)
                    ID_local_mean = np.mean(ID, axis=1) 
                    ID_global_error = np.std(ID)
                    ID_local_error = np.std(ID, axis=1)
                    PC_ID_mean = np.mean(PC_ID)
                    FT = PC_ID_mean - ID_global_mean
                  #  print ("ID = ", ID_global_mean)
                  #  print ("LID = ", PC_ID_mean)
                    logdict = dict(iter=epoch, ID_global_mean=ID_global_mean, ID_local_mean=ID_local_mean, PC_ID_mean=PC_ID_mean, ID_global_error=ID_global_error, ID_local_error=ID_local_error, FT=FT, acc=metrics.acc(y, y_pred) ,nmi=metrics.nmi(y, y_pred) )
                    logwriter.writerow(logdict)
                    logfile.flush()
                    
                    
            cb.append(PrintACC(x, y, self.encoder))
            cb.append(PrintID_LID(x, y, self.encoder))
        # begin pretraining
        t0 = time()
       
        if not aug_train:
         
            self.ae.fit(x, y, batch_size=batch_size, epochs=epochs, callbacks=cb, verbose=verbose)
        else:
       
            def gen(x, batch_size):
                if len(x.shape) > 2:  # image
           #         print('5--')
                    gen0 = self.datagen.flow(y, shuffle=True, batch_size=batch_size)
                    while True:
                        batch_x = gen0.next()
                        yield (batch_x, batch_x)
                else:
            #        print('6--')
                    width = int(np.sqrt(x.shape[-1]))
                   
             #       print('7--')
                    if width * width == x.shape[-1]:  # gray
                        
                        im_shape = [-1, width, width, 1]
              #          print('8--')
                    else:  # RGB
                        width = int(np.sqrt(x.shape[-1] / 3.0))
                        width = int(np.sqrt(x.shape[-1] / 3.0))
                 #       print(width)
                        im_shape = [-1, width, width, 3]
                        im_shape = [-1, width, width, 3]
                        print(im_shape)
              
                    gen0 = self.datagen.flow(np.reshape(x, im_shape), shuffle=True, batch_size=batch_size)
                    #np.array(x).shape[-1]
                    #print('1010--')
                    while True:
                      ##  batch_x = x_generator.next()
                        batch_x = gen0.next()
                     #   print('101010--')
                        batch_x = np.reshape(batch_x, [batch_x.shape[0], x.shape[-1]])
                        ##batch_x = np.reshape(batch_x, [batch_x.shape[0], x.shape[-1]])
                      #  print('1011--')
                        yield (batch_x, batch_x)
                       # print('101111--')
                        #print('11--')
                       
            self.ae.fit_generator(gen(x, batch_size), steps_per_epoch=int(x.shape[0]/batch_size), epochs=epochs, callbacks=cb, verbose=verbose)
            #print('12--')    
        exec_time = time() - t0
        print('Pretraining time: %ds' % (exec_time))
        if y is not None:
            features = self.encoder.predict(x)
            km = KMeans(n_clusters=len(np.unique(y)), n_init=20)
            y_pred = km.fit_predict(features)
            f = open(save_dir + '/' + self.dataset + '/pretrain/results_train_ae.txt', 'w+')
            f.write('|==> ACC: %.4f,  NMI: %.4f  <==| \n' % (metrics.acc(y, y_pred), metrics.nmi(y, y_pred)))
            f.write('|==> Execution time: %.5f <==| \n' % (exec_time))
            f.close()
        logfile.close()
        
        
       # if metrics.acc(y, y_pred) > acc_prev:
           #     acc_prev = metrics.acc(y, y_pred)
        self.ae.save_weights(save_dir + '/' + self.dataset + '/pretrain/ae_weights.h5')
        print('Pretrained weights are saved tkk %s/%s/pretrain/ae_weights.h5' % (save_dir, self.dataset))
        
        print('End pretraining: ', '-' * 60)
        
    
# bloodmnist
##phase 2
    def train_second_phase_bloodmnist(self, x, y=None, kappa=1, n_clusters=3, maxiter=15, batch_size=256, tol=1e-1, validate_interval=140, show_interval=None, save_interval=800, save_dir=PATH_RESULT, aug_train=True):    
    # def train_second_phase(self, x, y=None, kappa=3, n_clusters=3, maxiter=1e5, batch_size=256, tol=1e-1, validate_interval=140, show_interval=None, save_interval=2800, save_dir=PATH_RESULT, aug_train=True):  --best
                
    #def train_second_phase(self, x, y=None, kappa=3, n_clusters=10, maxiter=10, batch_size=, tol=1e-2, validate_interval=140, show_interval=None, save_interval=2800, save_dir=PATH_RESULT, aug_train=True):
        #init
        number_of_samples = x.shape[0]
        print(number_of_samples)
        img_h = int(math.sqrt(x.shape[0]))
        img_w = int(math.sqrt(x.shape[0]))
        print ('Num of sample', number_of_samples )
        print('img_h  ',  img_h)
        print ('img_w', img_w )
        #logging file
        import csv, os
        if not os.path.exists(save_dir):
            os.makedirs(save_dir)
        logfile = open(save_dir + '/' + self.dataset + '/phase_2/train_dynAE_gamma=' + str(self.gamma) + '_log.csv', 'w')
        logfile1 = open(save_dir + '/' + self.dataset + '/phase_2/train_dynAE_gamma=' + str(self.gamma) + '_log_MAX.csv', 'w')
        #logwriter = csv.DictWriter(logfile, fieldnames=['iter', 'acc', 'nmi', 'ari', 'acc_unconf', 'nmi_unconf', 'acc_conf', 'nmi_conf', 'nb_unconf', 'nb_conf', 'fr', 'fd', 'loss'])
        logwriter = csv.DictWriter(logfile, fieldnames=['iter', 'acc', 'nmi', 'ari', 'loss', 'trueN', 'FalseN', 'trueAF', 'FalseAF', 'accN','accF', 'ID_global_mean', 'ID_local_mean', 'PC_ID_mean', 'ID_global_error', 'ID_local_error', 'FT'])
        logwriter1 = csv.DictWriter(logfile1, fieldnames=['iter', 'accMax','nmi'])
        logwriter.writeheader()
        logwriter1.writeheader()
        #intervals config
        print('begin clustering:', '-' * 60)
        if save_interval is None: 
            save_interval = int(maxiter)  # only save the initial and final model
        print('Save interval ', save_interval)
        if show_interval is None:
            show_interval = int(np.ceil(number_of_samples/batch_size))*20
        print('show interval ', show_interval)  

        # Step 1: initialize cluster centers using k-means
        t1 = time()
        #print('Initializing cluster centers with k-means.')
        centers_emb, centers_img, y_pred, _ = self.generate_centers(x, n_clusters)
      #  print(x)
        
        #print("Centroids : ")
        #print("----------------------------------------------------------------------------------")
        #draw_centers(n_clusters, centers_img, img_h=img_h, img_w=img_w)
        
       # dists, nn_idx = torch.topk(x, k=5)
        
        # Step 2: beta1 and beta2
        #beta1, beta2 = self.generate_beta(kappa, n_clusters)
    
        # Step 3: deep clustering
        loss = 0
        index = 0
        nb_conf_prev = x.shape[0]
        index_array = np.arange(x.shape[0])
        #delta_kappa = 0.3 * kappa
        #sess = tf.keras.backend.get_session()
        first_time = 1
      #  print ('index_array', index_array)
   #      Alpha = 0.6--  best best 634
    #    Alpha = 0.5   -- best
  #      Beta = 0.6   -- best
     #   Beta = 0.6    best best 634
    
       # Alpha = 0.6   --- best medmnisr
    #    Beta = 0.6   --- best medmnit
        
        
        #Alpha = 0.6   breastmnist 73 nmi 4
        #Beta = 0.7    breastmnist 73 nmi 4
        
      ## good for poneu  Alpha = 0.6
      ##good for ponue  Beta = 0.7
        Alpha = 0.5
        Beta = 0.6
        
    #   Beta1 = 0.9
        #Beta2 = 0.6
        
      #  Beta2 = 0.2
    #    number_neighbours = 5  --best
        number_neighbours = 5
        #smallest_d = 0.0001
        #d = 0.0001
        #dd = 0
        acc_prev = 0
        acc_prev_sc = 0
       # batch_size = 256
       # wcss = []
    #    x_emb = self.encoder.predict(x)
     #   for i in range(1,11):
      #    kmeans = KMeans(n_clusters= i, n_init=20, random_state=0)
       #   kmeans = kmeans.fit(x_emb)
          
        #  wcss.append(kmeans.inertia_)
        #plt.plot(range(1,11), wcss)
        #plt.title('T')
        #plt.xlabel('T')
        #plt.ylabel('T')
        #plt.show()
        #validate_interval = 1
        print('N_B1_B2_Alpha', number_neighbours, Beta, Alpha)
        accuracy_unconflicted_point = 0
        accuracy_conflicted_point = 0
        unconflicted_point = 0
        conflicted_point = 0

      #  self.ae.load_weights(save_dir + '/' + self.dataset + '/pretrain/ae_weights.h5')
        for ite in range(int(maxiter)):
           
            if ite % validate_interval == 0:
                
               # x_emb = self.encoder.predict(x)
                x_emb = self.predict_encoder(x)
                
                centers_embs = x_emb
                
                centers_embs = centers_embs
               
                
                #nearest_neighbours = NearestNeighbors(n_neighbors=number_neighbours, algorithm='ball_tree').fit(x_emb)
                #x_emb = x_emb.astype(np.float32)
                x_emb = x_emb.astype(np.float64)
               
                
                nearest_neighbours = NearestNeighbors(n_neighbors=number_neighbours, algorithm='ball_tree').fit(x_emb)
                
              
                    
               # print('ddd-33')
                _, indices = nearest_neighbours.kneighbors(x_emb)
                #print('ddd-333')
                x_emb_neighbours = x_emb[indices]
                #print('ddd-3')
                # Selecting the unconflicted points
                unconflicted_point_indices = []
                conflicted_point_indices = []
                for i in range(len(x_emb)):
                    biggest_d = 0
                    smallest_d = 10000
                    smallest_dd = 0
                    for j in range(number_neighbours - 1):
                      #  print('ddd-4')
                        d = distance.euclidean(x_emb[i], x_emb_neighbours[i, j + 1, :])
                        if biggest_d < d:
                            biggest_d = d
                        if smallest_d > d:
                            smallest_d = d
                    if (smallest_d / biggest_d) > Alpha:
                        unconflicted_point_indices.append(i)
                    else:
                        conflicted_point_indices.append(i)
       #         print('unconflicted_point_indices ', len(unconflicted_point_indices))
        #        print('conflicted_point_indices ', len(conflicted_point_indices))
                unconflicted_point = len(unconflicted_point_indices)
                conflicted_point = len(conflicted_point_indices)
                #smallest_d = 0.0001       
                smallest_dd = 0
                for i in range(len(x_emb)):
                    sum_neighbours = 0
                    number_nn = 0
                  #  smallest_dd = 0
                    if i in unconflicted_point_indices:
                        for j in range(number_neighbours - 1):
                            d = distance.euclidean(x_emb[i], x_emb_neighbours[i, j + 1, :])
                            #print ('d', d)
                            if smallest_d > d:
                                smallest_d = d 
                            dd = d - smallest_d
                            #print ('smallest_d', smallest_d)
                            #dd = smallest_d / d
                            #print ('dd', dd)
                           # if smallest_dd < d:
                            #    smallest_dd = d
                            #else:
                            #    smallest_dd = smallest_dd 
                            #dd = smallest_dd / d
                            # dd = d  /  (smallest_d * smallest_d )
                           
                           # print ('dd ', dd)
                            if dd < Beta:
                                #if dd <= Beta2:
                                sum_neighbours = sum_neighbours + x_emb_neighbours[i, j + 1, :] 
                                number_nn = number_nn + 1
                        if number_nn != 0:
                            centers_embs[i] = sum_neighbours / number_nn
                        else:
                            centers_embs[i] = x_emb[i]
               
                x_emb = self.predict_encoder(x)
                ##
                #act='relu'
                #dims=[x.shape[-1], 500, 500, 2000, 2]
                
                #n_stacks = len(dims) - 1
                #init = VarianceScaling(scale=1. / 3., mode='fan_in', distribution='uniform')
                # input
                
                #x = Input(shape=(dims[0],), name='input_encoder')
                #print('xxxxxxx',x)
                #h = x
                # internal layers in encoder
                #for i in range(n_stacks-1):
                #    h = Dense(dims[i + 1], activation=act, kernel_initializer=init, name='encoder_%d' % i)(h)
                # hidden layer
                #h = Dense(dims[-1], kernel_initializer=init, name='encoder_%d' % (n_stacks - 1))(h)  # hidden layer, features are extracted from here
                #encoder = Model(inputs=x, outputs=h, name='encoder')
                #plot_model(encoder, show_shapes=True, show_layer_names=True, to_file=self.visualisation_dir + '/graph/FcEncoder.png')
                
                ##
               
                #print ('x_emb' , x_emb )
                #x_emb_u = self.predict_encoder(unconflicted_point_indices)
               # print ('x_emb' , x_emb )
               # print ('unconflicted_point_indices' , unconflicted_point_indices )
                accuracy_unconflicted_point = unconflicted_point / (unconflicted_point + conflicted_point)
                accuracy_conflicted_point = conflicted_point / (unconflicted_point + conflicted_point)
                print ('accuracy_unconflicted_point : ', accuracy_unconflicted_point)
                print ('accuracy_conflicted_point : ', accuracy_conflicted_point)
                
                #print ('x_emb_u' , x_emb_u )
                km = KMeans(n_clusters=self.n_clusters, n_init=20)
                y_pred = km.fit_predict(x_emb)
               
                #print('uncon00')
                
                
                #y_pred_sc = DBSCAN(eps=0.25, min_samples=9).fit_predict(x_emb)
              #  y_pred_sc= Birch(branching_factor = 50, n_clusters = self.n_clusters, threshold = 1.5).fit_predict(x_emb)
                #y_pred_sc= GaussianMixture(self.n_clusters, covariance_type='full', random_state=0).fit_predict(x_emb)
               # y_pred_sc = SpectralClustering(n_clusters=self.n_clusters).fit_predict(x_emb)
                #y_pred_sc = AgglomerativeClustering(n_clusters=self.n_clusters, affinity='euclidean', linkage='ward').fit_predict(x_emb)
                avg_loss = loss / validate_interval
                loss = 0
               # acc_prev = 0
                acc = 0
                if y is not None:
                 
                    km = KMeans(n_clusters=self.n_clusters, n_init=20)
                #km = kmedoids(n_clusters=self.n_clusters, n_init=20)
                    y_pred = km.fit_predict(x_emb)
                    
                   
                  #y_pred_sc = DBSCAN(eps=0.25, min_samples=9).fit_predict(x_emb)
                  #  y_pred_sc = SpectralClustering(n_clusters=self.n_clusters).fit_predict(x_emb)
                   # y_pred_sc= Birch(branching_factor = 50, n_clusters = self.n_clusters, threshold = 1.5).fit_predict(x_emb)
                   # y_pred_sc= GaussianMixture(self.n_clusters, covariance_type='full', random_state=0).fit_predict(x_emb)
                    #y_pred_sc = AgglomerativeClustering(n_clusters=self.n_clusters, affinity='euclidean', linkage='ward').fit_predict(x_emb)
# ##  New Part                    
                    true_neighbours = 0
                    false_neighbours = 0
                    true_neighbours_after_filtering = 0
                    false_neighbours_after_filtering = 0
                    #smallest_d = 0.0001
                    smallest_dd = 0
                    
                    # Selecting the neighbours of the unconflicted points
                    for i in range(len(x_emb)):
                      #  biggest_d = 0
                      #  smallest_d = 10000
                    #    smallest_dd = 0
                        
                        if i in unconflicted_point_indices:
                            for j in range(number_neighbours - 1):
                                d = distance.euclidean(x_emb[i], x_emb_neighbours[i, j + 1, :])
                                if smallest_d > d:
                                    smallest_d = d 
                                dd = d - smallest_d    
                               # print ('dd : ', dd)
                                #if smallest_dd < d:
                                 #   smallest_dd = d
                                  #  print ('smallest_dd d ' , smallest_dd)
                                #else:
                                 #   smallest_dd = smallest_dd 
                                   # print ('smallest_dd smallest_dd' , smallest_dd)
                                #dd = smallest_dd   /  d
                                    #print ('ddevaluation', dd)
                            
                                if dd < Beta:
                                    #if dd <= Beta2:
                                    if y[i] == y[indices[i, j]]:
                                        true_neighbours_after_filtering = true_neighbours_after_filtering + 1
                                    else:
                                        false_neighbours_after_filtering = false_neighbours_after_filtering + 1
                                
                                if y[i] == y[indices[i, j]]:
                                    true_neighbours = true_neighbours + 1
                                else: 
                                    false_neighbours = false_neighbours + 1
                        
                    
                                    
                    #if prev_acc <= acc: 
                    if (true_neighbours_after_filtering + false_neighbours_after_filtering) != 0:
                        acc_neighbours = true_neighbours / (false_neighbours + true_neighbours)
                        acc_filtered_neighbours = true_neighbours_after_filtering / (true_neighbours_after_filtering + false_neighbours_after_filtering)
                    else:
                        acc_neighbours = 0
                        acc_filtered_neighbours = 0
                    
                    
                   # acc_filtered_neighbours = true_neighbours_after_filtering / (true_neighbours_after_filtering + false_neighbours_after_filtering) 
                    print ('true_neighbours : ', true_neighbours)
                    print ('false_neighbours : ', false_neighbours)
                    print ('true_neighbours_after_filtering : ', true_neighbours_after_filtering)
                    print ('false_neighbours_after_filtering : ', false_neighbours_after_filtering)
                    print ('acc_neighbours : ', acc_neighbours)
                    print ('acc_filtered_neighbours : ', acc_filtered_neighbours)
                   
                    km = KMeans(n_clusters=self.n_clusters, n_init=20)
                    y_pred = km.fit_predict(x_emb)
                    
                    
                    
                    #y_pred_sc = DBSCAN(eps=0.25, min_samples=9).fit_predict(x_emb)
                #    y_pred_sc = SpectralClustering(n_clusters=self.n_clusters).fit_predict(x_emb)
                 #   y_pred_sc= Birch(branching_factor = 50, n_clusters = self.n_clusters, threshold = 1.5).fit_predict(x_emb)
                  #  y_pred_sc= GaussianMixture(self.n_clusters, covariance_type='full', random_state=0).fit_predict(x_emb)
                  #  y_pred_sc = AgglomerativeClustering(n_clusters=self.n_clusters, affinity='euclidean', linkage='ward').fit_predict(x_emb)
                    #y_mapped = map_vector_to_clusters(y, y_neighbours)
                   
                    ## End of new part
                    
                    acc = np.round(metrics.acc(y, y_pred), 5)
                    nmi = np.round(metrics.nmi(y, y_pred), 5)
                    ari = np.round(metrics.ari(y, y_pred), 5)
                    
                 
                  #  acc_sc = np.round(metrics.acc(y, y_pred_sc), 5)
                  #  nmi_sc = np.round(metrics.nmi(y, y_pred_sc), 5)
                #    ari_sc = np.round(metrics.ari(y, y_pred_sc), 5)
                    
                   # acc_prev = acc
                    ID = [] 
                    PC_ID = []
                    for k in range(self.n_clusters):
                      ID.append(computeID(x_emb[y_pred==k]))
                      PC_ID.append(computePC_ID(x_emb[y_pred==k]))
                    ID = np.asarray(ID)
                    PC_ID = np.asarray(PC_ID)
                    ID_global_mean = np.mean(ID)
                    ID_local_mean = np.mean(ID, axis=1) 
                    ID_global_error = np.std(ID)
                    ID_local_error = np.std(ID, axis=1)
                    PC_ID_mean = np.mean(PC_ID)
                    FT = PC_ID_mean - ID_global_mean
                    #print ("FT", FT)
                    print ("ID_local_mean = ", ID_local_mean)
                    print ("ID = ", ID_global_mean)
                    print ("LID = ", PC_ID_mean)
                    
                    if acc > acc_prev:
                        #acc_prev = acc
                        logdict = dict(iter=ite, acc=acc, nmi=nmi, ari=ari,  loss=avg_loss, trueN=true_neighbours, FalseN=false_neighbours, trueAF=true_neighbours_after_filtering, FalseAF=false_neighbours_after_filtering,  accN=acc_neighbours, accF=acc_filtered_neighbours, ID_global_mean=ID_global_mean, ID_local_mean=ID_local_mean.tolist(), PC_ID_mean=PC_ID_mean, ID_global_error=ID_global_error, ID_local_error=ID_local_error.tolist(), FT=FT)
                        logwriter.writerow(logdict)
                        logfile.flush()
                    
                    print('K-means: iter %d, acc=%.5f, nmi=%.5f, ari=%.5f, loss=%.5f' % (ite, acc, nmi, ari,  avg_loss))
                #    print('Spectral clustering: iter %d, acc=%.5f, nmi=%.5f, ari=%.5f, loss=%.5f' % (ite, acc_sc, nmi_sc, ari_sc, avg_loss))
                  
                    #print("")
                    #print("----------------------------------------------------------------------------------")
                    print("Centroids : ")
                    print("----------------------------------------------------------------------------------")
                    #centers_emb, centers_img, y_pred, _ = self.generate_centers(x, n_clusters)
                    #draw_centers(n_clusters, centers_img, img_h=img_h, img_w=img_w)
                    print('out')     
                else:
                    x_embed = self.encoder.predict(x)
                    silhouette_avg = silhouette_score(x_embed, y_pred) 
                    print("For number of clusters = " +  str(self.nearest_neighbours) + " The average silhouette_score is :", silhouette_avg) 
         #           print('x_embeddddd',x_embed) 
            
           
           # s = min((index+1) * batch_size, x.shape[0])
            #print('s',s)
            #print('nnjbj',index * batch_size)
            #idx = index_array[(index + 1) * batch_size: min((index+1) * batch_size, x.shape[-1])]
            #idx = index_array[index * batch_size: min((index+1) * batch_size, x.shape[0])]
           # idx = index_array[index * 10: min((index+1) * 10, x.shape[0])]
            idx = index_array[index * batch_size: min((index+1) * batch_size, x.shape[0])]
            
          #wit best  idx = index_array[index * 10: min((index+1) * 10, x.shape[0])]
           
            X_img = x[idx]
            
            #X_emb = self.predict_encoder(X_img)
            #X_emb = self.predict_encoder(X_img)
            
            #X_emb = self.predict_encoder(X_img)
            #    x_emb = self.predict_encoder(x[idx])
            
            x_emb = self.predict_encoder(x)
            Y_encoder = centers_embs[idx]
           
                        #print('Y_encoder',Y_encoder)
            Y_autoencoder = self.decoder.predict(centers_embs[idx])
            
              # Y_autoencoder = self.decoder.predict(x_emb)
            X_transformed = self.random_transform(X_img, ws=self.ws, hs=self.hs, rot=self.rot, scale=self.scale) if aug_train else X_img
           
            
            losses = self.train_on_batch_dynAE(X_transformed, Y_encoder, Y_autoencoder)
           
           # print('after losses', losses)
            
   
            loss = loss + losses
           
            #index = index + 1 if (index + 1) * batch_size <= x.shape[0] else 0
            
            index = index + 1 if (index + 1) * batch_size <= int(x.shape[0]) else 0
            

            logdict1 = dict(iter=ite, accMax=acc_prev, nmi=nmi)
            logwriter1.writerow(logdict1)
            logfile1.flush()

        
    
                
        print('Clustering time: %ds' % (time() - t1))
        print('End clustering:', '-' * 60)
        print("Centroids : ")
        print("----------------------------------------------------------------------------------")
                #centers_emb, centers_img, y_pred, _ = self.generate_centers(x, n_clusters)
                #draw_centers(n_clusters, centers_img, img_h=img_h, img_w=img_w)
        self.ae.save_weights(save_dir + '/' + self.dataset + '/phase_2/ae_weights.h5')        
        logfile.close()
       # print('training time: ', time() - t0)
    #    print('training: ', '-' * 60)
        print('Clustering time: %ds' % (time() - t1))
        print('End clustering:', '-' * 60)
        print("Centroids : ")
        print('end')
        print("----------------------------------------------------------------------------------")
       # centers_emb, centers_img, y_pred, _ = self.generate_centers(x, n_clusters)
    #    draw_centers(n_clusters, centers_img, img_h=img_h, img_w=img_w)
       
        
        return y_pred
    
##phase3
##bloodmnist

    def train_dynAE_New_bloodmnist(self, x, y=None, kappa=1, n_clusters=7, n_overclusters=10, maxiter=1e5, batch_size=256, tol=1e-2, validate_interval=140, show_interval=None, save_interval=2800, save_dir=PATH_RESULT, aug_train=True):
    #def train_dynAE_New(self, x, y=None, kappa=3, n_clusters=10, n_overclusters=40, maxiter=1e5, batch_size=256, tol=1e-2, validate_interval=140, show_interval=2000, save_interval=2800, save_dir=PATH_RESULT, aug_train=True):
        #init
        number_of_samples = x.shape[0]
        img_h = int(math.sqrt(x.shape[1]))
        img_w = int(math.sqrt(x.shape[1]))
        print('maxiter',maxiter)
        #logging file
        import csv, os
        if not os.path.exists(save_dir):
            os.makedirs(save_dir)
        logfile = open(save_dir + '/' + self.dataset + '/cluster/train_dynAE_gamma=' + str(self.gamma) + '_logآNew.csv', 'w')
        #logwriter = csv.DictWriter(logfile, fieldnames=['iter', 'acc', 'nmi', 'ari', 'acc_unconf', 'nmi_unconf', 'acc_conf', 'nmi_conf', 'nb_unconf', 'nb_conf', 'fr', 'fd', 'loss'])
        logwriter = csv.DictWriter(logfile, fieldnames=['iter', 'acc', 'nmi', 'ari', 'acc_unconf', 'nmi_unconf', 'acc_conf', 'nmi_conf', 'nb_unconf', 'nb_conf', 'loss'])
        logwriter.writeheader()

        #intervals config
        print('Begin clustering:', '-' * 60)
        if save_interval is None: 
            save_interval = int(maxiter)  # only save the initial and final model
        print('Save interval ', save_interval)
        if show_interval is None:
            show_interval = int(np.ceil(number_of_samples/batch_size))*20
        print('show interval ', show_interval)  
        
        # Step 1: initialize cluster centers using k-means
        t1 = time()
        print('Initializing cluster centers with k-means.')
        
        #new just for the centroid
        #x_emb = self.predict_encoder(x)
        #km = KMeans(n_clusters=self.n_clusters, n_init=20)
        #y_pred = km.fit_predict(x_emb)
        #new
        
        centers_emb, centers_img, y_pred, _ = self.generate_centers(x, n_clusters)
        overcenters_emb, overcenters_img, _, _ = self.generate_centers(x, n_overclusters)

        # Step 2: beta1 and beta2
        #beta1, beta2 = self.generate_beta(kappa, n_clusters)
        beta1 = 0.0
        #beta2 = 0.25
        beta2 = 0.15
        beta1_overclustering = 0.0
       # beta2_overclustering = 0.15
        beta2_overclustering = 0.15
        n_clusters = 2
        n_overclusters = 10
        kappa = 1
        acc_prev = 0
        print ('beta1 beta2 b1ove b2ove n_overclusters',beta1,beta2,beta1_overclustering,beta2_overclustering,n_overclusters)            
        # Step 3: deep clustering
        loss = 0
        index = 0
        nb_conf_prev = x.shape[0]
        index_array = np.arange(x.shape[0])
        delta_kappa = 0.1 * kappa
        #sess = tf.keras.backend.get_session()
        #first_time = 1
        #validate_interval = 1
        
        print("Centroids : ")
        print("----------------------------------------------------------------------------------")
        draw_centers(n_clusters, centers_img, img_h=img_h, img_w=img_w)
        
        
        
        for ite in range(int(maxiter)):
           # print ('range inside ite  ite' , range(int(maxiter))  , ite )
            if ite % validate_interval == 0:
                #print('inside if ite')
                x_emb = self.encoder.predict(x)
              
              #  print('xemb   ',x_emb)
                q = q_mat(x_emb, centers_emb)
               # print('change ypred ', ite % validate_interval)
                y_pred = q.argmax(1) 
                #print('y_pred after q', y_pred)
                
                avg_loss = loss / validate_interval
                loss = 0.
                if ite > 0:
                    nb_conf_prev = nb_conf 
                nb_unconf, nb_conf = self.compute_nb_conflicted_data(x, centers_emb, beta1, beta2)
                #update centers
                if nb_conf >= nb_conf_prev:
                    #centers_emb, centers_img, _, _ = self.generate_centers(x, n_clusters)
                  #  print("update centers", nb_conf , nb_conf_prev )
                    beta1 = beta1 - (delta_kappa / n_clusters)
                    beta2 = beta2 - (delta_kappa / n_clusters)
                    #beta1 = 0.5
                    #beta2 = 0.25
                    delta_kappa = 0.1 * kappa
                    kappa = delta_kappa
                    #first_time = 0
                    #print("update confidences")
                
              #  print(' before y')    

                if y is not None:
                    #y_mapped = map_vector_to_clusters(y, y_pred)
                    #x_emb = self.predict_encoder(x)
                    #y_encoder, y_autoencoder = generate_supervisory_signals(x_emb, x, centers_emb, centers_img, beta1, beta2)
                    #y_encoder_true = centers_emb[y_mapped]
                    #grad_loss_dynAE = sess.run(self.grad_loss_dynAE, feed_dict={'input_dynAE:0': x, 'target1_dynAE:0': y_encoder, 'target2_dynAE:0': y_autoencoder})
                    #grad_loss_pseudo_supervised = sess.run(self.grad_loss_pseudo_supervised, feed_dict={'input_dynAE:0': x, 'target1_dynAE:0': y_encoder})
                    #grad_loss_self_supervised = sess.run(self.grad_loss_self_supervised, feed_dict={'input_dynAE:0': x, 'target2_dynAE:0': y_autoencoder})
                    #grad_loss_supervised = sess.run(self.grad_loss_supervised, feed_dict={'input_dynAE:0': x, 'target3_dynAE:0': y_encoder_true})
                    
                    acc = np.round(metrics.acc(y, y_pred), 5)
                  #  print('accccccc',acc)
                    nmi = np.round(metrics.nmi(y, y_pred), 5)
                    ari = np.round(metrics.ari(y, y_pred), 5) 
                    #fr = np.round(metrics.cos_grad(grad_loss_supervised, grad_loss_dynAE), 5)
                    #fd = np.round(metrics.cos_grad(grad_loss_self_supervised, grad_loss_pseudo_supervised), 5)
                    acc_unconf, nmi_unconf, acc_conf, nmi_conf = self.compute_acc_and_nmi_conflicted_data(x, y, centers_emb, beta1, beta2)
                    #logdict = dict(iter=ite, acc=acc, nmi=nmi, ari=ari, acc_unconf=acc_unconf, nmi_unconf=nmi_unconf, acc_conf=acc_conf, nmi_conf=nmi_conf, nb_unconf=nb_unconf, nb_conf=nb_conf, fr=fr, fd=fd, loss=avg_loss)
                    logdict = dict(iter=ite, acc=acc, nmi=nmi, ari=ari, acc_unconf=acc_unconf, nmi_unconf=nmi_unconf, acc_conf=acc_conf, nmi_conf=nmi_conf, nb_unconf=nb_unconf, nb_conf=nb_conf, loss=avg_loss)
                    logwriter.writerow(logdict)
                    logfile.flush()
                    #print('Iter %d: acc=%.5f, nmi=%.5f, ari=%.5f, acc_unconf=%.5f, nmi_unconf=%.5f, acc_conf=%.5f, nmi_conf=%.5f, nb_unconf=%d, nb_conf=%d, fr=%.5f, fd=%.5f, loss=%.5f' % (ite, acc, nmi, ari, acc_unconf, nmi_unconf, acc_conf, nmi_conf, nb_unconf, nb_conf, fr, fd, avg_loss))
                    print('Iter %d: acc=%.5f, nmi=%.5f, ari=%.5f, acc_unconf=%.5f, nmi_unconf=%.5f, acc_conf=%.5f, nmi_conf=%.5f, nb_unconf=%d, nb_conf=%d, loss=%.5f' % (ite, acc, nmi, ari, acc_unconf, nmi_unconf, acc_conf, nmi_conf, nb_unconf, nb_conf, avg_loss))
                    print("The number of unconflicted data points is : " + str(nb_unconf))
                    print("The number of conflicted data points is : " + str(nb_conf))
                    if acc > acc_prev:
                        acc_prev = acc
                        self.ae.save_weights(save_dir + '/' + self.dataset + '/cluster/ae_N_weights.h5')
                    
                else:
                    x_embed = self.encoder.predict(x)
                    silhouette_avg = silhouette_score(x_embed, y_pred) 
                    print("For number of clusters = " +  str(self.n_clusters) + " The average silhouette_score is :", silhouette_avg) 
                    #logdict = dict(iter=ite, silhouette_avg=silhouette_avg, loss=avg_loss)
                    #logwriter.writerow(logdict)
                    #logfile.flush()
             #   print('nb_conf', nb_conf )
             #   print('xshape', x.shape[0])
             #   print('tol', tol)
                    
                if(nb_conf / x.shape[0]) < tol:
              #      print('clooooooose')
                    logfile.close()
                    break

            if ite % show_interval == 0:
               
                print("")
                print("----------------------------------------------------------------------------------")
                print("Centroids : ")
                print("----------------------------------------------------------------------------------")
               # draw_centers(n_clusters, centers_img, img_h=img_h, img_w=img_w)
                
                # save intermediate model
            
            if ite % save_interval == 0:
                
                z = self.predict_encoder(x)
                q1 = q_mat(z, centers_emb)
               # y1_pred = q1.argmax(1)
                y1_pred = q1.argmax(1)
               

                pca = PCA(n_components=2).fit(z)
                z_2d = pca.transform(z)
                centers_2d = pca.transform(centers_emb)

                # save states for visualization
                np.save(self.visualisation_dir + '/embeddings/' + self.dataset + '/vis_' + str(ite) + '.npy', {'z_2d': z_2d, 'centers_2d': centers_2d, 'y_pred': y1_pred})

                print('saving model to: ', save_dir + '/' + self.dataset + '/cluster/ae_' + str(ite) + '_weights.h5')
                self.ae.save_weights(save_dir + '/' + self.dataset + '/cluster/ae_' + str(ite) + '_weights.h5')

            # train on batch
            if ite % 2 == 2:
                idx = index_array[index * batch_size: min((index+1) * batch_size, x.shape[0])]
                X_img = x[idx]
                X_emb = self.predict_encoder(X_img)
                Y_encoder, Y_autoencoder = generate_supervisory_signals(X_emb, X_img, centers_emb, centers_img, beta1, beta2)
                X_transformed = self.random_transform(X_img, ws=self.ws, hs=self.hs, rot=self.rot, scale=self.scale) if aug_train else X_img
                losses = self.train_on_batch_dynAE(X_transformed, Y_encoder, Y_autoencoder)
                loss = loss + losses
              #  print('loooose')
                index = index + 1 if (index + 1) * batch_size <= x.shape[0] else 0
            else:
                idx = index_array[index * batch_size: min((index+1) * batch_size, x.shape[0])]
                X_img = x[idx]
                X_emb = self.predict_encoder(X_img)
                Y_encoder, Y_autoencoder = generate_supervisory_signals(X_emb, X_img, overcenters_emb, overcenters_img, beta1_overclustering, beta2_overclustering)
                X_transformed = self.random_transform(X_img, ws=self.ws, hs=self.hs, rot=self.rot, scale=self.scale) if aug_train else X_img
                losses = self.train_on_batch_dynAE(X_transformed, Y_encoder, Y_autoencoder)
                loss = loss + losses
                index = index + 1 if (index + 1) * batch_size <= x.shape[0] else 0
                
        logfile.close()
        print('saving model to:', save_dir + '/' + self.dataset + '/cluster/ae_weights.h5')
        self.ae.save_weights(save_dir + '/' + self.dataset + '/cluster/ae_weights.h5')
        print('Clustering time: %ds' % (time() - t1))
        print('End clustering:', '-' * 60)

        return y_pred
        



    
    
  
    
 
      
    
    def compute_F1(self, x, y):
        features = self.predict_encoder(x)
        km = KMeans(n_clusters=len(np.unique(y)), n_init=20)
        y_pred = km.fit_predict(features)
        print(' '*8 + '|==>  f1micro: %.4f, f1macro: %.4f  <==|'% (f1_score(y,y_pred,average='micro'), f1_score(y,y_pred,average='macro') ))

    def compute_cm(self, x, y):   
        features = self.predict_encoder(x)
        km = KMeans(n_clusters=len(np.unique(y)), n_init=20)
        y_pred = km.fit_predict(features)
        cm = metrics.clustering_metrics(y, y_pred)
        acc, nmi, adjscore, f1_macro, precision_macro, f1_micro, precision_micro = cm.evaluationClusterModelFromLabel()
        print(' '*8 + '|==>  acc: %.4f,  nmi: %.4f  <==|'% (acc, nmi))
       
        print(' '*8 + '|==>  adjscore: %.4f <==|'% (adjscore ))
        print(' '*8 + '|==>  f1_macro: %.4f, precision_macro: %.4f  <==|'% (f1_macro, precision_macro))
        print(' '*8 + '|==>  f1_micro: %.4f, precision_micro: %.4f  <==|'% (f1_micro, precision_micro))
        

    def generate_centers(self, x, n_clusters):
        features = self.predict_encoder(x)
        kmeans = KMeans(n_clusters=n_clusters, n_init=10) 
        y_pred = kmeans.fit_predict(features)
        q = q_mat(features, kmeans.cluster_centers_, alpha=.7)
        nn = NearestNeighbors(n_neighbors=1, algorithm='ball_tree').fit(features)
        _, indices = nn.kneighbors(kmeans.cluster_centers_)
        centers_emb = np.reshape(features[indices], (-1, self.encoder.output.shape[1]))
        centers_img = self.decoder.predict(centers_emb)
        return centers_emb, centers_img, y_pred, q
       
    def generate_centers_old(self, x, n_clusters):
        features = self.predict_encoder(x)
        kmeans = KMeans(n_clusters=n_clusters, n_init=10) 
        y_pred = kmeans.fit_predict(features)
        q = q_mat(features, kmeans.cluster_centers_, alpha=1.0)
        nn = NearestNeighbors(n_neighbors=1, algorithm='ball_tree').fit(features)
        _, indices = nn.kneighbors(kmeans.cluster_centers_)
        centers_emb = np.reshape(features[indices], (-1, self.encoder.output.shape[1]))
        centers_img = self.decoder.predict(centers_emb)
        return centers_emb, centers_img, y_pred, q

    def generate_unconflicted_data_index(self, x_img, centers_emb, beta1, beta2):
        x_emb = self.encoder.predict(x_img)
       # print('xemb', x_emb )
        unconf_indices = []
        conf_indices = []
        q = q_mat(x_emb, centers_emb, alpha=1.0)
        confidence1 = q.max(1)
        confidence2 = np.zeros((q.shape[0],))
        a = np.argsort(q, axis=1)[:,-2]  
        for i in range(x_img.shape[0]):
            confidence2[i] = q[i,a[i]]
            if (confidence1[i]) > beta1 and (confidence1[i] - confidence2[i]) > beta2:
                unconf_indices.append(i)
            else:
                conf_indices.append(i)
        unconf_indices = np.asarray(unconf_indices, dtype=int)
        conf_indices = np.asarray(conf_indices, dtype=int)
        
        
        return unconf_indices, conf_indices

    def compute_acc_and_nmi_conflicted_data(self, x, y, centers_emb, beta1, beta2):
        features = self.predict_encoder(x)
        unconf_indices, conf_indices = self.generate_unconflicted_data_index(x, centers_emb, beta1, beta2)
        
        if unconf_indices.size == 0:
            print(' '*8 + "Empty list of unconflicted data")
            acc_unconf = 0
            nmi_unconf = 0
        else:
            x_emb_unconf = self.predict_encoder(x[unconf_indices])
            y_unconf = y[unconf_indices]
            y_pred_unconf = q_mat(x_emb_unconf, centers_emb, alpha=1.0).argmax(axis=1)
            acc_unconf = metrics.acc(y_unconf, y_pred_unconf)
            nmi_unconf = metrics.nmi(y_unconf, y_pred_unconf)
            print(' '*8 + '|==>  acc unconflicted data: %.4f,  nmi unconflicted data: %.4f  <==|'% (acc_unconf, nmi_unconf))

        if conf_indices.size == 0:
            print(' '*8 + "Empty list of conflicted data")
            acc_conf = 0
            nmi_conf = 0
        else:
            x_emb_conf = self.predict_encoder(x[conf_indices])
            y_conf = y[conf_indices]
            y_pred_conf = q_mat(x_emb_conf, centers_emb, alpha=1.0).argmax(axis=1)
            acc_conf = metrics.acc(y_conf, y_pred_conf)
            nmi_conf = metrics.nmi(y_conf, y_pred_conf)
            print(' '*8 + '|==>  acc conflicted data: %.4f,  nmi conflicted data: %.4f  <==|'% (metrics.acc(y_conf, y_pred_conf), metrics.nmi(y_conf, y_pred_conf)))    
        return acc_unconf, nmi_unconf, acc_conf, nmi_conf

    def compute_acc_and_nmi(self, x, y):
       # print('x',x)
        features = self.predict_encoder(x)
        
        
        km = KMeans(n_clusters=len(np.unique(y)), n_init=20)
        y_pred = km.fit_predict(features)
        
        print(' '*8 + '|==>  acc: %.4f,  nmi: %.4f  <==|'% (metrics.acc(y, y_pred), metrics.nmi(y, y_pred) ))
        print(' '*8 + '|==>  f1micro: %.4f, f1macro: %.4f  <==|'% (f1_score(y,y_pred,average='micro'), f1_score(y,y_pred,average='macro') ))
    
    def compute_acc_and_nmi_old(self, x, y):
        features = self.predict_encoder(x)
        km = KMeans(n_clusters=len(np.unique(y)), n_init=20)
        y_pred = km.fit_predict(features)
        print(' '*8 + '|==>  acc: %.4f,  nmi: %.4f  <==|'% (metrics.acc(y, y_pred), metrics.nmi(y, y_pred) ))
        print(' '*8 + '|==>  f1micro: %.4f, f1macro: %.4f  <==|'% (f1_score(y,y_pred,average='micro'), f1_score(y,y_pred,average='macro') ))
        
    
   # def compute_F1(self, x, y):
    #    features = self.predict_encoder(x)
    #    km = KMeans(n_clusters=len(np.unique(y)), n_init=20)
    #    y_pred = km.fit_predict(features)
    #    print(' '*8 + '|==>  f1micro: %.4f, f1macro: %.4f  <==|'% (f1_score(y,y_pred,average='micro'), f1_score(y,y_pred,average='macro') ))
    
    
    def compute_nb_conflicted_data(self, x, centers_emb, beta1, beta2):
        unconf_indices, conf_indices = self.generate_unconflicted_data_index(x, centers_emb, beta1, beta2)
        return unconf_indices.shape[0], conf_indices.shape[0]
        
    def random_transform(self, x, ws=0.1, hs=0.1, rot=10, scale=0.0):
        self.datagen = ImageDataGenerator(width_shift_range=ws, height_shift_range=hs, rotation_range=rot, zoom_range=scale)
        if len(x.shape) > 2:  # image
            return self.datagen.flow(x, shuffle=False, batch_size=x.shape[0]).next()

        # if input a flattened vector, reshape to image before transform
        width = int(np.sqrt(x.shape[-1]))
        if width * width == x.shape[-1]:  # gray
            im_shape = [-1, width, width, 1]
        else:  # RGB
            width = int(np.sqrt(x.shape[-1] / 3.0))
            im_shape = [-1, width, width, 3]
        gen = self.datagen.flow(np.reshape(x, im_shape), shuffle=False, batch_size=x.shape[0])
        return np.reshape(gen.next(), x.shape)     

    def generate_beta(self, kappa, n_clusters):
        beta1 = kappa / n_clusters
        beta2 = beta1 / 2 
        print("Beta1 = " + str(beta1) + " and Beta2 = " + str(beta2))
        return beta1, beta2

    def show_hcImages(self, x, n_hCIndex, image_shape=(28, 28)):
        clusters_hCIndex, _, _ = self.get_clusters_hCIndex(x, n_hCIndex)
        f, axarr = plt.subplots(self.n_clusters, n_hCIndex, figsize=(10, 10))
        for i in range(self.n_clusters):
            for j in range(n_hCIndex):
                image = x[int(clusters_hCIndex[i, j]),:].reshape(image_shape)
                axarr[i, j].imshow(image, cmap="gray", aspect="auto")
                axarr[i, j].get_xaxis().set_visible(False)
                axarr[i, j].get_yaxis().set_visible(False)
        plt.subplots_adjust(wspace=0., hspace=0.)
        plt.show()

    def get_clusters_hCIndex(self, x, n_hCIndex):
        x_emb = self.encoder.predict(x)
        centers_emb, _, _, _ = self.generate_centers(x, self.n_clusters)
        q = q_mat(x_emb, centers_emb)
        conf = q.max(1)
        y_pred = q.argmax(1)
        clusters_hCIndex = np.zeros((self.n_clusters, n_hCIndex))
        clusters_index = [[] for i in range(self.n_clusters)]
        clusters_conf = [[] for i in range(self.n_clusters)]
        for i in range(x.shape[0]):
          clusters_index[y_pred[i]].append(i)
          clusters_conf[y_pred[i]].append(conf[i])
        for i in range(self.n_clusters):
          clusters_index[i] = np.asarray(clusters_index[i])
          clusters_conf[i] = np.asarray(clusters_conf[i])
          for j in range(n_hCIndex):
            l = np.argsort(clusters_conf[i], axis=0)[-(j+1)] 
            index = clusters_index[i][l]
            clusters_hCIndex[i][j] = index
        return clusters_hCIndex, clusters_index, clusters_conf

   
        
        
        
        
    
    
    
    
    ## chestmnist



    def train_ae_retinamnist(self, x, y=None, optimizer='adam', epochs=10, batch_size=50, save_dir=PATH_RESULT, verbose=1, aug_train=True):
    #def train_ae_bloodmnist(self, x, y=None, optimizer='adam', epochs=500, batch_size=256, save_dir=PATH_RESULT, verbose=1, aug_train=True):
     #   print('Begin pretraining: vesselmnist', '-' * 60)
        self.ae.compile(optimizer=optimizer, loss='mse')
        #csv_logger = callbacks.CSVLogger(save_dir + '/' + self.dataset + '/pretrain/log_ae.csv')
        #cb = [csv_logger]
        
        #Prepare log file
        import csv, os
        if not os.path.exists(save_dir):
            os.makedirs(save_dir)
        logfile = open(save_dir + '/' + self.dataset + '/pretrain/pretrain.csv', 'w')
        logwriter = csv.DictWriter(logfile, fieldnames=['iter', 'ID_global_mean', 'ID_local_mean', 'PC_ID_mean', 'ID_global_error', 'ID_local_error', 'FT','acc','nmi'])
        logwriter.writeheader()
        filepath = '/content/drive/My Drive/Colab/DynAE_Amal/results/vesselmnist3d/pretrain/ae_weightsC.h5'
        model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(filepath=filepath, save_weights_only=True, monitor='val_accuracy', verbose=1, mode='max')
        callbacks_list = [model_checkpoint_callback]
        cb = []
        acc_prev = 0
        if y is not None and verbose > 0:
            class PrintACC(callbacks.Callback):
                def __init__(self, x, y, encoder):
                    self.x = x
                    self.y = y
                    self.encoder = encoder
                    super(PrintACC, self).__init__()

                def on_epoch_end(self, epoch, logs=None):
                    if epoch % 20 != 0:
                        return
                    features = self.encoder.predict(self.x)
                    km = KMeans(n_clusters=len(np.unique(y)), n_init=20)
                   
                    y_pred = km.fit_predict(features,callbacks_list)
      #              print(' '*8 + '|==>  acc: %.4f,  nmi: %.4f  <==|'% (metrics.acc(y, y_pred), metrics.nmi(y, y_pred)))
            
            class PrintID_LID(callbacks.Callback):
                def __init__(self, x, y, encoder):
                    self.x = x
                    self.y = y
                    self.encoder = encoder
                    super(PrintID_LID, self).__init__()

                def on_epoch_end(self, epoch, logs=None):
                    features = self.encoder.predict(self.x)
                    km = KMeans(n_clusters=len(np.unique(y)), n_init=20)
                    y_pred = km.fit_predict(features,callbacks_list)
                    ID = [] 
                    PC_ID = []
                    for k in range(len(np.unique(y))):
                       ID.append(computeID(features[y_pred==k]))
                       PC_ID.append(computePC_ID(features[y_pred==k]))
                    ID = np.asarray(ID)
                    PC_ID = np.asarray(PC_ID)
                    ID_global_mean = np.mean(ID)
                    ID_local_mean = np.mean(ID, axis=1) 
                    ID_global_error = np.std(ID)
                    ID_local_error = np.std(ID, axis=1)
                    PC_ID_mean = np.mean(PC_ID)
                    FT = PC_ID_mean - ID_global_mean
                  #  print ("ID = ", ID_global_mean)
                   # print ("LID = ", PC_ID_mean)
                    logdict = dict(iter=epoch, ID_global_mean=ID_global_mean, ID_local_mean=ID_local_mean, PC_ID_mean=PC_ID_mean, ID_global_error=ID_global_error, ID_local_error=ID_local_error, FT=FT, acc=metrics.acc(y, y_pred) ,nmi=metrics.nmi(y, y_pred) )
                    logwriter.writerow(logdict)
                    logfile.flush()
                    
                    
            cb.append(PrintACC(x, y, self.encoder))
            cb.append(PrintID_LID(x, y, self.encoder))
        # begin pretraining
        t0 = time()
        #print('2--')
        if not aug_train:
         #   print('3--')
            self.ae.fit(x, y, batch_size=batch_size, epochs=epochs, callbacks=cb, verbose=verbose)
        else:
          #  print('4--')
       #     print('-=*'*20)
        #    print('Using augmentation for ae')
         #   print('-=*'*20)
            def gen(x, batch_size):
                if len(x.shape) > 2:  # image
                #    print('5--')
                    gen0 = self.datagen.flow(y, shuffle=True, batch_size=batch_size)
                    while True:
                        batch_x = gen0.next()
                        yield (batch_x, batch_x)
                else:
                 #   print('6--')
                    width = int(np.sqrt(x.shape[-1]))
          
                  #  print('7--')
                    if width * width == x.shape[-1]:  # gray
                        
                        im_shape = [-1, width, width, 1]
                   #     print('8--')
                    else:  # RGB
                        width = int(np.sqrt(x.shape[-1] / 3.0))
                        width = int(np.sqrt(x.shape[-1] / 3.0))
                       
                        im_shape = [-1, width, width, 3]
                        im_shape = [-1, width, width, 3]
                    xreshape = np.reshape(x, im_shape)
                    gen0 = self.datagen.flow(np.reshape(x, im_shape), shuffle=True, batch_size=batch_size)
                    while True:
                        batch_x = gen0.next()
                        batch_x = np.reshape(batch_x, [batch_x.shape[0], x.shape[-1]])
                        yield (batch_x, batch_x)
                       
                       
            self.ae.fit_generator(gen(x, batch_size), steps_per_epoch=int(x.shape[0]/batch_size), epochs=epochs, callbacks=cb, verbose=verbose)
        
        exec_time = time() - t0
        print('Pretraining time: %ds' % (exec_time))
        if y is not None:
            features = self.encoder.predict(x)
            km = KMeans(n_clusters=len(np.unique(y)), n_init=20)
            y_pred = km.fit_predict(features,callbacks_list)
            f = open(save_dir + '/' + self.dataset + '/pretrain/results_train_ae.txt', 'w+')
            f.write('|==> ACC: %.4f,  NMI: %.4f  <==| \n' % (metrics.acc(y, y_pred), metrics.nmi(y, y_pred)))
            f.write('|==> Execution time: %.5f <==| \n' % (exec_time))
            f.close()
            logfile.close()
        
        
        if metrics.acc(y, y_pred) > acc_prev:
                acc_prev = metrics.acc(y, y_pred)
                self.ae.save_weights(save_dir + '/' + self.dataset + '/pretrain/ae_weights.h5')
        print('Pretrained weights are saved tkk %s/%s/pretrain/ae_weights.h5' % (save_dir, self.dataset))
        
        print('End pretraining: ', '-' * 60)
        
## chest mnist second
    def train_second_phase_organsmnist(self, x, y=None, kappa=1, n_clusters=2, maxiter=1e5, batch_size=50, tol=1e-1, validate_interval=140, show_interval=None, save_interval=800, save_dir=PATH_RESULT, aug_train=True, callbacks=callbacks_list ):    
    # def train_second_phase(self, x, y=None, kappa=3, n_clusters=3, maxiter=1e5, batch_size=256, tol=1e-1, validate_interval=140, show_interval=None, save_interval=2800, save_dir=PATH_RESULT, aug_train=True):  --best
                
    #def train_second_phase(self, x, y=None, kappa=3, n_clusters=10, maxiter=10, batch_size=, tol=1e-2, validate_interval=140, show_interval=None, save_interval=2800, save_dir=PATH_RESULT, aug_train=True):
        #init
        number_of_samples = x.shape[0]
    #    print(number_of_samples)
        img_h = int(math.sqrt(x.shape[0]))
        img_w = int(math.sqrt(x.shape[0]))
     #   print ('Num of sample', number_of_samples )
      #  print('img_h  ',  img_h)
       # print ('img_w', img_w )
        #logging file
        import csv, os
        if not os.path.exists(save_dir):
            os.makedirs(save_dir)
        logfile = open(save_dir + '/' + self.dataset + '/phase_2/train_dynAE_gamma=' + str(self.gamma) + '_log.csv', 'w')
        logfile1 = open(save_dir + '/' + self.dataset + '/phase_2/train_dynAE_gamma=' + str(self.gamma) + '_log_MAX.csv', 'w')
        #logwriter = csv.DictWriter(logfile, fieldnames=['iter', 'acc', 'nmi', 'ari', 'acc_unconf', 'nmi_unconf', 'acc_conf', 'nmi_conf', 'nb_unconf', 'nb_conf', 'fr', 'fd', 'loss'])
        logwriter = csv.DictWriter(logfile, fieldnames=['iter', 'acc', 'nmi', 'ari', 'loss',  'trueN', 'FalseN', 'trueAF', 'FalseAF', 'accN','accF', 'ID_global_mean', 'ID_local_mean', 'PC_ID_mean', 'ID_global_error', 'ID_local_error', 'FT'])
        logwriter1 = csv.DictWriter(logfile1, fieldnames=['iter', 'accMax','nmi'])
        logwriter.writeheader()
        logwriter1.writeheader()
        #intervals config
    #    print('begin clustering:', '-' * 60)
        if save_interval is None: 
            save_interval = int(maxiter)  # only save the initial and final model
     #   print('Save interval ', save_interval)
        if show_interval is None:
            show_interval = int(np.ceil(number_of_samples/batch_size))*20
      #  print('show interval ', show_interval)  

        # Step 1: initialize cluster centers using k-means
        t1 = time()
        #print('Initializing cluster centers with k-means.')
        centers_emb, centers_img, y_pred, _ = self.generate_centers(x, n_clusters)
     
        # Step 3: deep clustering
        loss = 0
        index = 0
        nb_conf_prev = x.shape[0]
        index_array = np.arange(x.shape[0])
        #delta_kappa = 0.3 * kappa
        #sess = tf.keras.backend.get_session()
        first_time = 1
      #  print ('index_array', index_array)
   #      Alpha = 0.6--  best best 634
    #    Alpha = 0.5   -- best
  #      Beta = 0.6   -- best
     #   Beta = 0.6    best best 634
    
       # Alpha = 0.6   --- best medmnisr
    #    Beta = 0.6   --- best medmnit
        
        
        #Alpha = 0.6   breastmnist 73 nmi 4
        #Beta = 0.7    breastmnist 73 nmi 4
        
      ## good for poneu  Alpha = 0.6
      ##good for ponue  Beta = 0.7
        Alpha = 0.3
        Beta = 0.5
        
    #   Beta1 = 0.9
        #Beta2 = 0.6
        
      #  Beta2 = 0.2
    #    number_neighbours = 5  --best
        number_neighbours = 3
        #smallest_d = 0.0001
        #d = 0.0001
        #dd = 0
        acc_prev = 0
        acc_prev_sc = 0
       # batch_size = 256
       # wcss = []
    #    x_emb = self.encoder.predict(x)
     #   for i in range(1,11):
      #    kmeans = KMeans(n_clusters= i, n_init=20, random_state=0)
       #   kmeans = kmeans.fit(x_emb)
          
        #  wcss.append(kmeans.inertia_)
        #plt.plot(range(1,11), wcss)
        #plt.title('T')
        #plt.xlabel('T')
        #plt.ylabel('T')
        #plt.show()
       # validate_interval = 1
        print('N_B1_B2_Alpha', number_neighbours, Beta, Alpha)
        accuracy_unconflicted_point = 0
        accuracy_conflicted_point = 0
        unconflicted_point = 0
        conflicted_point = 0
        
        
        
         
        for ite in range(int(maxiter)):
           
            if ite % validate_interval == 0:
                
               # x_emb = self.encoder.predict(x)
                x_emb = self.predict_encoder(x)
                
                centers_embs = x_emb
                
                centers_embs = centers_embs
              
                nearest_neighbours = NearestNeighbors(n_neighbors=number_neighbours, algorithm='ball_tree').fit(x_emb)
          #      print('nnnnnnnnnnnnn',nearest_neighbours)
              
                    
               # print('ddd-33')
                _, indices = nearest_neighbours.kneighbors(x_emb)
                
                #print('ddd-333')
                x_emb_neighbours = x_emb[indices]
            #    print('x_emb_neighbours' , x_emb_neighbours)
                # Selecting the unconflicted points
                unconflicted_point_indices = []
                conflicted_point_indices = []
                for i in range(len(x_emb)):
                    biggest_d = 0
                    smallest_d = 10000
                    smallest_dd = 0
                    for j in range(number_neighbours - 1):
                     #   print('ddd-4', x_emb[i])
                      
                        d = distance.euclidean(x_emb[i], x_emb_neighbours[i, j + 1, :])
                      #  print('ddd',d)
                        if biggest_d < d:
                            biggest_d = d
                        if smallest_d > d:
                            smallest_d = d
               #     print('biggestd', biggest_d)      
                    
                    if biggest_d != 0:
                       # print ('biggest_d',biggest_d) 
                        if (smallest_d / biggest_d) > Alpha:
                            unconflicted_point_indices.append(i)
                        else:
                            conflicted_point_indices.append(i)
                print('unconflicted_point_indices ', len(unconflicted_point_indices))
                print('conflicted_point_indices ', len(conflicted_point_indices))
                unconflicted_point = len(unconflicted_point_indices)
                conflicted_point = len(conflicted_point_indices)
                #smallest_d = 0.0001       
                smallest_dd = 0
                for i in range(len(x_emb)):
                    sum_neighbours = 0
                    number_nn = 0
                  #  smallest_dd = 0
                    if i in unconflicted_point_indices:
                        for j in range(number_neighbours - 1):
                            d = distance.euclidean(x_emb[i], x_emb_neighbours[i, j + 1, :])
                            #print ('d', d)
                            if smallest_d > d:
                                smallest_d = d 
                            dd = d - smallest_d
                            #print ('smallest_d', smallest_d)
                            #dd = smallest_d / d
                            #print ('dd', dd)
                           # if smallest_dd < d:
                            #    smallest_dd = d
                            #else:
                            #    smallest_dd = smallest_dd 
                            #dd = smallest_dd / d
                            # dd = d  /  (smallest_d * smallest_d )
                           
                           # print ('dd ', dd)
                            if dd < Beta:
                                #if dd <= Beta2:
                                sum_neighbours = sum_neighbours + x_emb_neighbours[i, j + 1, :] 
                                number_nn = number_nn + 1
                        if number_nn != 0:
                            centers_embs[i] = sum_neighbours / number_nn
                        else:
                            centers_embs[i] = x_emb[i]
               
                x_emb = self.predict_encoder(x)
                ##
                #act='relu'
                #dims=[x.shape[-1], 500, 500, 2000, 2]
                
                #n_stacks = len(dims) - 1
                #init = VarianceScaling(scale=1. / 3., mode='fan_in', distribution='uniform')
                # input
                
                #x = Input(shape=(dims[0],), name='input_encoder')
                #print('xxxxxxx',x)
                #h = x
                # internal layers in encoder
                #for i in range(n_stacks-1):
                #    h = Dense(dims[i + 1], activation=act, kernel_initializer=init, name='encoder_%d' % i)(h)
                # hidden layer
                #h = Dense(dims[-1], kernel_initializer=init, name='encoder_%d' % (n_stacks - 1))(h)  # hidden layer, features are extracted from here
                #encoder = Model(inputs=x, outputs=h, name='encoder')
                #plot_model(encoder, show_shapes=True, show_layer_names=True, to_file=self.visualisation_dir + '/graph/FcEncoder.png')
                
                ##
               
                #print ('x_emb' , x_emb )
                #x_emb_u = self.predict_encoder(unconflicted_point_indices)
               # print ('x_emb' , x_emb )
               # print ('unconflicted_point_indices' , unconflicted_point_indices )
                accuracy_unconflicted_point = unconflicted_point / (unconflicted_point + conflicted_point)
                accuracy_conflicted_point = conflicted_point / (unconflicted_point + conflicted_point)
                print ('accuracy_unconflicted_point : ', accuracy_unconflicted_point)
                print ('accuracy_conflicted_point : ', accuracy_conflicted_point)
                
                #print ('x_emb_u' , x_emb_u )
                km = KMeans(n_clusters=self.n_clusters, n_init=20)
                y_pred = km.fit_predict(x_emb, callbacks)
               
                #print('uncon00')
                
                
                #y_pred_sc = DBSCAN(eps=0.25, min_samples=9).fit_predict(x_emb)
              #  y_pred_sc= Birch(branching_factor = 50, n_clusters = self.n_clusters, threshold = 1.5).fit_predict(x_emb)
                #y_pred_sc= GaussianMixture(self.n_clusters, covariance_type='full', random_state=0).fit_predict(x_emb)
               # y_pred_sc = SpectralClustering(n_clusters=self.n_clusters).fit_predict(x_emb)
                #y_pred_sc = AgglomerativeClustering(n_clusters=self.n_clusters, affinity='euclidean', linkage='ward').fit_predict(x_emb)
                avg_loss = loss / validate_interval
                loss = 0
               # acc_prev = 0
                acc = 0
                if y is not None:
                 
                    km = KMeans(n_clusters=self.n_clusters, n_init=20)
                #km = kmedoids(n_clusters=self.n_clusters, n_init=20)
                    y_pred = km.fit_predict(x_emb, callbacks)
                    
                   
                  #y_pred_sc = DBSCAN(eps=0.25, min_samples=9).fit_predict(x_emb)
                  #  y_pred_sc = SpectralClustering(n_clusters=self.n_clusters).fit_predict(x_emb)
                   # y_pred_sc= Birch(branching_factor = 50, n_clusters = self.n_clusters, threshold = 1.5).fit_predict(x_emb)
                   # y_pred_sc= GaussianMixture(self.n_clusters, covariance_type='full', random_state=0).fit_predict(x_emb)
                    #y_pred_sc = AgglomerativeClustering(n_clusters=self.n_clusters, affinity='euclidean', linkage='ward').fit_predict(x_emb)
# ##  New Part                    
                    true_neighbours = 0
                    false_neighbours = 0
                    true_neighbours_after_filtering = 0
                    false_neighbours_after_filtering = 0
                    #smallest_d = 0.0001
                    smallest_dd = 0
                    
                    # Selecting the neighbours of the unconflicted points
                    for i in range(len(x_emb)):
                      #  biggest_d = 0
                      #  smallest_d = 10000
                    #    smallest_dd = 0
                        
                        if i in unconflicted_point_indices:
                            for j in range(number_neighbours - 1):
                                d = distance.euclidean(x_emb[i], x_emb_neighbours[i, j + 1, :])
                                if smallest_d > d:
                                    smallest_d = d 
                                dd = d - smallest_d    
                               # print ('dd : ', dd)
                                #if smallest_dd < d:
                                 #   smallest_dd = d
                                  #  print ('smallest_dd d ' , smallest_dd)
                                #else:
                                 #   smallest_dd = smallest_dd 
                                   # print ('smallest_dd smallest_dd' , smallest_dd)
                                #dd = smallest_dd   /  d
                                    #print ('ddevaluation', dd)
                            
                                if dd < Beta:
                                    #if dd <= Beta2:
                                    if y[i] == y[indices[i, j]]:
                                        true_neighbours_after_filtering = true_neighbours_after_filtering + 1
                                    else:
                                        false_neighbours_after_filtering = false_neighbours_after_filtering + 1
                                
                                if y[i] == y[indices[i, j]]:
                                    true_neighbours = true_neighbours + 1
                                else: 
                                    false_neighbours = false_neighbours + 1
                        
                    
                                    
                    #if prev_acc <= acc: 
                    if (true_neighbours_after_filtering + false_neighbours_after_filtering) != 0:
                        acc_neighbours = true_neighbours / (false_neighbours + true_neighbours)
                        acc_filtered_neighbours = true_neighbours_after_filtering / (true_neighbours_after_filtering + false_neighbours_after_filtering)
                    else:
                        acc_neighbours = 0
                        acc_filtered_neighbours = 0
                    
                    
                   # acc_filtered_neighbours = true_neighbours_after_filtering / (true_neighbours_after_filtering + false_neighbours_after_filtering) 
                    print ('true_neighbours : ', true_neighbours)
                    print ('false_neighbours : ', false_neighbours)
                    print ('true_neighbours_after_filtering : ', true_neighbours_after_filtering)
                    print ('false_neighbours_after_filtering : ', false_neighbours_after_filtering)
                    print ('acc_neighbours : ', acc_neighbours)
                    print ('acc_filtered_neighbours : ', acc_filtered_neighbours)
                   
                    km = KMeans(n_clusters=self.n_clusters, n_init=20)
                    
                    y_pred = km.fit_predict(x_emb, callbacks)
                   
                    
                    
                    
              
                    
                    acc = np.round(metrics.acc(y, y_pred), 5)
                    nmi = np.round(metrics.nmi(y, y_pred), 5)
                    ari = np.round(metrics.ari(y, y_pred), 5)

                    ID = [] 
                    PC_ID = []
                    for k in range(self.n_clusters):
                      ID.append(computeID(x_emb[y_pred==k]))
                      PC_ID.append(computePC_ID(x_emb[y_pred==k]))
                    ID = np.asarray(ID)
                    PC_ID = np.asarray(PC_ID)
                    ID_global_mean = np.mean(ID)
                    ID_local_mean = np.mean(ID, axis=1) 
                    ID_global_error = np.std(ID)
                    ID_local_error = np.std(ID, axis=1)
                    PC_ID_mean = np.mean(PC_ID)
                    FT = PC_ID_mean - ID_global_mean
                    #print ("FT", FT)
                    print ("ID_local_mean = ", ID_local_mean)
                    print ("ID = ", ID_global_mean)
                    print ("LID = ", PC_ID_mean)
                    
                    if acc > acc_prev:
                        #acc_prev = acc
                        logdict = dict(iter=ite, acc=acc, nmi=nmi, ari=ari,  loss=avg_loss,  trueN=true_neighbours, FalseN=false_neighbours, trueAF=true_neighbours_after_filtering, FalseAF=false_neighbours_after_filtering,  accN=acc_neighbours, accF=acc_filtered_neighbours, ID_global_mean=ID_global_mean, ID_local_mean=ID_local_mean.tolist(), PC_ID_mean=PC_ID_mean, ID_global_error=ID_global_error, ID_local_error=ID_local_error.tolist(), FT=FT)
                        logwriter.writerow(logdict)
                        logfile.flush()
                    
                    print('K-means: iter %d, acc=%.5f, nmi=%.5f, ari=%.5f, loss=%.5f' % (ite, acc, nmi, ari,  avg_loss))
                #    print('Spectral clustering: iter %d, acc=%.5f, nmi=%.5f, ari=%.5f, loss=%.5f' % (ite, acc_sc, nmi_sc, ari_sc, avg_loss))
                  
                    #print("")
                    #print("----------------------------------------------------------------------------------")
                    print("Centroids : ")
                    print("----------------------------------------------------------------------------------")
                    #centers_emb, centers_img, y_pred, _ = self.generate_centers(x, n_clusters)
                    #draw_centers(n_clusters, centers_img, img_h=img_h, img_w=img_w)
                       
                else:
                    x_embed = self.encoder.predict(x)
                    silhouette_avg = silhouette_score(x_embed, y_pred) 
                    print("For number of clusters = " +  str(self.nearest_neighbours) + " The average silhouette_score is :", silhouette_avg) 
                   
            
           
          
            idx = index_array[index * batch_size: min((index+1) * batch_size, x.shape[0])]
            
          #wit best  idx = index_array[index * 10: min((index+1) * 10, x.shape[0])]
           
            X_img = x[idx]
            
           
            
            x_emb = self.predict_encoder(x)
            Y_encoder = centers_embs[idx]
           
                        #print('Y_encoder',Y_encoder)
            Y_autoencoder = self.decoder.predict(centers_embs[idx])
            
              # Y_autoencoder = self.decoder.predict(x_emb)
            X_transformed = self.random_transform(X_img, ws=self.ws, hs=self.hs, rot=self.rot, scale=self.scale) if aug_train else X_img
           
            
            losses = self.train_on_batch_dynAE(X_transformed, Y_encoder, Y_autoencoder)
           
            
            
   
            loss = loss + losses
           
          
            
            index = index + 1 if (index + 1) * batch_size <= int(x.shape[0]) else 0
            

         
            if acc > acc_prev:
                acc_prev = acc
           #     print('acc_prev2', acc_prev)
                if os.path.isfile(save_dir + '/' + self.dataset + '/phase_2/ae_weights.h5'):
                   os.remove (save_dir + '/' + self.dataset + '/phase_2/ae_weights.h5')
                   print('remove file' )
                   print('saving model to ::', save_dir + '/' + self.dataset + '/phase_2/ae_weights.h5')
                   self.ae.save_weights(save_dir + '/' + self.dataset + '/phase_2/ae_weights.h5')
                   print('save the best acc in W', acc )
                   logdict1 = dict(iter=ite, accMax=acc_prev, nmi=nmi)
                   logwriter1.writerow(logdict1)
                   logfile1.flush()
                else: ## Show an error ##
                   print('no file' )
                   print('saving model to ::', save_dir + '/' + self.dataset + '/phase_2/ae_weights.h5')
                   self.ae.save_weights(save_dir + '/' + self.dataset + '/phase_2/ae_weights.h5')
                   print('save the best acc in W', acc )
                   logdict1 = dict(iter=ite, accMax=acc_prev, nmi=nmi)
                   logwriter1.writerow(logdict1)
                   logfile1.flush()
  
  
                    
              
          
          
        print('Clustering time: %ds' % (time() - t1))
        print('End clustering:', '-' * 60)
        print("Centroids : ")
        print("----------------------------------------------------------------------------------")
                #centers_emb, centers_img, y_pred, _ = self.generate_centers(x, n_clusters)
                #draw_centers(n_clusters, centers_img, img_h=img_h, img_w=img_w)
                
        logfile.close()
       # print('training time: ', time() - t0)
    #    print('training: ', '-' * 60)
        print('Clustering time: %ds' % (time() - t1))
        print('End clustering:', '-' * 60)
        print("Centroids : ")
        print('end')
        print("----------------------------------------------------------------------------------")
       # centers_emb, centers_img, y_pred, _ = self.generate_centers(x, n_clusters)
    #    draw_centers(n_clusters, centers_img, img_h=img_h, img_w=img_w)
       
        
        return y_pred
        
       
    def train_dynAE_New_chestmnist(self, x, y=None, kappa=1, n_clusters=7, n_overclusters=10, maxiter=1e9, batch_size=256, tol=1e-2, validate_interval=140, show_interval=None, save_interval=2800, save_dir=PATH_RESULT, aug_train=True, callbacks=callbacks_list):
    #def train_dynAE_New(self, x, y=None, kappa=3, n_clusters=10, n_overclusters=40, maxiter=1e5, batch_size=256, tol=1e-2, validate_interval=140, show_interval=2000, save_interval=2800, save_dir=PATH_RESULT, aug_train=True):
        #init
        number_of_samples = x.shape[0]
        img_h = int(math.sqrt(x.shape[1]))
        img_w = int(math.sqrt(x.shape[1]))
       # print('maxiter',maxiter)
        #logging file
        import csv, os
        if not os.path.exists(save_dir):
            os.makedirs(save_dir)
        logfile = open(save_dir + '/' + self.dataset + '/cluster/train_dynAE_gamma=' + str(self.gamma) + '_logآNew.csv', 'w')
        #logwriter = csv.DictWriter(logfile, fieldnames=['iter', 'acc', 'nmi', 'ari', 'acc_unconf', 'nmi_unconf', 'acc_conf', 'nmi_conf', 'nb_unconf', 'nb_conf', 'fr', 'fd', 'loss'])
        logwriter = csv.DictWriter(logfile, fieldnames=['iter', 'acc', 'nmi', 'ari', 'acc_unconf', 'nmi_unconf', 'acc_conf', 'nmi_conf', 'nb_unconf', 'nb_conf', 'loss'])
        logwriter.writeheader()

        #intervals config
        print('Begin clustering:', '-' * 60)
        if save_interval is None: 
            save_interval = int(maxiter)  # only save the initial and final model
        print('Save interval ', save_interval)
        if show_interval is None:
            show_interval = int(np.ceil(number_of_samples/batch_size))*20
        print('show interval ', show_interval)  
        
        # Step 1: initialize cluster centers using k-means
        t1 = time()
        print('Initializing cluster centers with k-means.')
        
        #new just for the centroid
        #x_emb = self.predict_encoder(x)
        #km = KMeans(n_clusters=self.n_clusters, n_init=20)
        #y_pred = km.fit_predict(x_emb)
        #new
        
        

        # Step 2: beta1 and beta2
        #beta1, beta2 = self.generate_beta(kappa, n_clusters)
      #  beta1 = .7
        #beta2 = 0.25
       # beta2 = .5
        beta1 =0.5
        beta2=0.5
        beta1_overclustering = 0.0
       # beta2_overclustering = 0.15
        beta2_overclustering = 0.15
        n_clusters = 2
        n_overclusters = 200
        kappa = 2
        
        acc_prev = 0
        
        centers_emb, centers_img, y_pred, _ = self.generate_centers(x, n_clusters)
        overcenters_emb, overcenters_img, _, _ = self.generate_centers(x, n_overclusters)
       # print('cenemp', centers_emb )
        
        
        print ('beta1 beta2 b1ove b2ove n_overclusters n_clusters',beta1,beta2,beta1_overclustering,beta2_overclustering,n_overclusters, n_clusters)            
        # Step 3: deep clustering
        loss = 0
        index = 0
        nb_conf_prev = x.shape[0]
        index_array = np.arange(x.shape[0])
        delta_kappa = 0.1 * kappa
        #sess = tf.keras.backend.get_session()
        #first_time = 1
        #validate_interval = 1
        
        print("Centroids : ")
        print("----------------------------------------------------------------------------------")
        #draw_centers(n_clusters, centers_img, img_h=img_h, img_w=img_w)
        
        
        
        for ite in range(int(maxiter)):
          #  print ('range inside ite  ite' , range(int(maxiter))  , ite )
            if ite % validate_interval == 0:
              
                x_emb = self.encoder.predict(x)
              
              #  print('xemb   ',x_emb)
                q = q_mat(x_emb, centers_emb)
               # print('change ypred ', ite % validate_interval)
                y_pred = q.argmax(1) 
                #print('y_pred after q', y_pred)
                
                avg_loss = loss / validate_interval
                loss = 0.
                if ite > 0:
                    nb_conf_prev = nb_conf 
               # print('nb_conf_prev', nb_conf_prev )
                nb_unconf, nb_conf = self.compute_nb_conflicted_data(x, centers_emb, beta1, beta2)
    ##   from the fun         
                x_emb = self.encoder.predict(x)
                unconf_indices = []
                conf_indices = []
                q = q_mat(x_emb, centers_emb, alpha=3)
                confidence1 = q.max(1)
                confidence2 = np.zeros((q.shape[0],))
                a = np.argsort(q, axis=1)[:,-2]  
                for i in range(x.shape[0]):
                    confidence2[i] = q[i,a[i]]
              #  print('confidence1', confidence1[i])
               # print('confidence',confidence1[i] - confidence2[i])
                #print('beta1',beta1 ,beta2 )
                if (confidence1[i]) > beta1 and (confidence1[i] - confidence2[i]) > beta2:
                    
                    unconf_indices.append(i)
                else:
                    conf_indices.append(i)
                unconf_indices = np.asarray(unconf_indices, dtype=int)
                conf_indices = np.asarray(conf_indices, dtype=int)
    ###end the part -- remove 
                #nb_unconf, nb_conf = self.compute_nb_conflicted_data(x, centers_emb, beta1, beta2)
                print('nb_unconf', unconf_indices )
                print('nb_conf', conf_indices )
                
                #update centers
                if nb_conf >= nb_conf_prev:
                    #centers_emb, centers_img, _, _ = self.generate_centers(x, n_clusters) 00
                  #  print("update centers", nb_conf , nb_conf_prev )
                    beta1 = beta1 - (delta_kappa / n_clusters)
                    beta2 = beta2 - (delta_kappa / n_clusters)
                    #beta1 = 0.5
                    #beta2 = 0.25
                    delta_kappa = 0.1 * kappa
                    kappa = delta_kappa
                    #first_time = 0
                    #print("update confidences")
                
                print(' before y', y )    

                if y is not None:
                    #y_mapped = map_vector_to_clusters(y, y_pred)
                    #x_emb = self.predict_encoder(x)
                    #y_encoder, y_autoencoder = generate_supervisory_signals(x_emb, x, centers_emb, centers_img, beta1, beta2)
                    #y_encoder_true = centers_emb[y_mapped]
                    #grad_loss_dynAE = sess.run(self.grad_loss_dynAE, feed_dict={'input_dynAE:0': x, 'target1_dynAE:0': y_encoder, 'target2_dynAE:0': y_autoencoder})
                    #grad_loss_pseudo_supervised = sess.run(self.grad_loss_pseudo_supervised, feed_dict={'input_dynAE:0': x, 'target1_dynAE:0': y_encoder})
                    #grad_loss_self_supervised = sess.run(self.grad_loss_self_supervised, feed_dict={'input_dynAE:0': x, 'target2_dynAE:0': y_autoencoder})
                    #grad_loss_supervised = sess.run(self.grad_loss_supervised, feed_dict={'input_dynAE:0': x, 'target3_dynAE:0': y_encoder_true})
                    
                    acc = np.round(metrics.acc(y, y_pred), 5)
                  #  print('accccccc',acc)
                    nmi = np.round(metrics.nmi(y, y_pred), 5)
                    ari = np.round(metrics.ari(y, y_pred), 5) 
                    #fr = np.round(metrics.cos_grad(grad_loss_supervised, grad_loss_dynAE), 5)
                    #fd = np.round(metrics.cos_grad(grad_loss_self_supervised, grad_loss_pseudo_supervised), 5)
                    acc_unconf, nmi_unconf, acc_conf, nmi_conf = self.compute_acc_and_nmi_conflicted_data(x, y, centers_emb, beta1, beta2)
                    #logdict = dict(iter=ite, acc=acc, nmi=nmi, ari=ari, acc_unconf=acc_unconf, nmi_unconf=nmi_unconf, acc_conf=acc_conf, nmi_conf=nmi_conf, nb_unconf=nb_unconf, nb_conf=nb_conf, fr=fr, fd=fd, loss=avg_loss)
                    logdict = dict(iter=ite, acc=acc, nmi=nmi, ari=ari, acc_unconf=acc_unconf, nmi_unconf=nmi_unconf, acc_conf=acc_conf, nmi_conf=nmi_conf, nb_unconf=nb_unconf, nb_conf=nb_conf, loss=avg_loss)
                    logwriter.writerow(logdict)
                    logfile.flush()
                    #print('Iter %d: acc=%.5f, nmi=%.5f, ari=%.5f, acc_unconf=%.5f, nmi_unconf=%.5f, acc_conf=%.5f, nmi_conf=%.5f, nb_unconf=%d, nb_conf=%d, fr=%.5f, fd=%.5f, loss=%.5f' % (ite, acc, nmi, ari, acc_unconf, nmi_unconf, acc_conf, nmi_conf, nb_unconf, nb_conf, fr, fd, avg_loss))
                    print('Iter %d: acc=%.5f, nmi=%.5f, ari=%.5f, acc_unconf=%.5f, nmi_unconf=%.5f, acc_conf=%.5f, nmi_conf=%.5f, nb_unconf=%d, nb_conf=%d, loss=%.5f' % (ite, acc, nmi, ari, acc_unconf, nmi_unconf, acc_conf, nmi_conf, nb_unconf, nb_conf, avg_loss))
                    print("The number of unconflicted data points is : " + str(nb_unconf))
                    print("The number of conflicted data points is : " + str(nb_conf))
                    if acc > acc_prev:
                        acc_prev = acc
                        self.ae.save_weights(save_dir + '/' + self.dataset + '/cluster/ae_N_weights.h5')
                        
                else:
                    x_embed = self.encoder.predict(x)
                    silhouette_avg = silhouette_score(x_embed, y_pred) 
                    print("For number of clusters = " +  str(self.n_clusters) + " The average silhouette_score is :", silhouette_avg) 
                    #logdict = dict(iter=ite, silhouette_avg=silhouette_avg, loss=avg_loss)
                    #logwriter.writerow(logdict)
                    #logfile.flush()
              #  print('nb_conf', nb_conf )
               # print('xshape', x.shape[0])
                #print('tol', tol)
                    
                if(nb_conf / x.shape[0]) < tol:
                   # print('clooooooose')
                    logfile.close()
                    break

            if ite % show_interval == 0:
               
                print("")
                print("----------------------------------------------------------------------------------")
                print("Centroids : ")
                print("----------------------------------------------------------------------------------")
               # draw_centers(n_clusters, centers_img, img_h=img_h, img_w=img_w)
                
                # save intermediate model
            
            if ite % save_interval == 0:
                
                z = self.predict_encoder(x)
                q1 = q_mat(z, centers_emb)
               # y1_pred = q1.argmax(1)
                y1_pred = q1.argmax(1)
               

                pca = PCA(n_components=2).fit(z)
                z_2d = pca.transform(z)
                centers_2d = pca.transform(centers_emb)

                # save states for visualization
                np.save(self.visualisation_dir + '/embeddings/' + self.dataset + '/vis_' + str(ite) + '.npy', {'z_2d': z_2d, 'centers_2d': centers_2d, 'y_pred': y1_pred})

                print('saving model to: ', save_dir + '/' + self.dataset + '/cluster/ae_' + str(ite) + '_weights.h5')
                self.ae.save_weights(save_dir + '/' + self.dataset + '/cluster/ae_' + str(ite) + '_weights.h5')

            # train on batch
            if ite % 2 == 2:
                idx = index_array[index * batch_size: min((index+1) * batch_size, x.shape[0])]
                X_img = x[idx]
                X_emb = self.predict_encoder(X_img)
                Y_encoder, Y_autoencoder = generate_supervisory_signals(X_emb, X_img, centers_emb, centers_img, beta1, beta2)
                X_transformed = self.random_transform(X_img, ws=self.ws, hs=self.hs, rot=self.rot, scale=self.scale) if aug_train else X_img
                losses = self.train_on_batch_dynAE(X_transformed, Y_encoder, Y_autoencoder)
                loss = loss + losses
               # print('loooose')
                index = index + 1 if (index + 1) * batch_size <= x.shape[0] else 0
            else:
                idx = index_array[index * batch_size: min((index+1) * batch_size, x.shape[0])]
                X_img = x[idx]
                X_emb = self.predict_encoder(X_img)
                Y_encoder, Y_autoencoder = generate_supervisory_signals(X_emb, X_img, overcenters_emb, overcenters_img, beta1_overclustering, beta2_overclustering)
                X_transformed = self.random_transform(X_img, ws=self.ws, hs=self.hs, rot=self.rot, scale=self.scale) if aug_train else X_img
                losses = self.train_on_batch_dynAE(X_transformed, Y_encoder, Y_autoencoder)
                loss = loss + losses
                index = index + 1 if (index + 1) * batch_size <= x.shape[0] else 0
                
        
        
     
        print('Clustering time: %ds' % (time() - t1))
        print('End clustering:', '-' * 60)
        
        return y_pred
